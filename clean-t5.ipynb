{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d43e43f26c174ecfab68592058cfe253":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_04b0099718244ffa9a58aad685005b85","IPY_MODEL_68db0dcc596d4974a52c8ac2d1717ccc","IPY_MODEL_f871446020704d67b4535950c6cc6ea4"],"layout":"IPY_MODEL_9284c2c34d294eb8a30ac5c6a60aabe1"}},"04b0099718244ffa9a58aad685005b85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c3cf94547494db08b86b9eba3813dfe","placeholder":"​","style":"IPY_MODEL_b16e204e78da46fcafba0709fca60832","value":"spiece.model: 100%"}},"68db0dcc596d4974a52c8ac2d1717ccc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6e3d0cd15ea49d7af2e2f69ea10df59","max":791656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_838b9535234440a3a45d5d1ea565e4cc","value":791656}},"f871446020704d67b4535950c6cc6ea4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d2e85fbd14d48ae9345db36bf17ec40","placeholder":"​","style":"IPY_MODEL_48b7b75d97364696847dc04d69b13d9a","value":" 792k/792k [00:00&lt;00:00, 24.5MB/s]"}},"9284c2c34d294eb8a30ac5c6a60aabe1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c3cf94547494db08b86b9eba3813dfe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b16e204e78da46fcafba0709fca60832":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d6e3d0cd15ea49d7af2e2f69ea10df59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"838b9535234440a3a45d5d1ea565e4cc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2d2e85fbd14d48ae9345db36bf17ec40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48b7b75d97364696847dc04d69b13d9a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e67193f2d1784f5586a431bdb57f2003":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_79bc0b1c7db542d8ae8610de5bd1675c","IPY_MODEL_ba0de21d8cf74f4886f7abe85314176b","IPY_MODEL_9a781714b39a48d5b0222c61a9d97953"],"layout":"IPY_MODEL_7d79870ae32444208ad83f3ad19504e9"}},"79bc0b1c7db542d8ae8610de5bd1675c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ae0a97a67c44c6a85804a7a33c9fab9","placeholder":"​","style":"IPY_MODEL_8d993172a4c24b959ce56519fe0bf00d","value":"tokenizer.json: 100%"}},"ba0de21d8cf74f4886f7abe85314176b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_85c18d8ae6da49b39b613c2744a76921","max":1389353,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e18d33d70a354aa388e9ec749b7edc7c","value":1389353}},"9a781714b39a48d5b0222c61a9d97953":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48d63563126b431aa864fc41f7e5eeff","placeholder":"​","style":"IPY_MODEL_b81ddcd087724ee5b7fc688a1ab7fe94","value":" 1.39M/1.39M [00:00&lt;00:00, 6.36MB/s]"}},"7d79870ae32444208ad83f3ad19504e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ae0a97a67c44c6a85804a7a33c9fab9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d993172a4c24b959ce56519fe0bf00d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85c18d8ae6da49b39b613c2744a76921":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e18d33d70a354aa388e9ec749b7edc7c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"48d63563126b431aa864fc41f7e5eeff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b81ddcd087724ee5b7fc688a1ab7fe94":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f5ce8373b24c4ea4a06a908bfc247ee1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5833f0f15fd0495faf1736a4bba8b842","IPY_MODEL_4295c66a73534974a601a1be44b584f0","IPY_MODEL_66d03cef52334283916a88ad291ff02b"],"layout":"IPY_MODEL_54c35204193d477ebe68fd8665fa4286"}},"5833f0f15fd0495faf1736a4bba8b842":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f0cbd047f3840f5903c9b0b13887629","placeholder":"​","style":"IPY_MODEL_b13453fadbec46c7b1cad36ee90cc4be","value":"config.json: 100%"}},"4295c66a73534974a601a1be44b584f0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1cf8344acb441bb93ef0ba5f693597f","max":1208,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dbe7f42bf3f54f7a941e6579d912b97f","value":1208}},"66d03cef52334283916a88ad291ff02b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_116b73704f99412d9208aa422f76639b","placeholder":"​","style":"IPY_MODEL_f8a95f62eab14b20bd1e9046f9f1fe30","value":" 1.21k/1.21k [00:00&lt;00:00, 109kB/s]"}},"54c35204193d477ebe68fd8665fa4286":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f0cbd047f3840f5903c9b0b13887629":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b13453fadbec46c7b1cad36ee90cc4be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1cf8344acb441bb93ef0ba5f693597f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbe7f42bf3f54f7a941e6579d912b97f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"116b73704f99412d9208aa422f76639b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8a95f62eab14b20bd1e9046f9f1fe30":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"65a4b63349854e5798e055b68d2af719":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8742205ecccc4b4daf0c51e301077f26","IPY_MODEL_832b8c6a9cc94ecaa7aee2b6425c752b","IPY_MODEL_7c80ec3a8b9f444ca1092be88fb3f203"],"layout":"IPY_MODEL_0a53585282a3448ea5145df1c4bb9a4b"}},"8742205ecccc4b4daf0c51e301077f26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3312d94dd140447fa6b4adde75717dea","placeholder":"​","style":"IPY_MODEL_feaa4e1753e64e9bb4705385466a708c","value":"model.safetensors: 100%"}},"832b8c6a9cc94ecaa7aee2b6425c752b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca5855a3a81c4604b37f527ff2df8097","max":891646390,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ee678f1395b04f5eae5aa5920477a225","value":891646390}},"7c80ec3a8b9f444ca1092be88fb3f203":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7858630a17504a4c8521450cf57f5e4f","placeholder":"​","style":"IPY_MODEL_87c6b5f41d7c4447909e4fce265fc052","value":" 892M/892M [00:04&lt;00:00, 241MB/s]"}},"0a53585282a3448ea5145df1c4bb9a4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3312d94dd140447fa6b4adde75717dea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"feaa4e1753e64e9bb4705385466a708c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca5855a3a81c4604b37f527ff2df8097":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee678f1395b04f5eae5aa5920477a225":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7858630a17504a4c8521450cf57f5e4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87c6b5f41d7c4447909e4fce265fc052":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37ddfba1a7e9415badcc00a55e3f2407":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_42e0abe6b20a4f26ad520f911fc21505","IPY_MODEL_c0836c8f6f0a4a0aa0079d707e6b67f8","IPY_MODEL_9dda11d80d124cd9a3d5fb77495e2d48"],"layout":"IPY_MODEL_d5c86f70fc054763a91dcd2d4505273a"}},"42e0abe6b20a4f26ad520f911fc21505":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ed368d63057420fba470676f96aabe6","placeholder":"​","style":"IPY_MODEL_87b8a1451c9b491a9290255220678f43","value":"generation_config.json: 100%"}},"c0836c8f6f0a4a0aa0079d707e6b67f8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_70d12161f56a4dcbb227375830250f92","max":147,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9b4a4af357f345a2bc013eaaf2193736","value":147}},"9dda11d80d124cd9a3d5fb77495e2d48":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c75ece8c6494a33845efeba3228086a","placeholder":"​","style":"IPY_MODEL_a3e4c74c125147fc8e45a913bb9b0514","value":" 147/147 [00:00&lt;00:00, 12.9kB/s]"}},"d5c86f70fc054763a91dcd2d4505273a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ed368d63057420fba470676f96aabe6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87b8a1451c9b491a9290255220678f43":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"70d12161f56a4dcbb227375830250f92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b4a4af357f345a2bc013eaaf2193736":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8c75ece8c6494a33845efeba3228086a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3e4c74c125147fc8e45a913bb9b0514":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1772b18829864ac780f9aa1882948ea7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bbf5b08ed67846168649410e08ec3585","IPY_MODEL_239b09560b2b4434bc9be2948bd7c5a6","IPY_MODEL_ddeaf26f62b04026b917d53f7af7365c"],"layout":"IPY_MODEL_3bfc6b6391464313a59bbfc2f3b3a744"}},"bbf5b08ed67846168649410e08ec3585":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8cf04e86cf24c8c8c82c51c494ee8ed","placeholder":"​","style":"IPY_MODEL_cff2134fe4464226a1eee8d7277826f2","value":"tokenizer_config.json: 100%"}},"239b09560b2b4434bc9be2948bd7c5a6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_269160ba734b433b9f8e6e4dc2a6a3d9","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_de1dde6e457f4cc9a0de76d1f87d7300","value":25}},"ddeaf26f62b04026b917d53f7af7365c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8edd681b44934443b6473ca34ff1018d","placeholder":"​","style":"IPY_MODEL_9bf72bfff0394dcd83a1ed32b6d7e0c7","value":" 25.0/25.0 [00:00&lt;00:00, 2.26kB/s]"}},"3bfc6b6391464313a59bbfc2f3b3a744":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8cf04e86cf24c8c8c82c51c494ee8ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cff2134fe4464226a1eee8d7277826f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"269160ba734b433b9f8e6e4dc2a6a3d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de1dde6e457f4cc9a0de76d1f87d7300":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8edd681b44934443b6473ca34ff1018d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bf72bfff0394dcd83a1ed32b6d7e0c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f506842a8b5f44219e69183aa5d000d3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ed0dbe32fdc14cf28ceec820e1806ed7","IPY_MODEL_674544753486454686b0c5bd9bc58cbf","IPY_MODEL_5b105086ccbf4052b464da390206a72d"],"layout":"IPY_MODEL_1a4dc3dedbb44fd4a4393f8e8946e47d"}},"ed0dbe32fdc14cf28ceec820e1806ed7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_80191e5c9afd4df3b218a33bac287406","placeholder":"​","style":"IPY_MODEL_a495b0d9d22947519f8af11d3e03c98a","value":"config.json: 100%"}},"674544753486454686b0c5bd9bc58cbf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f97897c2a1640bc8bc61b8c5250e14a","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6f5ab6ae0b9b4cd89f1dc3b4c673350b","value":482}},"5b105086ccbf4052b464da390206a72d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db9bc4f0b23149ef94aa5b9911ad7b0e","placeholder":"​","style":"IPY_MODEL_b8d71e943ca54f6baeaa77b0de5b96fe","value":" 482/482 [00:00&lt;00:00, 39.5kB/s]"}},"1a4dc3dedbb44fd4a4393f8e8946e47d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80191e5c9afd4df3b218a33bac287406":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a495b0d9d22947519f8af11d3e03c98a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f97897c2a1640bc8bc61b8c5250e14a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f5ab6ae0b9b4cd89f1dc3b4c673350b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"db9bc4f0b23149ef94aa5b9911ad7b0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8d71e943ca54f6baeaa77b0de5b96fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56e169ce22d7401ebf7744b365bcb53b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a3d327d5498640dba6631addb30a6954","IPY_MODEL_dcda6e866bbc499f823815dbf13d2d49","IPY_MODEL_53aae5bec8f4485482c0824782de026c"],"layout":"IPY_MODEL_75e87831ee474e58a00c117e45082c22"}},"a3d327d5498640dba6631addb30a6954":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2456ee785c3246f5bcf79b2b50be299b","placeholder":"​","style":"IPY_MODEL_e87e16bdc3254f35accdf3b342952424","value":"vocab.json: 100%"}},"dcda6e866bbc499f823815dbf13d2d49":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_63ddb14ec318445f9510c069de053fd8","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd802d183825457e9b7873fb9c7116f4","value":898823}},"53aae5bec8f4485482c0824782de026c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94521f2c821948f194de01647153a9fd","placeholder":"​","style":"IPY_MODEL_ffeb1f9dcaf24f9589d1e67eed77ae9f","value":" 899k/899k [00:00&lt;00:00, 3.39MB/s]"}},"75e87831ee474e58a00c117e45082c22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2456ee785c3246f5bcf79b2b50be299b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e87e16bdc3254f35accdf3b342952424":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"63ddb14ec318445f9510c069de053fd8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd802d183825457e9b7873fb9c7116f4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"94521f2c821948f194de01647153a9fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffeb1f9dcaf24f9589d1e67eed77ae9f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"855a12b62aa141f088525dc548e63d1c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3ed32774c63a4d3da077559cd11a8734","IPY_MODEL_fc79560572bd4a3696c7380a0cb23f91","IPY_MODEL_ec8ff7b7f68f434a8715fa24a8fd63f6"],"layout":"IPY_MODEL_5abf4f1500194b1ba0e5b601a97abc9a"}},"3ed32774c63a4d3da077559cd11a8734":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2839b1542f5c46d1bad33ba7ad62de87","placeholder":"​","style":"IPY_MODEL_309a8bfd77ab4d499e8f61e94cb3e48f","value":"merges.txt: 100%"}},"fc79560572bd4a3696c7380a0cb23f91":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f883f4a1140497bbaba7b78f33f9863","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cf2c1f073a1a4c3f984a5a57e0fbe4dd","value":456318}},"ec8ff7b7f68f434a8715fa24a8fd63f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee3fab716ac143029338b739127258aa","placeholder":"​","style":"IPY_MODEL_d8aa794b12464f6cb4ffe771e84f0f59","value":" 456k/456k [00:00&lt;00:00, 1.09MB/s]"}},"5abf4f1500194b1ba0e5b601a97abc9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2839b1542f5c46d1bad33ba7ad62de87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"309a8bfd77ab4d499e8f61e94cb3e48f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f883f4a1140497bbaba7b78f33f9863":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf2c1f073a1a4c3f984a5a57e0fbe4dd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ee3fab716ac143029338b739127258aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8aa794b12464f6cb4ffe771e84f0f59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cae457a0571c497eb42b8e16110bb12d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d3b5c0022ac046f39fdd84fbd7dd8df3","IPY_MODEL_df476200067b4444a23c5e6588d074de","IPY_MODEL_5c2bbe6c4e92461bbf8c1562259daefe"],"layout":"IPY_MODEL_94f1df94a7a748a5bcb1143c6dc7f4be"}},"d3b5c0022ac046f39fdd84fbd7dd8df3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d6a579157e143b58e969a2069c74719","placeholder":"​","style":"IPY_MODEL_2dfba69d07ab4eaf83a0badbb0a65eeb","value":"tokenizer.json: 100%"}},"df476200067b4444a23c5e6588d074de":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_998fde3da7a440bd8b1f2d1c5d8755a2","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3a2c5ad8046e45dab6aab160cfc22d09","value":1355863}},"5c2bbe6c4e92461bbf8c1562259daefe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eebde703b0bc44fea24f47a466142165","placeholder":"​","style":"IPY_MODEL_3fa315ec7f0740fe83ffa596cffa6c8b","value":" 1.36M/1.36M [00:00&lt;00:00, 1.98MB/s]"}},"94f1df94a7a748a5bcb1143c6dc7f4be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d6a579157e143b58e969a2069c74719":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2dfba69d07ab4eaf83a0badbb0a65eeb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"998fde3da7a440bd8b1f2d1c5d8755a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a2c5ad8046e45dab6aab160cfc22d09":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eebde703b0bc44fea24f47a466142165":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fa315ec7f0740fe83ffa596cffa6c8b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"13de5723b40f4183b7e16d6ff13c05f9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4cd06b36f99c4b7bb58102e41f6832f2","IPY_MODEL_838e3eb76a604283b3e72b29e13a2b28","IPY_MODEL_97ea2c10d2654481b6205feee4271955"],"layout":"IPY_MODEL_acbb095fe51349868a5968d45023cb1b"}},"4cd06b36f99c4b7bb58102e41f6832f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba48a602a3d54dd393dface1c5baa7e2","placeholder":"​","style":"IPY_MODEL_c3d6cd1499e941dda30eb0d9374e3640","value":"model.safetensors: 100%"}},"838e3eb76a604283b3e72b29e13a2b28":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1531d4477bc41df9e4d72791d40b883","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0338c7473d8f4dbbb55411a94c32dcff","value":1421700479}},"97ea2c10d2654481b6205feee4271955":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7101f5951594b6da9a0db6e39de01a5","placeholder":"​","style":"IPY_MODEL_c0bba2b9f7d44bce8a4ccb3067ac05ee","value":" 1.42G/1.42G [00:06&lt;00:00, 231MB/s]"}},"acbb095fe51349868a5968d45023cb1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba48a602a3d54dd393dface1c5baa7e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3d6cd1499e941dda30eb0d9374e3640":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a1531d4477bc41df9e4d72791d40b883":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0338c7473d8f4dbbb55411a94c32dcff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d7101f5951594b6da9a0db6e39de01a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0bba2b9f7d44bce8a4ccb3067ac05ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10085920,"sourceType":"datasetVersion","datasetId":6218505},{"sourceId":10214840,"sourceType":"datasetVersion","datasetId":6313697}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers rouge-score bert-score","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vCyvENskg7WI","outputId":"5d9d7330-f106-4c95-a6fa-ae2faaae78ae","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:34:11.760414Z","iopub.execute_input":"2024-12-16T09:34:11.760742Z","iopub.status.idle":"2024-12-16T09:34:25.089140Z","shell.execute_reply.started":"2024-12-16T09:34:11.760705Z","shell.execute_reply":"2024-12-16T09:34:25.088298Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting bert-score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.4.0)\nRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.2.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert-score) (3.7.5)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2024.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.1.4)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (10.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=bf2a0912cc7632026cdddd748dc3e44b288b332e26f70946dedde2aac2ef8f22\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score, bert-score\nSuccessfully installed bert-score-0.3.13 rouge-score-0.1.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom sklearn.model_selection import train_test_split\nfrom rouge_score import rouge_scorer\nfrom bert_score import score as bert_score","metadata":{"id":"PYEwp36JhEyZ","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:34:25.090901Z","iopub.execute_input":"2024-12-16T09:34:25.091218Z","iopub.status.idle":"2024-12-16T09:34:53.765416Z","shell.execute_reply.started":"2024-12-16T09:34:25.091190Z","shell.execute_reply":"2024-12-16T09:34:53.764728Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"model_name = \"t5-base\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":254,"referenced_widgets":["d43e43f26c174ecfab68592058cfe253","04b0099718244ffa9a58aad685005b85","68db0dcc596d4974a52c8ac2d1717ccc","f871446020704d67b4535950c6cc6ea4","9284c2c34d294eb8a30ac5c6a60aabe1","6c3cf94547494db08b86b9eba3813dfe","b16e204e78da46fcafba0709fca60832","d6e3d0cd15ea49d7af2e2f69ea10df59","838b9535234440a3a45d5d1ea565e4cc","2d2e85fbd14d48ae9345db36bf17ec40","48b7b75d97364696847dc04d69b13d9a","e67193f2d1784f5586a431bdb57f2003","79bc0b1c7db542d8ae8610de5bd1675c","ba0de21d8cf74f4886f7abe85314176b","9a781714b39a48d5b0222c61a9d97953","7d79870ae32444208ad83f3ad19504e9","3ae0a97a67c44c6a85804a7a33c9fab9","8d993172a4c24b959ce56519fe0bf00d","85c18d8ae6da49b39b613c2744a76921","e18d33d70a354aa388e9ec749b7edc7c","48d63563126b431aa864fc41f7e5eeff","b81ddcd087724ee5b7fc688a1ab7fe94","f5ce8373b24c4ea4a06a908bfc247ee1","5833f0f15fd0495faf1736a4bba8b842","4295c66a73534974a601a1be44b584f0","66d03cef52334283916a88ad291ff02b","54c35204193d477ebe68fd8665fa4286","7f0cbd047f3840f5903c9b0b13887629","b13453fadbec46c7b1cad36ee90cc4be","c1cf8344acb441bb93ef0ba5f693597f","dbe7f42bf3f54f7a941e6579d912b97f","116b73704f99412d9208aa422f76639b","f8a95f62eab14b20bd1e9046f9f1fe30"]},"id":"9ga5wV8rjx6A","outputId":"1c68174d-356e-4d74-df49-e574747a767d","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:34:53.766501Z","iopub.execute_input":"2024-12-16T09:34:53.767169Z","iopub.status.idle":"2024-12-16T09:34:54.981705Z","shell.execute_reply.started":"2024-12-16T09:34:53.767129Z","shell.execute_reply":"2024-12-16T09:34:54.981040Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"801bfbebbdb74b7cacc9360bf122a94e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"619fe5dde1494d41a5ee2d60b05ba494"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df0760a03b5d4211b8b93fcea6fe42b7"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# File paths\nfile_paths = {\n    'train': '/kaggle/input/dataset-lowercase-clean/train_data (1).csv',\n    'test': '/kaggle/input/dataset-lowercase-clean/test_data (1).csv',\n    'val': '/kaggle/input/dataset-lowercase-clean/val_data.csv'\n}\n\ndef process_data(file_path):\n    # Membaca CSV\n    data = pd.read_csv(file_path)\n    \n    # Mengganti NaN dengan string kosong agar tokenizer bisa bekerja dengan data yang valid\n    data['content'] = data['content'].fillna('')\n    data['summary'] = data['summary'].fillna('')\n    \n    # Menambahkan prefix \"summarize: \" pada kolom 'content'\n    data['content_with_prefix'] = 'summarize: ' + data['content']\n    \n    # Tokenisasi kolom 'content_with_prefix' dan 'summary' dengan padding dan attention mask otomatis\n    input_encodings = tokenizer(\n        data['content_with_prefix'].tolist(),\n        max_length=512,\n        padding='max_length',\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n    label_encodings = tokenizer(\n        data['summary'].tolist(),\n        max_length=150,\n        padding='max_length',\n        truncation=True,\n        return_tensors=\"pt\"\n    )\n    \n    # Menyimpan hasil tokenisasi dalam DataFrame untuk melihat hasilnya\n    tokenized_data = pd.DataFrame({\n        'content_with_prefix': data['content_with_prefix'],\n        'input_ids': input_encodings['input_ids'].tolist(),\n        'attention_mask': input_encodings['attention_mask'].tolist(),\n        'labels': label_encodings['input_ids'].tolist()\n    })\n    \n    return tokenized_data\n\n# Proses data untuk train, test, dan val\ntrain_data = process_data(file_paths['train'])\ntest_data = process_data(file_paths['test'])\nval_data = process_data(file_paths['val'])\n\n# Tampilkan hasil tokenisasi sebagai DataFrame tanpa print\ntrain_data.head()  # Tampilkan 5 baris pertama dari data pelatihan","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:34:54.984247Z","iopub.execute_input":"2024-12-16T09:34:54.984640Z","iopub.status.idle":"2024-12-16T09:35:32.073588Z","shell.execute_reply.started":"2024-12-16T09:34:54.984600Z","shell.execute_reply":"2024-12-16T09:35:32.072651Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                 content_with_prefix  \\\n0  summarize: ketua dewan pimpinan wilayah psi ja...   \n1  summarize: pt pelayaran nasional indonesia (pe...   \n2  summarize: shahnaz haque bercerita tentang kro...   \n3  summarize: bank indonesia (bi) mengajak invest...   \n4  summarize: ketua bidang pelayanan pusat kedokt...   \n\n                                           input_ids  \\\n0  [21603, 10, 3, 8044, 76, 9, 20, 3877, 2816, 11...   \n1  [21603, 10, 3, 102, 17, 158, 5595, 9, 2002, 3,...   \n2  [21603, 10, 6660, 9, 107, 29, 9, 172, 4244, 83...   \n3  [21603, 10, 2137, 16, 2029, 15, 7, 23, 9, 41, ...   \n4  [21603, 10, 3, 8044, 76, 9, 6894, 1468, 158, 5...   \n\n                                      attention_mask  \\\n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n\n                                              labels  \n0  [3, 102, 7, 23, 1076, 291, 2782, 3304, 5413, 3...  \n1  [3, 102, 17, 3, 4343, 29, 23, 41, 4660, 49, 32...  \n2  [3, 9, 23692, 2774, 7, 7, 9, 4244, 835, 6, 666...  \n3  [2647, 1076, 122, 9, 1191, 157, 12024, 20576, ...  \n4  [6629, 26, 1825, 7735, 3, 3233, 52, 23, 3, 275...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content_with_prefix</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>summarize: ketua dewan pimpinan wilayah psi ja...</td>\n      <td>[21603, 10, 3, 8044, 76, 9, 20, 3877, 2816, 11...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[3, 102, 7, 23, 1076, 291, 2782, 3304, 5413, 3...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>summarize: pt pelayaran nasional indonesia (pe...</td>\n      <td>[21603, 10, 3, 102, 17, 158, 5595, 9, 2002, 3,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[3, 102, 17, 3, 4343, 29, 23, 41, 4660, 49, 32...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>summarize: shahnaz haque bercerita tentang kro...</td>\n      <td>[21603, 10, 6660, 9, 107, 29, 9, 172, 4244, 83...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[3, 9, 23692, 2774, 7, 7, 9, 4244, 835, 6, 666...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>summarize: bank indonesia (bi) mengajak invest...</td>\n      <td>[21603, 10, 2137, 16, 2029, 15, 7, 23, 9, 41, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[2647, 1076, 122, 9, 1191, 157, 12024, 20576, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>summarize: ketua bidang pelayanan pusat kedokt...</td>\n      <td>[21603, 10, 3, 8044, 76, 9, 6894, 1468, 158, 5...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[6629, 26, 1825, 7735, 3, 3233, 52, 23, 3, 275...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"train_data.to_csv('train_data_tokenized_t5.csv', index=False)\ntest_data.to_csv('test_data_tokenized_t5.csv', index=False)\nval_data.to_csv('val_data_tokenized_t5.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:35:32.074506Z","iopub.execute_input":"2024-12-16T09:35:32.074773Z","iopub.status.idle":"2024-12-16T09:35:34.479631Z","shell.execute_reply.started":"2024-12-16T09:35:32.074748Z","shell.execute_reply":"2024-12-16T09:35:34.479026Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Dataset Class (menggunakan token yang sudah ada)\nclass SummaryDataset(Dataset):\n    def __init__(self, dataframe):\n        self.data = dataframe\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': torch.tensor(self.data.iloc[idx]['input_ids']),\n            'attention_mask': torch.tensor(self.data.iloc[idx]['attention_mask']),\n            'labels': torch.tensor(self.data.iloc[idx]['labels'])\n        }\n\n# Membuat dataset untuk train, validation, dan test set\ntrain_dataset = SummaryDataset(train_data)\nval_dataset = SummaryDataset(val_data)\ntest_dataset = SummaryDataset(test_data)\n\n# Membuat DataLoader untuk batch training\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:35:34.480896Z","iopub.execute_input":"2024-12-16T09:35:34.481583Z","iopub.status.idle":"2024-12-16T09:35:34.487810Z","shell.execute_reply.started":"2024-12-16T09:35:34.481537Z","shell.execute_reply":"2024-12-16T09:35:34.487041Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["65a4b63349854e5798e055b68d2af719","8742205ecccc4b4daf0c51e301077f26","832b8c6a9cc94ecaa7aee2b6425c752b","7c80ec3a8b9f444ca1092be88fb3f203","0a53585282a3448ea5145df1c4bb9a4b","3312d94dd140447fa6b4adde75717dea","feaa4e1753e64e9bb4705385466a708c","ca5855a3a81c4604b37f527ff2df8097","ee678f1395b04f5eae5aa5920477a225","7858630a17504a4c8521450cf57f5e4f","87c6b5f41d7c4447909e4fce265fc052","37ddfba1a7e9415badcc00a55e3f2407","42e0abe6b20a4f26ad520f911fc21505","c0836c8f6f0a4a0aa0079d707e6b67f8","9dda11d80d124cd9a3d5fb77495e2d48","d5c86f70fc054763a91dcd2d4505273a","9ed368d63057420fba470676f96aabe6","87b8a1451c9b491a9290255220678f43","70d12161f56a4dcbb227375830250f92","9b4a4af357f345a2bc013eaaf2193736","8c75ece8c6494a33845efeba3228086a","a3e4c74c125147fc8e45a913bb9b0514"]},"id":"8NzV39Ofuelf","outputId":"8952c019-0507-4582-d9fa-649bc1901638","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:35:34.488942Z","iopub.execute_input":"2024-12-16T09:35:34.489230Z","iopub.status.idle":"2024-12-16T09:35:39.741534Z","shell.execute_reply.started":"2024-12-16T09:35:34.489206Z","shell.execute_reply":"2024-12-16T09:35:39.740622Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60b2a43e44fd4fc49755d1f4b08d0462"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d7e894d2f4249268c595f13c52929b9"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from transformers import AdamW\n\n# Menggunakan Adam optimizer\noptimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n\n# Tentukan jumlah epoch untuk pelatihan\nnum_epochs = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:35:39.742533Z","iopub.execute_input":"2024-12-16T09:35:39.742788Z","iopub.status.idle":"2024-12-16T09:35:39.763482Z","shell.execute_reply.started":"2024-12-16T09:35:39.742764Z","shell.execute_reply":"2024-12-16T09:35:39.762656Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\n\ntrain_losses_per_batch = []\nval_losses_per_batch = []\ntrain_losses = []  # Initialize train_losses here\nval_losses = []  # Initialize val_losses here\n\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n\n    # Training phase\n    model.train()  # Pastikan model dalam mode pelatihan\n    total_train_loss = 0\n    for batch_idx, batch in enumerate(train_dataloader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n        total_train_loss += loss.item()\n        train_losses_per_batch.append(loss.item())  # Menyimpan loss per batch\n\n        # Menampilkan loss per batch\n        if batch_idx % 10 == 0:  # Menampilkan setiap 10 batch\n            print(f\"Batch {batch_idx+1}/{len(train_dataloader)} - Train Loss: {loss.item()}\")\n\n    # Menampilkan loss rata-rata per epoch (Training Loss)\n    avg_train_loss = total_train_loss / len(train_dataloader)\n    train_losses.append(avg_train_loss)\n    print(f\"Epoch {epoch+1} - Average Train Loss: {avg_train_loss}\")\n\n    # Clear GPU memory after training epoch\n    torch.cuda.empty_cache()\n\n    # Validation phase\n    model.eval()  # Pastikan model dalam mode evaluasi\n    total_val_loss = 0\n    with torch.no_grad():\n        for batch_idx, batch in enumerate(val_dataloader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            total_val_loss += loss.item()\n            val_losses_per_batch.append(loss.item())  # Menyimpan loss per batch\n\n            # Menampilkan loss per batch pada validation set\n            if batch_idx % 10 == 0:  # Menampilkan setiap 10 batch\n                print(f\"Batch {batch_idx+1}/{len(val_dataloader)} - Validation Loss: {loss.item()}\")\n\n    # Menampilkan loss rata-rata per epoch (Validation Loss)\n    avg_val_loss = total_val_loss / len(val_dataloader)\n    val_losses.append(avg_val_loss)  # Append to val_losses list\n    print(f\"Epoch {epoch+1} - Average Validation Loss: {avg_val_loss}\")\n\n    # Clear GPU memory after validation epoch\n    torch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T09:35:39.764474Z","iopub.execute_input":"2024-12-16T09:35:39.764721Z","iopub.status.idle":"2024-12-16T13:58:54.283684Z","shell.execute_reply.started":"2024-12-16T09:35:39.764698Z","shell.execute_reply":"2024-12-16T13:58:54.282720Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"name":"stdout","text":"Batch 1/2082 - Train Loss: 8.174301147460938\nBatch 11/2082 - Train Loss: 6.1691575050354\nBatch 21/2082 - Train Loss: 3.628033399581909\nBatch 31/2082 - Train Loss: 2.652179002761841\nBatch 41/2082 - Train Loss: 1.7419002056121826\nBatch 51/2082 - Train Loss: 1.1284793615341187\nBatch 61/2082 - Train Loss: 1.0745275020599365\nBatch 71/2082 - Train Loss: 1.4577007293701172\nBatch 81/2082 - Train Loss: 0.8391144275665283\nBatch 91/2082 - Train Loss: 0.8309460282325745\nBatch 101/2082 - Train Loss: 0.7050058245658875\nBatch 111/2082 - Train Loss: 0.6837393641471863\nBatch 121/2082 - Train Loss: 0.8366195559501648\nBatch 131/2082 - Train Loss: 0.8070188164710999\nBatch 141/2082 - Train Loss: 0.6714670658111572\nBatch 151/2082 - Train Loss: 0.9157434105873108\nBatch 161/2082 - Train Loss: 2.0168793201446533\nBatch 171/2082 - Train Loss: 0.8221359252929688\nBatch 181/2082 - Train Loss: 0.9201052784919739\nBatch 191/2082 - Train Loss: 0.6336273550987244\nBatch 201/2082 - Train Loss: 0.7694923877716064\nBatch 211/2082 - Train Loss: 0.7311195135116577\nBatch 221/2082 - Train Loss: 0.8751468658447266\nBatch 231/2082 - Train Loss: 0.7003170251846313\nBatch 241/2082 - Train Loss: 0.6276260614395142\nBatch 251/2082 - Train Loss: 0.46233707666397095\nBatch 261/2082 - Train Loss: 0.6575686931610107\nBatch 271/2082 - Train Loss: 0.6634534597396851\nBatch 281/2082 - Train Loss: 0.5725806951522827\nBatch 291/2082 - Train Loss: 0.40877440571784973\nBatch 301/2082 - Train Loss: 1.0194587707519531\nBatch 311/2082 - Train Loss: 0.6066693067550659\nBatch 321/2082 - Train Loss: 0.4015323221683502\nBatch 331/2082 - Train Loss: 0.8606982231140137\nBatch 341/2082 - Train Loss: 0.27699804306030273\nBatch 351/2082 - Train Loss: 0.5630737543106079\nBatch 361/2082 - Train Loss: 0.417245090007782\nBatch 371/2082 - Train Loss: 1.2079192399978638\nBatch 381/2082 - Train Loss: 0.34015965461730957\nBatch 391/2082 - Train Loss: 0.40100473165512085\nBatch 401/2082 - Train Loss: 0.5835897922515869\nBatch 411/2082 - Train Loss: 0.4472617208957672\nBatch 421/2082 - Train Loss: 0.526455819606781\nBatch 431/2082 - Train Loss: 0.8449813723564148\nBatch 441/2082 - Train Loss: 0.3734561800956726\nBatch 451/2082 - Train Loss: 0.4189622104167938\nBatch 461/2082 - Train Loss: 1.0087441205978394\nBatch 471/2082 - Train Loss: 0.5318921804428101\nBatch 481/2082 - Train Loss: 0.7848165035247803\nBatch 491/2082 - Train Loss: 1.3916069269180298\nBatch 501/2082 - Train Loss: 0.35246145725250244\nBatch 511/2082 - Train Loss: 0.6366531252861023\nBatch 521/2082 - Train Loss: 0.4215206801891327\nBatch 531/2082 - Train Loss: 0.2855687141418457\nBatch 541/2082 - Train Loss: 0.33456435799598694\nBatch 551/2082 - Train Loss: 0.5347317457199097\nBatch 561/2082 - Train Loss: 0.6758469939231873\nBatch 571/2082 - Train Loss: 1.2746551036834717\nBatch 581/2082 - Train Loss: 0.442254900932312\nBatch 591/2082 - Train Loss: 0.3021465539932251\nBatch 601/2082 - Train Loss: 0.4272018373012543\nBatch 611/2082 - Train Loss: 1.0347554683685303\nBatch 621/2082 - Train Loss: 0.33968421816825867\nBatch 631/2082 - Train Loss: 0.7306188941001892\nBatch 641/2082 - Train Loss: 0.43709707260131836\nBatch 651/2082 - Train Loss: 0.5526660084724426\nBatch 661/2082 - Train Loss: 0.5742907524108887\nBatch 671/2082 - Train Loss: 0.5268475413322449\nBatch 681/2082 - Train Loss: 0.35580989718437195\nBatch 691/2082 - Train Loss: 0.34733709692955017\nBatch 701/2082 - Train Loss: 0.7726260423660278\nBatch 711/2082 - Train Loss: 0.5412632822990417\nBatch 721/2082 - Train Loss: 0.6854798793792725\nBatch 731/2082 - Train Loss: 0.8243833780288696\nBatch 741/2082 - Train Loss: 4.54263162612915\nBatch 751/2082 - Train Loss: 0.46555328369140625\nBatch 761/2082 - Train Loss: 0.6291331052780151\nBatch 771/2082 - Train Loss: 0.5401016473770142\nBatch 781/2082 - Train Loss: 0.3701765537261963\nBatch 791/2082 - Train Loss: 0.4775654673576355\nBatch 801/2082 - Train Loss: 0.7101388573646545\nBatch 811/2082 - Train Loss: 0.5534698367118835\nBatch 821/2082 - Train Loss: 0.5608053803443909\nBatch 831/2082 - Train Loss: 0.6555579304695129\nBatch 841/2082 - Train Loss: 0.9996575713157654\nBatch 851/2082 - Train Loss: 0.650424599647522\nBatch 861/2082 - Train Loss: 0.5363906621932983\nBatch 871/2082 - Train Loss: 0.38625600934028625\nBatch 881/2082 - Train Loss: 0.5273342132568359\nBatch 891/2082 - Train Loss: 0.6861085295677185\nBatch 901/2082 - Train Loss: 0.6538650989532471\nBatch 911/2082 - Train Loss: 0.40133529901504517\nBatch 921/2082 - Train Loss: 0.38211309909820557\nBatch 931/2082 - Train Loss: 0.4767257571220398\nBatch 941/2082 - Train Loss: 0.6685397624969482\nBatch 951/2082 - Train Loss: 0.2998430132865906\nBatch 961/2082 - Train Loss: 0.6803938150405884\nBatch 971/2082 - Train Loss: 0.8631263375282288\nBatch 981/2082 - Train Loss: 0.46192672848701477\nBatch 991/2082 - Train Loss: 0.39412936568260193\nBatch 1001/2082 - Train Loss: 0.6004201173782349\nBatch 1011/2082 - Train Loss: 0.31186729669570923\nBatch 1021/2082 - Train Loss: 0.5124360918998718\nBatch 1031/2082 - Train Loss: 0.35888171195983887\nBatch 1041/2082 - Train Loss: 0.9078338742256165\nBatch 1051/2082 - Train Loss: 0.5537286400794983\nBatch 1061/2082 - Train Loss: 0.4833369851112366\nBatch 1071/2082 - Train Loss: 0.5868033170700073\nBatch 1081/2082 - Train Loss: 0.5569897890090942\nBatch 1091/2082 - Train Loss: 0.5558275580406189\nBatch 1101/2082 - Train Loss: 0.18830294907093048\nBatch 1111/2082 - Train Loss: 0.6581716537475586\nBatch 1121/2082 - Train Loss: 0.6353196501731873\nBatch 1131/2082 - Train Loss: 0.3975902199745178\nBatch 1141/2082 - Train Loss: 0.4975435435771942\nBatch 1151/2082 - Train Loss: 0.5058672428131104\nBatch 1161/2082 - Train Loss: 0.4198084771633148\nBatch 1171/2082 - Train Loss: 0.26086685061454773\nBatch 1181/2082 - Train Loss: 0.3705906569957733\nBatch 1191/2082 - Train Loss: 0.6804144382476807\nBatch 1201/2082 - Train Loss: 0.4682937562465668\nBatch 1211/2082 - Train Loss: 0.28364306688308716\nBatch 1221/2082 - Train Loss: 0.8170525431632996\nBatch 1231/2082 - Train Loss: 0.5383023619651794\nBatch 1241/2082 - Train Loss: 0.2756001651287079\nBatch 1251/2082 - Train Loss: 0.36444514989852905\nBatch 1261/2082 - Train Loss: 0.530832052230835\nBatch 1271/2082 - Train Loss: 0.6658105850219727\nBatch 1281/2082 - Train Loss: 0.36419016122817993\nBatch 1291/2082 - Train Loss: 0.4693506360054016\nBatch 1301/2082 - Train Loss: 0.47402480244636536\nBatch 1311/2082 - Train Loss: 0.5296861529350281\nBatch 1321/2082 - Train Loss: 0.730868935585022\nBatch 1331/2082 - Train Loss: 0.5710464715957642\nBatch 1341/2082 - Train Loss: 0.8347017168998718\nBatch 1351/2082 - Train Loss: 0.4017426669597626\nBatch 1361/2082 - Train Loss: 0.5997048616409302\nBatch 1371/2082 - Train Loss: 0.3281109929084778\nBatch 1381/2082 - Train Loss: 0.4116389751434326\nBatch 1391/2082 - Train Loss: 0.4147922992706299\nBatch 1401/2082 - Train Loss: 0.4233436584472656\nBatch 1411/2082 - Train Loss: 0.8037995100021362\nBatch 1421/2082 - Train Loss: 0.37689438462257385\nBatch 1431/2082 - Train Loss: 0.4702233374118805\nBatch 1441/2082 - Train Loss: 0.2863381803035736\nBatch 1451/2082 - Train Loss: 0.7899953722953796\nBatch 1461/2082 - Train Loss: 2.228346109390259\nBatch 1471/2082 - Train Loss: 0.686795175075531\nBatch 1481/2082 - Train Loss: 0.3331303596496582\nBatch 1491/2082 - Train Loss: 1.151570439338684\nBatch 1501/2082 - Train Loss: 0.9184884428977966\nBatch 1511/2082 - Train Loss: 0.5932325720787048\nBatch 1521/2082 - Train Loss: 0.34424883127212524\nBatch 1531/2082 - Train Loss: 0.648174524307251\nBatch 1541/2082 - Train Loss: 0.4710443615913391\nBatch 1551/2082 - Train Loss: 0.41907986998558044\nBatch 1561/2082 - Train Loss: 0.6239755153656006\nBatch 1571/2082 - Train Loss: 0.35943377017974854\nBatch 1581/2082 - Train Loss: 0.6370618939399719\nBatch 1591/2082 - Train Loss: 0.5365703105926514\nBatch 1601/2082 - Train Loss: 0.3682098090648651\nBatch 1611/2082 - Train Loss: 0.5632606148719788\nBatch 1621/2082 - Train Loss: 0.18055003881454468\nBatch 1631/2082 - Train Loss: 0.38364455103874207\nBatch 1641/2082 - Train Loss: 0.37637388706207275\nBatch 1651/2082 - Train Loss: 0.5955991744995117\nBatch 1661/2082 - Train Loss: 0.5141907334327698\nBatch 1671/2082 - Train Loss: 0.5317943096160889\nBatch 1681/2082 - Train Loss: 0.1498488187789917\nBatch 1691/2082 - Train Loss: 0.3116587698459625\nBatch 1701/2082 - Train Loss: 0.7024640440940857\nBatch 1711/2082 - Train Loss: 0.5972362756729126\nBatch 1721/2082 - Train Loss: 0.5023590326309204\nBatch 1731/2082 - Train Loss: 0.462954044342041\nBatch 1741/2082 - Train Loss: 0.5070657134056091\nBatch 1751/2082 - Train Loss: 0.17070895433425903\nBatch 1761/2082 - Train Loss: 0.3622343838214874\nBatch 1771/2082 - Train Loss: 0.8143534064292908\nBatch 1781/2082 - Train Loss: 0.4237874150276184\nBatch 1791/2082 - Train Loss: 0.6878597736358643\nBatch 1801/2082 - Train Loss: 0.3723292648792267\nBatch 1811/2082 - Train Loss: 0.5059089064598083\nBatch 1821/2082 - Train Loss: 0.4275211989879608\nBatch 1831/2082 - Train Loss: 0.6741493344306946\nBatch 1841/2082 - Train Loss: 0.4316173195838928\nBatch 1851/2082 - Train Loss: 0.538464367389679\nBatch 1861/2082 - Train Loss: 0.7552871704101562\nBatch 1871/2082 - Train Loss: 0.4737189710140228\nBatch 1881/2082 - Train Loss: 0.27636420726776123\nBatch 1891/2082 - Train Loss: 0.6605056524276733\nBatch 1901/2082 - Train Loss: 0.3052181303501129\nBatch 1911/2082 - Train Loss: 0.26211172342300415\nBatch 1921/2082 - Train Loss: 0.7565857172012329\nBatch 1931/2082 - Train Loss: 0.8114590644836426\nBatch 1941/2082 - Train Loss: 0.276058554649353\nBatch 1951/2082 - Train Loss: 0.8378986716270447\nBatch 1961/2082 - Train Loss: 0.5117545127868652\nBatch 1971/2082 - Train Loss: 0.5513242483139038\nBatch 1981/2082 - Train Loss: 0.5217074751853943\nBatch 1991/2082 - Train Loss: 0.34694889187812805\nBatch 2001/2082 - Train Loss: 0.8444803357124329\nBatch 2011/2082 - Train Loss: 0.3720238506793976\nBatch 2021/2082 - Train Loss: 0.42147237062454224\nBatch 2031/2082 - Train Loss: 0.5214248895645142\nBatch 2041/2082 - Train Loss: 0.6237345933914185\nBatch 2051/2082 - Train Loss: 0.7809788584709167\nBatch 2061/2082 - Train Loss: 0.28676775097846985\nBatch 2071/2082 - Train Loss: 0.5238986015319824\nBatch 2081/2082 - Train Loss: 0.6934452652931213\nEpoch 1 - Average Train Loss: 0.6419490835415584\nBatch 1/261 - Validation Loss: 0.07445007562637329\nBatch 11/261 - Validation Loss: 0.5390588045120239\nBatch 21/261 - Validation Loss: 0.3463810682296753\nBatch 31/261 - Validation Loss: 0.38197046518325806\nBatch 41/261 - Validation Loss: 0.5304094552993774\nBatch 51/261 - Validation Loss: 0.438199907541275\nBatch 61/261 - Validation Loss: 0.2760401964187622\nBatch 71/261 - Validation Loss: 0.19155806303024292\nBatch 81/261 - Validation Loss: 0.56634920835495\nBatch 91/261 - Validation Loss: 0.5692950487136841\nBatch 101/261 - Validation Loss: 0.41417810320854187\nBatch 111/261 - Validation Loss: 0.5915548205375671\nBatch 121/261 - Validation Loss: 0.24532181024551392\nBatch 131/261 - Validation Loss: 0.7490041255950928\nBatch 141/261 - Validation Loss: 0.694793164730072\nBatch 151/261 - Validation Loss: 0.5769340395927429\nBatch 161/261 - Validation Loss: 0.40147829055786133\nBatch 171/261 - Validation Loss: 0.7793816924095154\nBatch 181/261 - Validation Loss: 0.4277878701686859\nBatch 191/261 - Validation Loss: 0.5200134515762329\nBatch 201/261 - Validation Loss: 0.8574852347373962\nBatch 211/261 - Validation Loss: 0.3154262602329254\nBatch 221/261 - Validation Loss: 0.1476796269416809\nBatch 231/261 - Validation Loss: 0.15746940672397614\nBatch 241/261 - Validation Loss: 0.2707686126232147\nBatch 251/261 - Validation Loss: 0.734243631362915\nBatch 261/261 - Validation Loss: 0.029819440096616745\nEpoch 1 - Average Validation Loss: 0.4493153217280733\nEpoch 2/10\nBatch 1/2082 - Train Loss: 0.413941890001297\nBatch 11/2082 - Train Loss: 0.4969400465488434\nBatch 21/2082 - Train Loss: 0.8578802347183228\nBatch 31/2082 - Train Loss: 0.7081092596054077\nBatch 41/2082 - Train Loss: 0.09669370204210281\nBatch 51/2082 - Train Loss: 0.4181176424026489\nBatch 61/2082 - Train Loss: 0.825619101524353\nBatch 71/2082 - Train Loss: 0.34412282705307007\nBatch 81/2082 - Train Loss: 0.49190255999565125\nBatch 91/2082 - Train Loss: 0.5388737320899963\nBatch 101/2082 - Train Loss: 0.5629823207855225\nBatch 111/2082 - Train Loss: 0.2692483961582184\nBatch 121/2082 - Train Loss: 0.4540172815322876\nBatch 131/2082 - Train Loss: 0.5051366686820984\nBatch 141/2082 - Train Loss: 0.5708987712860107\nBatch 151/2082 - Train Loss: 0.20533670485019684\nBatch 161/2082 - Train Loss: 0.3470441401004791\nBatch 171/2082 - Train Loss: 0.3147723972797394\nBatch 181/2082 - Train Loss: 0.23661087453365326\nBatch 191/2082 - Train Loss: 0.33144938945770264\nBatch 201/2082 - Train Loss: 0.335555762052536\nBatch 211/2082 - Train Loss: 0.8245212435722351\nBatch 221/2082 - Train Loss: 0.7221270203590393\nBatch 231/2082 - Train Loss: 0.5988940596580505\nBatch 241/2082 - Train Loss: 0.9758970141410828\nBatch 251/2082 - Train Loss: 0.91973477602005\nBatch 261/2082 - Train Loss: 0.6994308233261108\nBatch 271/2082 - Train Loss: 0.3436814844608307\nBatch 281/2082 - Train Loss: 0.7411794662475586\nBatch 291/2082 - Train Loss: 0.6292175054550171\nBatch 301/2082 - Train Loss: 0.5891801714897156\nBatch 311/2082 - Train Loss: 0.5791303515434265\nBatch 321/2082 - Train Loss: 0.4303589463233948\nBatch 331/2082 - Train Loss: 0.5199973583221436\nBatch 341/2082 - Train Loss: 0.504375159740448\nBatch 351/2082 - Train Loss: 0.3945080041885376\nBatch 361/2082 - Train Loss: 0.1487654745578766\nBatch 371/2082 - Train Loss: 0.8153095245361328\nBatch 381/2082 - Train Loss: 0.3929900825023651\nBatch 391/2082 - Train Loss: 0.3103640377521515\nBatch 401/2082 - Train Loss: 0.37865838408470154\nBatch 411/2082 - Train Loss: 0.5915865898132324\nBatch 421/2082 - Train Loss: 0.4254898130893707\nBatch 431/2082 - Train Loss: 0.26159071922302246\nBatch 441/2082 - Train Loss: 0.4262685775756836\nBatch 451/2082 - Train Loss: 0.3411790132522583\nBatch 461/2082 - Train Loss: 0.3080892264842987\nBatch 471/2082 - Train Loss: 0.3637625575065613\nBatch 481/2082 - Train Loss: 0.552782416343689\nBatch 491/2082 - Train Loss: 0.532017707824707\nBatch 501/2082 - Train Loss: 0.574993371963501\nBatch 511/2082 - Train Loss: 0.36312204599380493\nBatch 521/2082 - Train Loss: 0.39250802993774414\nBatch 531/2082 - Train Loss: 0.517055332660675\nBatch 541/2082 - Train Loss: 0.4192156493663788\nBatch 551/2082 - Train Loss: 0.3847311735153198\nBatch 561/2082 - Train Loss: 0.4502294957637787\nBatch 571/2082 - Train Loss: 0.5521478056907654\nBatch 581/2082 - Train Loss: 0.4519309997558594\nBatch 591/2082 - Train Loss: 0.4565814733505249\nBatch 601/2082 - Train Loss: 0.5492706298828125\nBatch 611/2082 - Train Loss: 0.8665602803230286\nBatch 621/2082 - Train Loss: 0.5122880339622498\nBatch 631/2082 - Train Loss: 0.8506937623023987\nBatch 641/2082 - Train Loss: 0.5275188684463501\nBatch 651/2082 - Train Loss: 0.2093912810087204\nBatch 661/2082 - Train Loss: 0.19058041274547577\nBatch 671/2082 - Train Loss: 0.22670522332191467\nBatch 681/2082 - Train Loss: 0.489045649766922\nBatch 691/2082 - Train Loss: 0.732645571231842\nBatch 701/2082 - Train Loss: 0.37343356013298035\nBatch 711/2082 - Train Loss: 0.21122272312641144\nBatch 721/2082 - Train Loss: 0.7495341300964355\nBatch 731/2082 - Train Loss: 0.6392896175384521\nBatch 741/2082 - Train Loss: 0.13821406662464142\nBatch 751/2082 - Train Loss: 0.6730697751045227\nBatch 761/2082 - Train Loss: 0.7515791654586792\nBatch 771/2082 - Train Loss: 0.7876550555229187\nBatch 781/2082 - Train Loss: 0.5208475589752197\nBatch 791/2082 - Train Loss: 0.5026389956474304\nBatch 801/2082 - Train Loss: 0.6146830916404724\nBatch 811/2082 - Train Loss: 0.6227068305015564\nBatch 821/2082 - Train Loss: 0.4692532420158386\nBatch 831/2082 - Train Loss: 0.23927077651023865\nBatch 841/2082 - Train Loss: 0.5020185112953186\nBatch 851/2082 - Train Loss: 0.4113175570964813\nBatch 861/2082 - Train Loss: 0.47891953587532043\nBatch 871/2082 - Train Loss: 0.4921140670776367\nBatch 881/2082 - Train Loss: 0.5161764025688171\nBatch 891/2082 - Train Loss: 0.4650406539440155\nBatch 901/2082 - Train Loss: 0.4390958249568939\nBatch 911/2082 - Train Loss: 0.4370114207267761\nBatch 921/2082 - Train Loss: 0.5050950050354004\nBatch 931/2082 - Train Loss: 0.2655491530895233\nBatch 941/2082 - Train Loss: 0.39955079555511475\nBatch 951/2082 - Train Loss: 0.3458811938762665\nBatch 961/2082 - Train Loss: 0.26050353050231934\nBatch 971/2082 - Train Loss: 0.5948681235313416\nBatch 981/2082 - Train Loss: 0.34033966064453125\nBatch 991/2082 - Train Loss: 0.47524017095565796\nBatch 1001/2082 - Train Loss: 0.29243943095207214\nBatch 1011/2082 - Train Loss: 0.9720907807350159\nBatch 1021/2082 - Train Loss: 0.24606165289878845\nBatch 1031/2082 - Train Loss: 0.1855771392583847\nBatch 1041/2082 - Train Loss: 0.69636470079422\nBatch 1051/2082 - Train Loss: 0.7232108116149902\nBatch 1061/2082 - Train Loss: 0.6688475608825684\nBatch 1071/2082 - Train Loss: 0.1338067203760147\nBatch 1081/2082 - Train Loss: 0.43908268213272095\nBatch 1091/2082 - Train Loss: 0.3449566066265106\nBatch 1101/2082 - Train Loss: 0.3848016858100891\nBatch 1111/2082 - Train Loss: 0.2618778347969055\nBatch 1121/2082 - Train Loss: 0.4295505881309509\nBatch 1131/2082 - Train Loss: 0.5660009980201721\nBatch 1141/2082 - Train Loss: 0.3950926959514618\nBatch 1151/2082 - Train Loss: 0.21040505170822144\nBatch 1161/2082 - Train Loss: 0.6803640723228455\nBatch 1171/2082 - Train Loss: 0.2939724028110504\nBatch 1181/2082 - Train Loss: 0.6642758846282959\nBatch 1191/2082 - Train Loss: 0.4440465271472931\nBatch 1201/2082 - Train Loss: 0.09057282656431198\nBatch 1211/2082 - Train Loss: 0.33571648597717285\nBatch 1221/2082 - Train Loss: 0.5878108739852905\nBatch 1231/2082 - Train Loss: 0.4751445949077606\nBatch 1241/2082 - Train Loss: 0.7052693963050842\nBatch 1251/2082 - Train Loss: 0.8519343733787537\nBatch 1261/2082 - Train Loss: 0.38821470737457275\nBatch 1271/2082 - Train Loss: 0.39260801672935486\nBatch 1281/2082 - Train Loss: 0.6609978079795837\nBatch 1291/2082 - Train Loss: 0.48317626118659973\nBatch 1301/2082 - Train Loss: 0.26753970980644226\nBatch 1311/2082 - Train Loss: 0.6649341583251953\nBatch 1321/2082 - Train Loss: 0.34699535369873047\nBatch 1331/2082 - Train Loss: 0.13454212248325348\nBatch 1341/2082 - Train Loss: 0.5607338547706604\nBatch 1351/2082 - Train Loss: 0.5704565644264221\nBatch 1361/2082 - Train Loss: 0.43559885025024414\nBatch 1371/2082 - Train Loss: 0.7579691410064697\nBatch 1381/2082 - Train Loss: 0.5493385195732117\nBatch 1391/2082 - Train Loss: 0.311300128698349\nBatch 1401/2082 - Train Loss: 0.575765073299408\nBatch 1411/2082 - Train Loss: 0.797153115272522\nBatch 1421/2082 - Train Loss: 0.21808229386806488\nBatch 1431/2082 - Train Loss: 0.42436546087265015\nBatch 1441/2082 - Train Loss: 0.3292655646800995\nBatch 1451/2082 - Train Loss: 0.5984055995941162\nBatch 1461/2082 - Train Loss: 0.5395494103431702\nBatch 1471/2082 - Train Loss: 0.4823494553565979\nBatch 1481/2082 - Train Loss: 0.3599139153957367\nBatch 1491/2082 - Train Loss: 0.631956934928894\nBatch 1501/2082 - Train Loss: 0.6079901456832886\nBatch 1511/2082 - Train Loss: 0.17417947947978973\nBatch 1521/2082 - Train Loss: 0.5172439217567444\nBatch 1531/2082 - Train Loss: 0.41375723481178284\nBatch 1541/2082 - Train Loss: 0.6648712158203125\nBatch 1551/2082 - Train Loss: 0.4990842640399933\nBatch 1561/2082 - Train Loss: 0.5107610821723938\nBatch 1571/2082 - Train Loss: 0.635840117931366\nBatch 1581/2082 - Train Loss: 0.6372658610343933\nBatch 1591/2082 - Train Loss: 0.6755198240280151\nBatch 1601/2082 - Train Loss: 0.6364359855651855\nBatch 1611/2082 - Train Loss: 0.4461135268211365\nBatch 1621/2082 - Train Loss: 0.2897436320781708\nBatch 1631/2082 - Train Loss: 0.49026742577552795\nBatch 1641/2082 - Train Loss: 0.5834506750106812\nBatch 1651/2082 - Train Loss: 0.28305739164352417\nBatch 1661/2082 - Train Loss: 0.7863114476203918\nBatch 1671/2082 - Train Loss: 0.430998831987381\nBatch 1681/2082 - Train Loss: 0.6661901473999023\nBatch 1691/2082 - Train Loss: 0.4100978374481201\nBatch 1701/2082 - Train Loss: 0.40689611434936523\nBatch 1711/2082 - Train Loss: 0.4329238831996918\nBatch 1721/2082 - Train Loss: 0.2071397602558136\nBatch 1731/2082 - Train Loss: 0.3291396200656891\nBatch 1741/2082 - Train Loss: 0.4793316125869751\nBatch 1751/2082 - Train Loss: 0.3794943392276764\nBatch 1761/2082 - Train Loss: 0.2403935194015503\nBatch 1771/2082 - Train Loss: 0.496176540851593\nBatch 1781/2082 - Train Loss: 0.25891441106796265\nBatch 1791/2082 - Train Loss: 0.5576055645942688\nBatch 1801/2082 - Train Loss: 0.4204202890396118\nBatch 1811/2082 - Train Loss: 0.30047863721847534\nBatch 1821/2082 - Train Loss: 0.2277442216873169\nBatch 1831/2082 - Train Loss: 0.33879974484443665\nBatch 1841/2082 - Train Loss: 0.8381496667861938\nBatch 1851/2082 - Train Loss: 0.4670959413051605\nBatch 1861/2082 - Train Loss: 0.2970329523086548\nBatch 1871/2082 - Train Loss: 0.20387999713420868\nBatch 1881/2082 - Train Loss: 0.2846565842628479\nBatch 1891/2082 - Train Loss: 0.5897075533866882\nBatch 1901/2082 - Train Loss: 0.3497677445411682\nBatch 1911/2082 - Train Loss: 0.5746405124664307\nBatch 1921/2082 - Train Loss: 0.6075159907341003\nBatch 1931/2082 - Train Loss: 0.6721006035804749\nBatch 1941/2082 - Train Loss: 0.3779667615890503\nBatch 1951/2082 - Train Loss: 0.3842645287513733\nBatch 1961/2082 - Train Loss: 0.42996761202812195\nBatch 1971/2082 - Train Loss: 0.2981034815311432\nBatch 1981/2082 - Train Loss: 0.6158369183540344\nBatch 1991/2082 - Train Loss: 0.1931348592042923\nBatch 2001/2082 - Train Loss: 0.42453619837760925\nBatch 2011/2082 - Train Loss: 0.603428840637207\nBatch 2021/2082 - Train Loss: 0.2540018856525421\nBatch 2031/2082 - Train Loss: 0.3448091149330139\nBatch 2041/2082 - Train Loss: 0.3608635365962982\nBatch 2051/2082 - Train Loss: 0.39110836386680603\nBatch 2061/2082 - Train Loss: 0.6628545522689819\nBatch 2071/2082 - Train Loss: 0.2705325186252594\nBatch 2081/2082 - Train Loss: 0.48254039883613586\nEpoch 2 - Average Train Loss: 0.47235819763404663\nBatch 1/261 - Validation Loss: 0.07447142899036407\nBatch 11/261 - Validation Loss: 0.5107776522636414\nBatch 21/261 - Validation Loss: 0.3215790390968323\nBatch 31/261 - Validation Loss: 0.358062207698822\nBatch 41/261 - Validation Loss: 0.49987706542015076\nBatch 51/261 - Validation Loss: 0.42730677127838135\nBatch 61/261 - Validation Loss: 0.2656780183315277\nBatch 71/261 - Validation Loss: 0.1755426973104477\nBatch 81/261 - Validation Loss: 0.5391858220100403\nBatch 91/261 - Validation Loss: 0.5350903272628784\nBatch 101/261 - Validation Loss: 0.3888353109359741\nBatch 111/261 - Validation Loss: 0.5581650733947754\nBatch 121/261 - Validation Loss: 0.22074353694915771\nBatch 131/261 - Validation Loss: 0.7169170379638672\nBatch 141/261 - Validation Loss: 0.6790481805801392\nBatch 151/261 - Validation Loss: 0.5487726330757141\nBatch 161/261 - Validation Loss: 0.3781982362270355\nBatch 171/261 - Validation Loss: 0.7540203332901001\nBatch 181/261 - Validation Loss: 0.4090495705604553\nBatch 191/261 - Validation Loss: 0.5114862322807312\nBatch 201/261 - Validation Loss: 0.8200333118438721\nBatch 211/261 - Validation Loss: 0.2942340672016144\nBatch 221/261 - Validation Loss: 0.13976052403450012\nBatch 231/261 - Validation Loss: 0.15365248918533325\nBatch 241/261 - Validation Loss: 0.25848063826560974\nBatch 251/261 - Validation Loss: 0.6953601837158203\nBatch 261/261 - Validation Loss: 0.03897271677851677\nEpoch 2 - Average Validation Loss: 0.42742460642109886\nEpoch 3/10\nBatch 1/2082 - Train Loss: 0.40346646308898926\nBatch 11/2082 - Train Loss: 0.5941822528839111\nBatch 21/2082 - Train Loss: 0.45499947667121887\nBatch 31/2082 - Train Loss: 0.3883092701435089\nBatch 41/2082 - Train Loss: 0.5143766403198242\nBatch 51/2082 - Train Loss: 0.2993435859680176\nBatch 61/2082 - Train Loss: 0.4623654782772064\nBatch 71/2082 - Train Loss: 0.41200053691864014\nBatch 81/2082 - Train Loss: 0.3300457298755646\nBatch 91/2082 - Train Loss: 0.36192086338996887\nBatch 101/2082 - Train Loss: 0.44321802258491516\nBatch 111/2082 - Train Loss: 0.4202469289302826\nBatch 121/2082 - Train Loss: 0.5151771306991577\nBatch 131/2082 - Train Loss: 0.3120906352996826\nBatch 141/2082 - Train Loss: 0.3629666566848755\nBatch 151/2082 - Train Loss: 0.6725278496742249\nBatch 161/2082 - Train Loss: 0.3979562520980835\nBatch 171/2082 - Train Loss: 0.4391515552997589\nBatch 181/2082 - Train Loss: 0.33925989270210266\nBatch 191/2082 - Train Loss: 0.46490275859832764\nBatch 201/2082 - Train Loss: 0.5778079628944397\nBatch 211/2082 - Train Loss: 0.6247299313545227\nBatch 221/2082 - Train Loss: 0.14012455940246582\nBatch 231/2082 - Train Loss: 0.4481048583984375\nBatch 241/2082 - Train Loss: 0.6841848492622375\nBatch 251/2082 - Train Loss: 0.5473915934562683\nBatch 261/2082 - Train Loss: 0.7374356985092163\nBatch 271/2082 - Train Loss: 0.29302459955215454\nBatch 281/2082 - Train Loss: 0.7029775381088257\nBatch 291/2082 - Train Loss: 0.8978013396263123\nBatch 301/2082 - Train Loss: 0.6200991272926331\nBatch 311/2082 - Train Loss: 0.3470570147037506\nBatch 321/2082 - Train Loss: 0.2516632378101349\nBatch 331/2082 - Train Loss: 0.6764179468154907\nBatch 341/2082 - Train Loss: 0.36816707253456116\nBatch 351/2082 - Train Loss: 0.18958981335163116\nBatch 361/2082 - Train Loss: 0.3921475410461426\nBatch 371/2082 - Train Loss: 0.5328637957572937\nBatch 381/2082 - Train Loss: 0.608329713344574\nBatch 391/2082 - Train Loss: 0.39811256527900696\nBatch 401/2082 - Train Loss: 0.490052193403244\nBatch 411/2082 - Train Loss: 0.7218658924102783\nBatch 421/2082 - Train Loss: 0.48853129148483276\nBatch 431/2082 - Train Loss: 0.6869765520095825\nBatch 441/2082 - Train Loss: 0.4608413577079773\nBatch 451/2082 - Train Loss: 0.3054118752479553\nBatch 461/2082 - Train Loss: 0.1311422437429428\nBatch 471/2082 - Train Loss: 0.459300696849823\nBatch 481/2082 - Train Loss: 0.26690077781677246\nBatch 491/2082 - Train Loss: 0.4153251647949219\nBatch 501/2082 - Train Loss: 0.555357813835144\nBatch 511/2082 - Train Loss: 0.45323657989501953\nBatch 521/2082 - Train Loss: 0.23680172860622406\nBatch 531/2082 - Train Loss: 0.19635748863220215\nBatch 541/2082 - Train Loss: 0.2414141297340393\nBatch 551/2082 - Train Loss: 0.5034700036048889\nBatch 561/2082 - Train Loss: 0.37033548951148987\nBatch 571/2082 - Train Loss: 0.46649473905563354\nBatch 581/2082 - Train Loss: 0.2508963644504547\nBatch 591/2082 - Train Loss: 0.6291306018829346\nBatch 601/2082 - Train Loss: 0.16569745540618896\nBatch 611/2082 - Train Loss: 0.18680967390537262\nBatch 621/2082 - Train Loss: 0.6219496130943298\nBatch 631/2082 - Train Loss: 0.5675565004348755\nBatch 641/2082 - Train Loss: 0.26335516571998596\nBatch 651/2082 - Train Loss: 0.5857870578765869\nBatch 661/2082 - Train Loss: 0.16371150314807892\nBatch 671/2082 - Train Loss: 0.5678637027740479\nBatch 681/2082 - Train Loss: 0.5302811861038208\nBatch 691/2082 - Train Loss: 0.6073245406150818\nBatch 701/2082 - Train Loss: 0.6822088360786438\nBatch 711/2082 - Train Loss: 0.23892298340797424\nBatch 721/2082 - Train Loss: 0.5080257654190063\nBatch 731/2082 - Train Loss: 0.5718479156494141\nBatch 741/2082 - Train Loss: 0.525175154209137\nBatch 751/2082 - Train Loss: 0.3695403039455414\nBatch 761/2082 - Train Loss: 0.5310108065605164\nBatch 771/2082 - Train Loss: 0.338022381067276\nBatch 781/2082 - Train Loss: 0.36798185110092163\nBatch 791/2082 - Train Loss: 0.3181034028530121\nBatch 801/2082 - Train Loss: 0.5460928678512573\nBatch 811/2082 - Train Loss: 0.5825642347335815\nBatch 821/2082 - Train Loss: 0.8776678442955017\nBatch 831/2082 - Train Loss: 0.38775303959846497\nBatch 841/2082 - Train Loss: 0.29550257325172424\nBatch 851/2082 - Train Loss: 0.3996763825416565\nBatch 861/2082 - Train Loss: 0.2157912403345108\nBatch 871/2082 - Train Loss: 0.1899806261062622\nBatch 881/2082 - Train Loss: 0.24485516548156738\nBatch 891/2082 - Train Loss: 0.2754657566547394\nBatch 901/2082 - Train Loss: 0.44145700335502625\nBatch 911/2082 - Train Loss: 0.4161785840988159\nBatch 921/2082 - Train Loss: 0.1342470794916153\nBatch 931/2082 - Train Loss: 0.616361141204834\nBatch 941/2082 - Train Loss: 0.5541951060295105\nBatch 951/2082 - Train Loss: 0.3987438976764679\nBatch 961/2082 - Train Loss: 0.7491028308868408\nBatch 971/2082 - Train Loss: 0.46878305077552795\nBatch 981/2082 - Train Loss: 0.5182466506958008\nBatch 991/2082 - Train Loss: 0.5765570402145386\nBatch 1001/2082 - Train Loss: 0.3577280044555664\nBatch 1011/2082 - Train Loss: 0.3920770585536957\nBatch 1021/2082 - Train Loss: 0.34223100543022156\nBatch 1031/2082 - Train Loss: 0.3488769829273224\nBatch 1041/2082 - Train Loss: 0.6509759426116943\nBatch 1051/2082 - Train Loss: 0.3185238540172577\nBatch 1061/2082 - Train Loss: 0.08324113488197327\nBatch 1071/2082 - Train Loss: 0.2065678983926773\nBatch 1081/2082 - Train Loss: 0.5028074383735657\nBatch 1091/2082 - Train Loss: 0.7533101439476013\nBatch 1101/2082 - Train Loss: 0.3772726058959961\nBatch 1111/2082 - Train Loss: 0.3864407241344452\nBatch 1121/2082 - Train Loss: 0.461442768573761\nBatch 1131/2082 - Train Loss: 0.4864254295825958\nBatch 1141/2082 - Train Loss: 0.5526381134986877\nBatch 1151/2082 - Train Loss: 0.5890616774559021\nBatch 1161/2082 - Train Loss: 0.5361784100532532\nBatch 1171/2082 - Train Loss: 0.5389172434806824\nBatch 1181/2082 - Train Loss: 0.4833374619483948\nBatch 1191/2082 - Train Loss: 0.4304981529712677\nBatch 1201/2082 - Train Loss: 0.4005783200263977\nBatch 1211/2082 - Train Loss: 0.4019225835800171\nBatch 1221/2082 - Train Loss: 0.31619521975517273\nBatch 1231/2082 - Train Loss: 0.45336857438087463\nBatch 1241/2082 - Train Loss: 0.4382578134536743\nBatch 1251/2082 - Train Loss: 0.48545655608177185\nBatch 1261/2082 - Train Loss: 0.41935107111930847\nBatch 1271/2082 - Train Loss: 0.26687517762184143\nBatch 1281/2082 - Train Loss: 0.3994802236557007\nBatch 1291/2082 - Train Loss: 0.5629851818084717\nBatch 1301/2082 - Train Loss: 0.3361666202545166\nBatch 1311/2082 - Train Loss: 0.4194435775279999\nBatch 1321/2082 - Train Loss: 0.29704853892326355\nBatch 1331/2082 - Train Loss: 0.7282259464263916\nBatch 1341/2082 - Train Loss: 0.43442341685295105\nBatch 1351/2082 - Train Loss: 0.39144185185432434\nBatch 1361/2082 - Train Loss: 0.5835698843002319\nBatch 1371/2082 - Train Loss: 0.614892303943634\nBatch 1381/2082 - Train Loss: 0.45326951146125793\nBatch 1391/2082 - Train Loss: 0.5825120806694031\nBatch 1401/2082 - Train Loss: 0.43243128061294556\nBatch 1411/2082 - Train Loss: 0.33900341391563416\nBatch 1421/2082 - Train Loss: 0.5190382599830627\nBatch 1431/2082 - Train Loss: 0.2771945297718048\nBatch 1441/2082 - Train Loss: 0.5112502574920654\nBatch 1451/2082 - Train Loss: 0.3788294792175293\nBatch 1461/2082 - Train Loss: 0.6694605350494385\nBatch 1471/2082 - Train Loss: 0.26651936769485474\nBatch 1481/2082 - Train Loss: 0.40126436948776245\nBatch 1491/2082 - Train Loss: 0.16438740491867065\nBatch 1501/2082 - Train Loss: 0.1357213407754898\nBatch 1511/2082 - Train Loss: 0.3705669343471527\nBatch 1521/2082 - Train Loss: 0.7038884162902832\nBatch 1531/2082 - Train Loss: 0.4002913534641266\nBatch 1541/2082 - Train Loss: 0.35940277576446533\nBatch 1551/2082 - Train Loss: 0.31967848539352417\nBatch 1561/2082 - Train Loss: 0.6735720038414001\nBatch 1571/2082 - Train Loss: 0.6743605136871338\nBatch 1581/2082 - Train Loss: 0.7263877391815186\nBatch 1591/2082 - Train Loss: 0.911429762840271\nBatch 1601/2082 - Train Loss: 0.5078615546226501\nBatch 1611/2082 - Train Loss: 0.19587983191013336\nBatch 1621/2082 - Train Loss: 0.23048578202724457\nBatch 1631/2082 - Train Loss: 0.6503634452819824\nBatch 1641/2082 - Train Loss: 0.26749277114868164\nBatch 1651/2082 - Train Loss: 0.48046329617500305\nBatch 1661/2082 - Train Loss: 0.3896747827529907\nBatch 1671/2082 - Train Loss: 0.6804649829864502\nBatch 1681/2082 - Train Loss: 0.6054024696350098\nBatch 1691/2082 - Train Loss: 0.5811660289764404\nBatch 1701/2082 - Train Loss: 0.7464465498924255\nBatch 1711/2082 - Train Loss: 0.6442685127258301\nBatch 1721/2082 - Train Loss: 0.5786746144294739\nBatch 1731/2082 - Train Loss: 0.5535123348236084\nBatch 1741/2082 - Train Loss: 0.43171805143356323\nBatch 1751/2082 - Train Loss: 0.7027872800827026\nBatch 1761/2082 - Train Loss: 0.47204020619392395\nBatch 1771/2082 - Train Loss: 0.5040450692176819\nBatch 1781/2082 - Train Loss: 0.5365777015686035\nBatch 1791/2082 - Train Loss: 0.5655481219291687\nBatch 1801/2082 - Train Loss: 0.19276529550552368\nBatch 1811/2082 - Train Loss: 0.5003937482833862\nBatch 1821/2082 - Train Loss: 0.5188591480255127\nBatch 1831/2082 - Train Loss: 0.3791722059249878\nBatch 1841/2082 - Train Loss: 0.5055121779441833\nBatch 1851/2082 - Train Loss: 0.6479518413543701\nBatch 1861/2082 - Train Loss: 0.44948291778564453\nBatch 1871/2082 - Train Loss: 0.22522783279418945\nBatch 1881/2082 - Train Loss: 0.7247191667556763\nBatch 1891/2082 - Train Loss: 0.6004828810691833\nBatch 1901/2082 - Train Loss: 0.550544261932373\nBatch 1911/2082 - Train Loss: 0.5659475922584534\nBatch 1921/2082 - Train Loss: 0.4749336242675781\nBatch 1931/2082 - Train Loss: 0.8476158976554871\nBatch 1941/2082 - Train Loss: 0.3641606271266937\nBatch 1951/2082 - Train Loss: 0.5620012879371643\nBatch 1961/2082 - Train Loss: 0.9915655255317688\nBatch 1971/2082 - Train Loss: 0.4915730655193329\nBatch 1981/2082 - Train Loss: 0.26590418815612793\nBatch 1991/2082 - Train Loss: 0.34089499711990356\nBatch 2001/2082 - Train Loss: 0.3393397629261017\nBatch 2011/2082 - Train Loss: 0.24351324141025543\nBatch 2021/2082 - Train Loss: 0.8221699595451355\nBatch 2031/2082 - Train Loss: 0.5151873826980591\nBatch 2041/2082 - Train Loss: 0.5195701718330383\nBatch 2051/2082 - Train Loss: 0.6769932508468628\nBatch 2061/2082 - Train Loss: 0.3519669771194458\nBatch 2071/2082 - Train Loss: 0.41705122590065\nBatch 2081/2082 - Train Loss: 0.2984471619129181\nEpoch 3 - Average Train Loss: 0.4502090936764631\nBatch 1/261 - Validation Loss: 0.07632678002119064\nBatch 11/261 - Validation Loss: 0.49442967772483826\nBatch 21/261 - Validation Loss: 0.30983152985572815\nBatch 31/261 - Validation Loss: 0.34471938014030457\nBatch 41/261 - Validation Loss: 0.4826199412345886\nBatch 51/261 - Validation Loss: 0.41934940218925476\nBatch 61/261 - Validation Loss: 0.26476529240608215\nBatch 71/261 - Validation Loss: 0.16763617098331451\nBatch 81/261 - Validation Loss: 0.5298995971679688\nBatch 91/261 - Validation Loss: 0.5226449370384216\nBatch 101/261 - Validation Loss: 0.3911948502063751\nBatch 111/261 - Validation Loss: 0.5412141680717468\nBatch 121/261 - Validation Loss: 0.21457552909851074\nBatch 131/261 - Validation Loss: 0.7080389857292175\nBatch 141/261 - Validation Loss: 0.664191722869873\nBatch 151/261 - Validation Loss: 0.5459119081497192\nBatch 161/261 - Validation Loss: 0.3707176744937897\nBatch 171/261 - Validation Loss: 0.732421338558197\nBatch 181/261 - Validation Loss: 0.39748114347457886\nBatch 191/261 - Validation Loss: 0.5052859783172607\nBatch 201/261 - Validation Loss: 0.8132230639457703\nBatch 211/261 - Validation Loss: 0.2820730209350586\nBatch 221/261 - Validation Loss: 0.13499519228935242\nBatch 231/261 - Validation Loss: 0.1549854874610901\nBatch 241/261 - Validation Loss: 0.2615475356578827\nBatch 251/261 - Validation Loss: 0.6969217658042908\nBatch 261/261 - Validation Loss: 0.03825429081916809\nEpoch 3 - Average Validation Loss: 0.4186750232773722\nEpoch 4/10\nBatch 1/2082 - Train Loss: 0.2001817524433136\nBatch 11/2082 - Train Loss: 0.5974268913269043\nBatch 21/2082 - Train Loss: 0.5849485993385315\nBatch 31/2082 - Train Loss: 0.14184941351413727\nBatch 41/2082 - Train Loss: 0.5751044750213623\nBatch 51/2082 - Train Loss: 0.41262760758399963\nBatch 61/2082 - Train Loss: 0.28518763184547424\nBatch 71/2082 - Train Loss: 0.4909000098705292\nBatch 81/2082 - Train Loss: 0.16408543288707733\nBatch 91/2082 - Train Loss: 0.434961199760437\nBatch 101/2082 - Train Loss: 0.45431843400001526\nBatch 111/2082 - Train Loss: 0.41760018467903137\nBatch 121/2082 - Train Loss: 0.5002452731132507\nBatch 131/2082 - Train Loss: 0.34998416900634766\nBatch 141/2082 - Train Loss: 0.5117099285125732\nBatch 151/2082 - Train Loss: 0.36030635237693787\nBatch 161/2082 - Train Loss: 0.4964413046836853\nBatch 171/2082 - Train Loss: 0.2682975232601166\nBatch 181/2082 - Train Loss: 0.28524312376976013\nBatch 191/2082 - Train Loss: 0.23089894652366638\nBatch 201/2082 - Train Loss: 0.4370046854019165\nBatch 211/2082 - Train Loss: 0.3177177309989929\nBatch 221/2082 - Train Loss: 0.3030041456222534\nBatch 231/2082 - Train Loss: 0.4577533006668091\nBatch 241/2082 - Train Loss: 0.20970982313156128\nBatch 251/2082 - Train Loss: 0.34039705991744995\nBatch 261/2082 - Train Loss: 0.27532321214675903\nBatch 271/2082 - Train Loss: 0.896409273147583\nBatch 281/2082 - Train Loss: 0.428090363740921\nBatch 291/2082 - Train Loss: 0.43958693742752075\nBatch 301/2082 - Train Loss: 0.22064663469791412\nBatch 311/2082 - Train Loss: 0.38894379138946533\nBatch 321/2082 - Train Loss: 0.9128878712654114\nBatch 331/2082 - Train Loss: 0.23109295964241028\nBatch 341/2082 - Train Loss: 0.43741872906684875\nBatch 351/2082 - Train Loss: 0.3080102801322937\nBatch 361/2082 - Train Loss: 0.4246870279312134\nBatch 371/2082 - Train Loss: 0.5202340483665466\nBatch 381/2082 - Train Loss: 0.174043670296669\nBatch 391/2082 - Train Loss: 0.471259206533432\nBatch 401/2082 - Train Loss: 0.4588732421398163\nBatch 411/2082 - Train Loss: 0.26279354095458984\nBatch 421/2082 - Train Loss: 0.2818065583705902\nBatch 431/2082 - Train Loss: 0.5155864953994751\nBatch 441/2082 - Train Loss: 0.39898455142974854\nBatch 451/2082 - Train Loss: 0.35208386182785034\nBatch 461/2082 - Train Loss: 0.8422996401786804\nBatch 471/2082 - Train Loss: 0.3327842056751251\nBatch 481/2082 - Train Loss: 0.715481698513031\nBatch 491/2082 - Train Loss: 0.3708716332912445\nBatch 501/2082 - Train Loss: 0.4485706686973572\nBatch 511/2082 - Train Loss: 0.5486912131309509\nBatch 521/2082 - Train Loss: 0.30117908120155334\nBatch 531/2082 - Train Loss: 0.19147278368473053\nBatch 541/2082 - Train Loss: 0.5831266045570374\nBatch 551/2082 - Train Loss: 0.39728033542633057\nBatch 561/2082 - Train Loss: 0.33330783247947693\nBatch 571/2082 - Train Loss: 0.27343544363975525\nBatch 581/2082 - Train Loss: 0.5026407241821289\nBatch 591/2082 - Train Loss: 0.39899322390556335\nBatch 601/2082 - Train Loss: 0.29933637380599976\nBatch 611/2082 - Train Loss: 0.46719756722450256\nBatch 621/2082 - Train Loss: 0.18328839540481567\nBatch 631/2082 - Train Loss: 0.5791177153587341\nBatch 641/2082 - Train Loss: 0.5471073985099792\nBatch 651/2082 - Train Loss: 0.3579525649547577\nBatch 661/2082 - Train Loss: 0.4841226637363434\nBatch 671/2082 - Train Loss: 0.1935427337884903\nBatch 681/2082 - Train Loss: 0.3289429843425751\nBatch 691/2082 - Train Loss: 0.5389457941055298\nBatch 701/2082 - Train Loss: 0.5074152946472168\nBatch 711/2082 - Train Loss: 0.45616933703422546\nBatch 721/2082 - Train Loss: 0.3094913363456726\nBatch 731/2082 - Train Loss: 0.5360799431800842\nBatch 741/2082 - Train Loss: 0.4858855605125427\nBatch 751/2082 - Train Loss: 0.24224267899990082\nBatch 761/2082 - Train Loss: 0.1365503966808319\nBatch 771/2082 - Train Loss: 0.42905035614967346\nBatch 781/2082 - Train Loss: 0.7171946167945862\nBatch 791/2082 - Train Loss: 0.08787068724632263\nBatch 801/2082 - Train Loss: 0.45968109369277954\nBatch 811/2082 - Train Loss: 0.46915099024772644\nBatch 821/2082 - Train Loss: 0.37825194001197815\nBatch 831/2082 - Train Loss: 0.36096513271331787\nBatch 841/2082 - Train Loss: 0.5267388820648193\nBatch 851/2082 - Train Loss: 0.4508229196071625\nBatch 861/2082 - Train Loss: 0.32647204399108887\nBatch 871/2082 - Train Loss: 0.41041186451911926\nBatch 881/2082 - Train Loss: 0.42041245102882385\nBatch 891/2082 - Train Loss: 0.5669926404953003\nBatch 901/2082 - Train Loss: 0.3366141617298126\nBatch 911/2082 - Train Loss: 0.28043806552886963\nBatch 921/2082 - Train Loss: 0.3651677668094635\nBatch 931/2082 - Train Loss: 0.6504960656166077\nBatch 941/2082 - Train Loss: 0.34652265906333923\nBatch 951/2082 - Train Loss: 0.3857640326023102\nBatch 961/2082 - Train Loss: 0.3948781192302704\nBatch 971/2082 - Train Loss: 0.5064986944198608\nBatch 981/2082 - Train Loss: 0.32987305521965027\nBatch 991/2082 - Train Loss: 0.46222954988479614\nBatch 1001/2082 - Train Loss: 0.2122422158718109\nBatch 1011/2082 - Train Loss: 0.31422123312950134\nBatch 1021/2082 - Train Loss: 0.37657496333122253\nBatch 1031/2082 - Train Loss: 0.6900338530540466\nBatch 1041/2082 - Train Loss: 0.25829511880874634\nBatch 1051/2082 - Train Loss: 0.5503502488136292\nBatch 1061/2082 - Train Loss: 0.46088331937789917\nBatch 1071/2082 - Train Loss: 0.8816010355949402\nBatch 1081/2082 - Train Loss: 0.3072570860385895\nBatch 1091/2082 - Train Loss: 0.32478272914886475\nBatch 1101/2082 - Train Loss: 0.39146459102630615\nBatch 1111/2082 - Train Loss: 0.4559422731399536\nBatch 1121/2082 - Train Loss: 0.5228571891784668\nBatch 1131/2082 - Train Loss: 0.20114873349666595\nBatch 1141/2082 - Train Loss: 0.2899295687675476\nBatch 1151/2082 - Train Loss: 0.41449594497680664\nBatch 1161/2082 - Train Loss: 0.3955170512199402\nBatch 1171/2082 - Train Loss: 0.4820452928543091\nBatch 1181/2082 - Train Loss: 0.3025374412536621\nBatch 1191/2082 - Train Loss: 0.3507065176963806\nBatch 1201/2082 - Train Loss: 0.6126276850700378\nBatch 1211/2082 - Train Loss: 0.195967435836792\nBatch 1221/2082 - Train Loss: 0.7431707978248596\nBatch 1231/2082 - Train Loss: 0.665591835975647\nBatch 1241/2082 - Train Loss: 0.45263469219207764\nBatch 1251/2082 - Train Loss: 0.41548725962638855\nBatch 1261/2082 - Train Loss: 0.5431138873100281\nBatch 1271/2082 - Train Loss: 0.4027634859085083\nBatch 1281/2082 - Train Loss: 0.2424406111240387\nBatch 1291/2082 - Train Loss: 0.10981135070323944\nBatch 1301/2082 - Train Loss: 0.5968265533447266\nBatch 1311/2082 - Train Loss: 0.4747557044029236\nBatch 1321/2082 - Train Loss: 0.38962963223457336\nBatch 1331/2082 - Train Loss: 0.3988719880580902\nBatch 1341/2082 - Train Loss: 0.12849830090999603\nBatch 1351/2082 - Train Loss: 0.48742493987083435\nBatch 1361/2082 - Train Loss: 0.7781420946121216\nBatch 1371/2082 - Train Loss: 0.8239154815673828\nBatch 1381/2082 - Train Loss: 0.5223029255867004\nBatch 1391/2082 - Train Loss: 0.31845495104789734\nBatch 1401/2082 - Train Loss: 0.7948015928268433\nBatch 1411/2082 - Train Loss: 0.5666280388832092\nBatch 1421/2082 - Train Loss: 0.4363206624984741\nBatch 1431/2082 - Train Loss: 0.20017185807228088\nBatch 1441/2082 - Train Loss: 0.41219377517700195\nBatch 1451/2082 - Train Loss: 0.236080601811409\nBatch 1461/2082 - Train Loss: 0.7265167832374573\nBatch 1471/2082 - Train Loss: 0.29197901487350464\nBatch 1481/2082 - Train Loss: 0.10974723845720291\nBatch 1491/2082 - Train Loss: 0.09127005934715271\nBatch 1501/2082 - Train Loss: 0.46359217166900635\nBatch 1511/2082 - Train Loss: 0.5733863711357117\nBatch 1521/2082 - Train Loss: 0.32549262046813965\nBatch 1531/2082 - Train Loss: 0.5666821002960205\nBatch 1541/2082 - Train Loss: 0.3882335126399994\nBatch 1551/2082 - Train Loss: 0.5267482399940491\nBatch 1561/2082 - Train Loss: 0.10576098412275314\nBatch 1571/2082 - Train Loss: 0.282639741897583\nBatch 1581/2082 - Train Loss: 0.4192698299884796\nBatch 1591/2082 - Train Loss: 0.30550679564476013\nBatch 1601/2082 - Train Loss: 0.15274488925933838\nBatch 1611/2082 - Train Loss: 0.28648021817207336\nBatch 1621/2082 - Train Loss: 0.40629807114601135\nBatch 1631/2082 - Train Loss: 0.6836450099945068\nBatch 1641/2082 - Train Loss: 0.3814132511615753\nBatch 1651/2082 - Train Loss: 0.7685513496398926\nBatch 1661/2082 - Train Loss: 0.4130210280418396\nBatch 1671/2082 - Train Loss: 0.45415782928466797\nBatch 1681/2082 - Train Loss: 0.32823675870895386\nBatch 1691/2082 - Train Loss: 0.491517573595047\nBatch 1701/2082 - Train Loss: 0.25289684534072876\nBatch 1711/2082 - Train Loss: 0.3805910348892212\nBatch 1721/2082 - Train Loss: 0.27870601415634155\nBatch 1731/2082 - Train Loss: 0.46571603417396545\nBatch 1741/2082 - Train Loss: 0.20211195945739746\nBatch 1751/2082 - Train Loss: 0.5981277227401733\nBatch 1761/2082 - Train Loss: 0.8767775297164917\nBatch 1771/2082 - Train Loss: 0.47283217310905457\nBatch 1781/2082 - Train Loss: 0.2409280687570572\nBatch 1791/2082 - Train Loss: 0.6243874430656433\nBatch 1801/2082 - Train Loss: 0.290040522813797\nBatch 1811/2082 - Train Loss: 0.1651320457458496\nBatch 1821/2082 - Train Loss: 0.429963082075119\nBatch 1831/2082 - Train Loss: 0.6779292225837708\nBatch 1841/2082 - Train Loss: 0.6578670144081116\nBatch 1851/2082 - Train Loss: 0.8184322714805603\nBatch 1861/2082 - Train Loss: 0.23614519834518433\nBatch 1871/2082 - Train Loss: 0.3693508207798004\nBatch 1881/2082 - Train Loss: 0.28695550560951233\nBatch 1891/2082 - Train Loss: 0.6606525182723999\nBatch 1901/2082 - Train Loss: 0.31869828701019287\nBatch 1911/2082 - Train Loss: 0.3270034193992615\nBatch 1961/2082 - Train Loss: 0.36896565556526184\nBatch 1971/2082 - Train Loss: 0.5563905239105225\nBatch 1981/2082 - Train Loss: 0.5011178255081177\nBatch 1991/2082 - Train Loss: 0.26520541310310364\nBatch 2001/2082 - Train Loss: 0.25137877464294434\nBatch 2011/2082 - Train Loss: 0.5327600836753845\nBatch 2021/2082 - Train Loss: 0.6748861074447632\nBatch 2031/2082 - Train Loss: 0.4040660858154297\nBatch 2041/2082 - Train Loss: 0.36910995841026306\nBatch 2051/2082 - Train Loss: 0.366959810256958\nBatch 2061/2082 - Train Loss: 0.5733987092971802\nBatch 2071/2082 - Train Loss: 0.2640138864517212\nBatch 2081/2082 - Train Loss: 0.3654455542564392\nEpoch 4 - Average Train Loss: 0.43514908532416097\nBatch 1/261 - Validation Loss: 0.07847700268030167\nBatch 11/261 - Validation Loss: 0.4803766906261444\nBatch 21/261 - Validation Loss: 0.3063500225543976\nBatch 31/261 - Validation Loss: 0.33536893129348755\nBatch 41/261 - Validation Loss: 0.4695143699645996\nBatch 51/261 - Validation Loss: 0.427850604057312\nBatch 61/261 - Validation Loss: 0.2680925726890564\nBatch 71/261 - Validation Loss: 0.15989871323108673\nBatch 81/261 - Validation Loss: 0.5267107486724854\nBatch 91/261 - Validation Loss: 0.5240901708602905\nBatch 101/261 - Validation Loss: 0.3822801411151886\nBatch 111/261 - Validation Loss: 0.5318723320960999\nBatch 121/261 - Validation Loss: 0.20520314574241638\nBatch 131/261 - Validation Loss: 0.7084330916404724\nBatch 141/261 - Validation Loss: 0.6535847187042236\nBatch 151/261 - Validation Loss: 0.5386365652084351\nBatch 161/261 - Validation Loss: 0.36537665128707886\nBatch 171/261 - Validation Loss: 0.7134397029876709\nBatch 181/261 - Validation Loss: 0.3925260603427887\nBatch 191/261 - Validation Loss: 0.500012993812561\nBatch 201/261 - Validation Loss: 0.801136314868927\nBatch 211/261 - Validation Loss: 0.27640417218208313\nBatch 221/261 - Validation Loss: 0.1304798722267151\nBatch 231/261 - Validation Loss: 0.15409350395202637\nBatch 241/261 - Validation Loss: 0.257004976272583\nBatch 251/261 - Validation Loss: 0.6854797601699829\nBatch 261/261 - Validation Loss: 0.03933490440249443\nEpoch 4 - Average Validation Loss: 0.4107561794420083\nEpoch 5/10\nBatch 1/2082 - Train Loss: 0.3935607075691223\nBatch 11/2082 - Train Loss: 0.26677122712135315\nBatch 21/2082 - Train Loss: 0.3012638986110687\nBatch 31/2082 - Train Loss: 0.5970927476882935\nBatch 41/2082 - Train Loss: 0.4840468466281891\nBatch 51/2082 - Train Loss: 0.23904724419116974\nBatch 61/2082 - Train Loss: 0.32486769556999207\nBatch 71/2082 - Train Loss: 0.26122409105300903\nBatch 81/2082 - Train Loss: 0.3504607081413269\nBatch 91/2082 - Train Loss: 0.43997371196746826\nBatch 101/2082 - Train Loss: 0.5730016827583313\nBatch 111/2082 - Train Loss: 0.42183953523635864\nBatch 121/2082 - Train Loss: 0.6270614266395569\nBatch 131/2082 - Train Loss: 0.26346683502197266\nBatch 141/2082 - Train Loss: 0.44665172696113586\nBatch 151/2082 - Train Loss: 0.5756840705871582\nBatch 161/2082 - Train Loss: 0.47541263699531555\nBatch 171/2082 - Train Loss: 0.3710958957672119\nBatch 181/2082 - Train Loss: 0.5455988049507141\nBatch 191/2082 - Train Loss: 0.42260855436325073\nBatch 201/2082 - Train Loss: 0.492007851600647\nBatch 211/2082 - Train Loss: 0.7067747116088867\nBatch 221/2082 - Train Loss: 0.43888697028160095\nBatch 231/2082 - Train Loss: 0.24279755353927612\nBatch 241/2082 - Train Loss: 0.3468402326107025\nBatch 251/2082 - Train Loss: 0.6686075925827026\nBatch 261/2082 - Train Loss: 0.17677980661392212\nBatch 271/2082 - Train Loss: 0.5862324237823486\nBatch 281/2082 - Train Loss: 0.29597747325897217\nBatch 291/2082 - Train Loss: 0.17353451251983643\nBatch 301/2082 - Train Loss: 0.24092306196689606\nBatch 311/2082 - Train Loss: 0.3766385018825531\nBatch 321/2082 - Train Loss: 0.26371362805366516\nBatch 331/2082 - Train Loss: 0.5426256656646729\nBatch 341/2082 - Train Loss: 0.41695696115493774\nBatch 351/2082 - Train Loss: 0.32706186175346375\nBatch 361/2082 - Train Loss: 0.1770823746919632\nBatch 371/2082 - Train Loss: 0.48707154393196106\nBatch 381/2082 - Train Loss: 0.30855676531791687\nBatch 391/2082 - Train Loss: 0.2945657968521118\nBatch 401/2082 - Train Loss: 0.10876251012086868\nBatch 411/2082 - Train Loss: 0.4886856973171234\nBatch 421/2082 - Train Loss: 0.40189361572265625\nBatch 431/2082 - Train Loss: 0.4566322863101959\nBatch 441/2082 - Train Loss: 0.6758278608322144\nBatch 451/2082 - Train Loss: 0.2074604481458664\nBatch 461/2082 - Train Loss: 0.6343148946762085\nBatch 471/2082 - Train Loss: 0.2567092776298523\nBatch 481/2082 - Train Loss: 0.39086565375328064\nBatch 491/2082 - Train Loss: 0.3973321318626404\nBatch 501/2082 - Train Loss: 0.15178707242012024\nBatch 511/2082 - Train Loss: 0.5788606405258179\nBatch 521/2082 - Train Loss: 0.528361976146698\nBatch 531/2082 - Train Loss: 0.6174356341362\nBatch 541/2082 - Train Loss: 0.19905045628547668\nBatch 551/2082 - Train Loss: 0.5045710802078247\nBatch 561/2082 - Train Loss: 0.23177991807460785\nBatch 571/2082 - Train Loss: 0.287266343832016\nBatch 581/2082 - Train Loss: 0.647155225276947\nBatch 591/2082 - Train Loss: 0.7041716575622559\nBatch 601/2082 - Train Loss: 0.6073553562164307\nBatch 611/2082 - Train Loss: 0.6742045879364014\nBatch 621/2082 - Train Loss: 0.807786226272583\nBatch 631/2082 - Train Loss: 0.7591637372970581\nBatch 641/2082 - Train Loss: 0.22361689805984497\nBatch 651/2082 - Train Loss: 0.4226592779159546\nBatch 661/2082 - Train Loss: 0.6097234487533569\nBatch 671/2082 - Train Loss: 0.2485692799091339\nBatch 681/2082 - Train Loss: 0.5140222311019897\nBatch 691/2082 - Train Loss: 0.7613252997398376\nBatch 701/2082 - Train Loss: 0.31942567229270935\nBatch 711/2082 - Train Loss: 0.22725051641464233\nBatch 721/2082 - Train Loss: 0.5988108515739441\nBatch 731/2082 - Train Loss: 0.3426770269870758\nBatch 741/2082 - Train Loss: 0.4747985303401947\nBatch 751/2082 - Train Loss: 0.44949957728385925\nBatch 761/2082 - Train Loss: 0.5523278117179871\nBatch 771/2082 - Train Loss: 0.48370125889778137\nBatch 781/2082 - Train Loss: 0.3617411255836487\nBatch 791/2082 - Train Loss: 0.39387255907058716\nBatch 801/2082 - Train Loss: 0.7080835700035095\nBatch 811/2082 - Train Loss: 0.7438192367553711\nBatch 821/2082 - Train Loss: 0.26628053188323975\nBatch 831/2082 - Train Loss: 0.6196901798248291\nBatch 841/2082 - Train Loss: 0.45174112915992737\nBatch 851/2082 - Train Loss: 0.32294517755508423\nBatch 861/2082 - Train Loss: 0.3866974711418152\nBatch 871/2082 - Train Loss: 0.2989095449447632\nBatch 881/2082 - Train Loss: 0.6234068274497986\nBatch 891/2082 - Train Loss: 0.3439601957798004\nBatch 901/2082 - Train Loss: 0.47036972641944885\nBatch 911/2082 - Train Loss: 0.6608248353004456\nBatch 921/2082 - Train Loss: 0.4021424353122711\nBatch 931/2082 - Train Loss: 0.638468325138092\nBatch 941/2082 - Train Loss: 0.45963042974472046\nBatch 951/2082 - Train Loss: 0.3895125091075897\nBatch 961/2082 - Train Loss: 0.42182624340057373\nBatch 971/2082 - Train Loss: 0.48865270614624023\nBatch 981/2082 - Train Loss: 0.4382658004760742\nBatch 991/2082 - Train Loss: 0.4211920201778412\nBatch 1001/2082 - Train Loss: 0.34026917815208435\nBatch 1011/2082 - Train Loss: 0.5491671562194824\nBatch 1021/2082 - Train Loss: 0.5555490255355835\nBatch 1031/2082 - Train Loss: 0.483910471200943\nBatch 1041/2082 - Train Loss: 0.5027186870574951\nBatch 1051/2082 - Train Loss: 0.40427300333976746\nBatch 1061/2082 - Train Loss: 0.41027700901031494\nBatch 1071/2082 - Train Loss: 0.44503289461135864\nBatch 1081/2082 - Train Loss: 0.6322520971298218\nBatch 1091/2082 - Train Loss: 0.45287761092185974\nBatch 1101/2082 - Train Loss: 0.4099091589450836\nBatch 1111/2082 - Train Loss: 0.535836398601532\nBatch 1121/2082 - Train Loss: 0.28362229466438293\nBatch 1131/2082 - Train Loss: 0.2966539263725281\nBatch 1141/2082 - Train Loss: 0.2090163379907608\nBatch 1151/2082 - Train Loss: 0.1651330143213272\nBatch 1161/2082 - Train Loss: 0.36159470677375793\nBatch 1171/2082 - Train Loss: 0.38154205679893494\nBatch 1181/2082 - Train Loss: 0.3932347595691681\nBatch 1191/2082 - Train Loss: 0.40089547634124756\nBatch 1201/2082 - Train Loss: 0.7351194024085999\nBatch 1211/2082 - Train Loss: 0.6319631338119507\nBatch 1221/2082 - Train Loss: 0.19395355880260468\nBatch 1231/2082 - Train Loss: 0.5511847138404846\nBatch 1241/2082 - Train Loss: 0.4837242066860199\nBatch 1251/2082 - Train Loss: 0.24524343013763428\nBatch 1261/2082 - Train Loss: 0.3205575942993164\nBatch 1271/2082 - Train Loss: 0.5529206991195679\nBatch 1281/2082 - Train Loss: 0.3862939774990082\nBatch 1291/2082 - Train Loss: 0.24280105531215668\nBatch 1301/2082 - Train Loss: 0.13200250267982483\nBatch 1311/2082 - Train Loss: 0.26044172048568726\nBatch 1321/2082 - Train Loss: 0.43863606452941895\nBatch 1331/2082 - Train Loss: 0.43148863315582275\nBatch 1341/2082 - Train Loss: 0.4154300093650818\nBatch 1351/2082 - Train Loss: 0.36231839656829834\nBatch 1361/2082 - Train Loss: 0.4404873251914978\nBatch 1371/2082 - Train Loss: 0.4056225121021271\nBatch 1381/2082 - Train Loss: 0.5483371019363403\nBatch 1391/2082 - Train Loss: 0.36614322662353516\nBatch 1401/2082 - Train Loss: 0.33659183979034424\nBatch 1411/2082 - Train Loss: 0.602116048336029\nBatch 1421/2082 - Train Loss: 0.4661906063556671\nBatch 1431/2082 - Train Loss: 0.2703617811203003\nBatch 1441/2082 - Train Loss: 0.4609909653663635\nBatch 1451/2082 - Train Loss: 0.21816390752792358\nBatch 1461/2082 - Train Loss: 0.2389041632413864\nBatch 1471/2082 - Train Loss: 0.5617039799690247\nBatch 1481/2082 - Train Loss: 0.4955447316169739\nBatch 1491/2082 - Train Loss: 0.31926268339157104\nBatch 1501/2082 - Train Loss: 0.4838450253009796\nBatch 1511/2082 - Train Loss: 0.555819571018219\nBatch 1521/2082 - Train Loss: 0.36259064078330994\nBatch 1531/2082 - Train Loss: 0.40845030546188354\nBatch 1541/2082 - Train Loss: 0.5127367973327637\nBatch 1551/2082 - Train Loss: 0.18806660175323486\nBatch 1561/2082 - Train Loss: 0.38549500703811646\nBatch 1571/2082 - Train Loss: 0.40723833441734314\nBatch 1581/2082 - Train Loss: 0.35099467635154724\nBatch 1591/2082 - Train Loss: 0.48179489374160767\nBatch 1601/2082 - Train Loss: 0.3985929787158966\nBatch 1611/2082 - Train Loss: 0.30913957953453064\nBatch 1621/2082 - Train Loss: 0.20624896883964539\nBatch 1631/2082 - Train Loss: 0.3791743218898773\nBatch 1641/2082 - Train Loss: 0.6506751179695129\nBatch 1651/2082 - Train Loss: 0.6244723200798035\nBatch 1661/2082 - Train Loss: 0.7507045269012451\nBatch 1671/2082 - Train Loss: 0.3231751024723053\nBatch 1681/2082 - Train Loss: 0.5628843903541565\nBatch 1691/2082 - Train Loss: 0.6692467927932739\nBatch 1701/2082 - Train Loss: 0.3766230046749115\nBatch 1711/2082 - Train Loss: 0.4243326187133789\nBatch 1721/2082 - Train Loss: 0.1885596066713333\nBatch 1731/2082 - Train Loss: 0.5329720973968506\nBatch 1741/2082 - Train Loss: 0.5348247289657593\nBatch 1751/2082 - Train Loss: 0.37874317169189453\nBatch 1761/2082 - Train Loss: 0.5761860609054565\nBatch 1771/2082 - Train Loss: 0.4242841303348541\nBatch 1781/2082 - Train Loss: 0.27655228972435\nBatch 1791/2082 - Train Loss: 0.20305858552455902\nBatch 1801/2082 - Train Loss: 0.47698119282722473\nBatch 1811/2082 - Train Loss: 0.5226935148239136\nBatch 1821/2082 - Train Loss: 0.23663218319416046\nBatch 1831/2082 - Train Loss: 0.18192459642887115\nBatch 1841/2082 - Train Loss: 0.3331570625305176\nBatch 1851/2082 - Train Loss: 0.48653289675712585\nBatch 1861/2082 - Train Loss: 0.5463789105415344\nBatch 1871/2082 - Train Loss: 0.5937376618385315\nBatch 1881/2082 - Train Loss: 0.29599717259407043\nBatch 1891/2082 - Train Loss: 0.732479989528656\nBatch 1901/2082 - Train Loss: 0.43197810649871826\nBatch 1911/2082 - Train Loss: 0.33881711959838867\nBatch 1921/2082 - Train Loss: 0.3627099394798279\nBatch 1931/2082 - Train Loss: 0.749649703502655\nBatch 1941/2082 - Train Loss: 0.3139897882938385\nBatch 1951/2082 - Train Loss: 0.42853662371635437\nBatch 1961/2082 - Train Loss: 0.4705120325088501\nBatch 1971/2082 - Train Loss: 0.36574894189834595\nBatch 1981/2082 - Train Loss: 0.17022863030433655\nBatch 1991/2082 - Train Loss: 0.5691037774085999\nBatch 2001/2082 - Train Loss: 0.22148694097995758\nBatch 2011/2082 - Train Loss: 0.6228762865066528\nBatch 2021/2082 - Train Loss: 0.4114828109741211\nBatch 2031/2082 - Train Loss: 0.4697355628013611\nBatch 2041/2082 - Train Loss: 0.2647508382797241\nBatch 2051/2082 - Train Loss: 0.5765498876571655\nBatch 2061/2082 - Train Loss: 0.9149397611618042\nBatch 2071/2082 - Train Loss: 0.36860403418540955\nBatch 2081/2082 - Train Loss: 0.4464530348777771\nEpoch 5 - Average Train Loss: 0.4236140635551654\nBatch 1/261 - Validation Loss: 0.0795748382806778\nBatch 11/261 - Validation Loss: 0.4655696153640747\nBatch 21/261 - Validation Loss: 0.3051040768623352\nBatch 31/261 - Validation Loss: 0.32616791129112244\nBatch 41/261 - Validation Loss: 0.45847630500793457\nBatch 51/261 - Validation Loss: 0.4223555326461792\nBatch 61/261 - Validation Loss: 0.2667822539806366\nBatch 71/261 - Validation Loss: 0.16302601993083954\nBatch 81/261 - Validation Loss: 0.5183989405632019\nBatch 91/261 - Validation Loss: 0.5151472687721252\nBatch 101/261 - Validation Loss: 0.38008031249046326\nBatch 111/261 - Validation Loss: 0.5289574265480042\nBatch 121/261 - Validation Loss: 0.20574195683002472\nBatch 131/261 - Validation Loss: 0.7115103006362915\nBatch 141/261 - Validation Loss: 0.6481395363807678\nBatch 151/261 - Validation Loss: 0.5277073383331299\nBatch 161/261 - Validation Loss: 0.3631901443004608\nBatch 171/261 - Validation Loss: 0.7043322324752808\nBatch 181/261 - Validation Loss: 0.3953057825565338\nBatch 191/261 - Validation Loss: 0.4884597361087799\nBatch 201/261 - Validation Loss: 0.7971614003181458\nBatch 211/261 - Validation Loss: 0.2705226540565491\nBatch 221/261 - Validation Loss: 0.12514719367027283\nBatch 231/261 - Validation Loss: 0.1552792191505432\nBatch 241/261 - Validation Loss: 0.2561124861240387\nBatch 251/261 - Validation Loss: 0.6742704510688782\nBatch 261/261 - Validation Loss: 0.040515922009944916\nEpoch 5 - Average Validation Loss: 0.40634949990615993\nEpoch 6/10\nBatch 1/2082 - Train Loss: 0.5452439188957214\nBatch 11/2082 - Train Loss: 0.09746848791837692\nBatch 21/2082 - Train Loss: 0.390460729598999\nBatch 31/2082 - Train Loss: 0.6754558086395264\nBatch 41/2082 - Train Loss: 0.4875119924545288\nBatch 51/2082 - Train Loss: 0.5429173707962036\nBatch 61/2082 - Train Loss: 0.21794632077217102\nBatch 71/2082 - Train Loss: 0.346139132976532\nBatch 81/2082 - Train Loss: 0.4265666902065277\nBatch 91/2082 - Train Loss: 0.6223604679107666\nBatch 101/2082 - Train Loss: 0.5500039458274841\nBatch 111/2082 - Train Loss: 0.27332445979118347\nBatch 121/2082 - Train Loss: 0.2604375183582306\nBatch 131/2082 - Train Loss: 0.47485798597335815\nBatch 141/2082 - Train Loss: 0.5123135447502136\nBatch 151/2082 - Train Loss: 0.4133334457874298\nBatch 161/2082 - Train Loss: 0.4670625925064087\nBatch 171/2082 - Train Loss: 0.6704320907592773\nBatch 181/2082 - Train Loss: 0.38934561610221863\nBatch 191/2082 - Train Loss: 0.3110407888889313\nBatch 201/2082 - Train Loss: 0.3656653165817261\nBatch 211/2082 - Train Loss: 0.35890379548072815\nBatch 221/2082 - Train Loss: 0.16109049320220947\nBatch 231/2082 - Train Loss: 0.6200354099273682\nBatch 241/2082 - Train Loss: 0.4625426232814789\nBatch 251/2082 - Train Loss: 0.09304334968328476\nBatch 261/2082 - Train Loss: 0.7503570318222046\nBatch 271/2082 - Train Loss: 0.24878810346126556\nBatch 281/2082 - Train Loss: 0.34778982400894165\nBatch 291/2082 - Train Loss: 0.7132934331893921\nBatch 301/2082 - Train Loss: 0.33767154812812805\nBatch 311/2082 - Train Loss: 0.3242343068122864\nBatch 321/2082 - Train Loss: 0.8461817502975464\nBatch 331/2082 - Train Loss: 0.44529494643211365\nBatch 341/2082 - Train Loss: 0.6311165690422058\nBatch 351/2082 - Train Loss: 0.3346351087093353\nBatch 361/2082 - Train Loss: 0.36318063735961914\nBatch 371/2082 - Train Loss: 0.5029721260070801\nBatch 381/2082 - Train Loss: 0.3793790936470032\nBatch 391/2082 - Train Loss: 0.14528098702430725\nBatch 401/2082 - Train Loss: 0.3086031973361969\nBatch 411/2082 - Train Loss: 0.30992186069488525\nBatch 421/2082 - Train Loss: 0.1594206839799881\nBatch 431/2082 - Train Loss: 0.41528406739234924\nBatch 441/2082 - Train Loss: 0.5423809289932251\nBatch 451/2082 - Train Loss: 0.3294234275817871\nBatch 461/2082 - Train Loss: 0.5817757248878479\nBatch 471/2082 - Train Loss: 0.380953848361969\nBatch 481/2082 - Train Loss: 0.401024729013443\nBatch 491/2082 - Train Loss: 0.14730538427829742\nBatch 501/2082 - Train Loss: 0.492521733045578\nBatch 511/2082 - Train Loss: 0.4215719997882843\nBatch 521/2082 - Train Loss: 0.30620530247688293\nBatch 531/2082 - Train Loss: 0.29700767993927\nBatch 541/2082 - Train Loss: 0.07334308326244354\nBatch 551/2082 - Train Loss: 0.45989930629730225\nBatch 561/2082 - Train Loss: 0.5090531706809998\nBatch 571/2082 - Train Loss: 0.42948949337005615\nBatch 581/2082 - Train Loss: 0.6358640193939209\nBatch 591/2082 - Train Loss: 0.21603645384311676\nBatch 601/2082 - Train Loss: 0.3312266767024994\nBatch 611/2082 - Train Loss: 0.46701404452323914\nBatch 621/2082 - Train Loss: 0.7099742889404297\nBatch 631/2082 - Train Loss: 0.35413992404937744\nBatch 641/2082 - Train Loss: 0.27199092507362366\nBatch 651/2082 - Train Loss: 0.43689432740211487\nBatch 661/2082 - Train Loss: 0.30853667855262756\nBatch 671/2082 - Train Loss: 0.40354645252227783\nBatch 681/2082 - Train Loss: 0.21286450326442719\nBatch 691/2082 - Train Loss: 0.33877477049827576\nBatch 701/2082 - Train Loss: 0.6818606853485107\nBatch 711/2082 - Train Loss: 0.3916037678718567\nBatch 721/2082 - Train Loss: 0.6816000938415527\nBatch 731/2082 - Train Loss: 0.5771348476409912\nBatch 741/2082 - Train Loss: 0.4037010073661804\nBatch 751/2082 - Train Loss: 0.26140540838241577\nBatch 761/2082 - Train Loss: 0.30145907402038574\nBatch 771/2082 - Train Loss: 0.3483225107192993\nBatch 781/2082 - Train Loss: 0.31366220116615295\nBatch 791/2082 - Train Loss: 0.6838533282279968\nBatch 801/2082 - Train Loss: 0.3074696958065033\nBatch 811/2082 - Train Loss: 0.34270259737968445\nBatch 821/2082 - Train Loss: 0.5498071908950806\nBatch 831/2082 - Train Loss: 0.21900151669979095\nBatch 841/2082 - Train Loss: 0.2441863715648651\nBatch 851/2082 - Train Loss: 0.42054492235183716\nBatch 861/2082 - Train Loss: 0.5545665621757507\nBatch 871/2082 - Train Loss: 0.42509323358535767\nBatch 881/2082 - Train Loss: 0.41734758019447327\nBatch 891/2082 - Train Loss: 0.46008020639419556\nBatch 901/2082 - Train Loss: 0.3052200973033905\nBatch 911/2082 - Train Loss: 0.5246437788009644\nBatch 921/2082 - Train Loss: 0.30788153409957886\nBatch 931/2082 - Train Loss: 0.6253049373626709\nBatch 941/2082 - Train Loss: 0.38203755021095276\nBatch 951/2082 - Train Loss: 0.2746748626232147\nBatch 961/2082 - Train Loss: 0.6719726920127869\nBatch 971/2082 - Train Loss: 0.5259649157524109\nBatch 981/2082 - Train Loss: 0.2905177175998688\nBatch 991/2082 - Train Loss: 0.623309314250946\nBatch 1001/2082 - Train Loss: 0.4871051609516144\nBatch 1011/2082 - Train Loss: 0.2628324329853058\nBatch 1021/2082 - Train Loss: 0.3152402937412262\nBatch 1031/2082 - Train Loss: 0.12431269139051437\nBatch 1041/2082 - Train Loss: 0.2348140925168991\nBatch 1051/2082 - Train Loss: 0.46280020475387573\nBatch 1061/2082 - Train Loss: 0.24958844482898712\nBatch 1071/2082 - Train Loss: 0.8625914454460144\nBatch 1081/2082 - Train Loss: 0.3387148678302765\nBatch 1091/2082 - Train Loss: 0.38506749272346497\nBatch 1101/2082 - Train Loss: 0.2359667718410492\nBatch 1111/2082 - Train Loss: 0.49900171160697937\nBatch 1121/2082 - Train Loss: 0.5374317169189453\nBatch 1131/2082 - Train Loss: 0.3110201954841614\nBatch 1141/2082 - Train Loss: 0.3602534830570221\nBatch 1151/2082 - Train Loss: 0.4806222915649414\nBatch 1161/2082 - Train Loss: 0.3626345694065094\nBatch 1171/2082 - Train Loss: 0.2506465017795563\nBatch 1181/2082 - Train Loss: 0.4653533399105072\nBatch 1191/2082 - Train Loss: 0.648271918296814\nBatch 1201/2082 - Train Loss: 0.5339226722717285\nBatch 1211/2082 - Train Loss: 0.7020930051803589\nBatch 1221/2082 - Train Loss: 0.11533868312835693\nBatch 1231/2082 - Train Loss: 0.35274413228034973\nBatch 1241/2082 - Train Loss: 0.5732780694961548\nBatch 1251/2082 - Train Loss: 0.5671455264091492\nBatch 1261/2082 - Train Loss: 0.1504969298839569\nBatch 1271/2082 - Train Loss: 0.6191664338111877\nBatch 1281/2082 - Train Loss: 0.44296789169311523\nBatch 1291/2082 - Train Loss: 0.5615566968917847\nBatch 1301/2082 - Train Loss: 0.42969217896461487\nBatch 1311/2082 - Train Loss: 0.23721468448638916\nBatch 1321/2082 - Train Loss: 0.42623284459114075\nBatch 1331/2082 - Train Loss: 0.42876407504081726\nBatch 1341/2082 - Train Loss: 0.05147746577858925\nBatch 1351/2082 - Train Loss: 0.38184940814971924\nBatch 1361/2082 - Train Loss: 0.4461897313594818\nBatch 1371/2082 - Train Loss: 0.19214749336242676\nBatch 1381/2082 - Train Loss: 0.36346733570098877\nBatch 1391/2082 - Train Loss: 0.22313179075717926\nBatch 1401/2082 - Train Loss: 0.28489363193511963\nBatch 1411/2082 - Train Loss: 0.23220016062259674\nBatch 1421/2082 - Train Loss: 0.7443650960922241\nBatch 1431/2082 - Train Loss: 0.4532669186592102\nBatch 1441/2082 - Train Loss: 0.09465349465608597\nBatch 1451/2082 - Train Loss: 0.4502266049385071\nBatch 1461/2082 - Train Loss: 0.4059463441371918\nBatch 1471/2082 - Train Loss: 0.3691689968109131\nBatch 1481/2082 - Train Loss: 0.20670181512832642\nBatch 1491/2082 - Train Loss: 0.43853041529655457\nBatch 1501/2082 - Train Loss: 0.42007213830947876\nBatch 1511/2082 - Train Loss: 0.3848051130771637\nBatch 1521/2082 - Train Loss: 0.5130155086517334\nBatch 1531/2082 - Train Loss: 0.37187135219573975\nBatch 1541/2082 - Train Loss: 0.2416456788778305\nBatch 1551/2082 - Train Loss: 0.41212791204452515\nBatch 1561/2082 - Train Loss: 0.5408427715301514\nBatch 1571/2082 - Train Loss: 0.2619991600513458\nBatch 1581/2082 - Train Loss: 0.6282223463058472\nBatch 1591/2082 - Train Loss: 0.37565311789512634\nBatch 1601/2082 - Train Loss: 0.29526907205581665\nBatch 1611/2082 - Train Loss: 0.3412696421146393\nBatch 1621/2082 - Train Loss: 0.0625726729631424\nBatch 1631/2082 - Train Loss: 0.44956129789352417\nBatch 1641/2082 - Train Loss: 0.09313561767339706\nBatch 1651/2082 - Train Loss: 0.5274059176445007\nBatch 1661/2082 - Train Loss: 0.31742456555366516\nBatch 1671/2082 - Train Loss: 0.3047596216201782\nBatch 1681/2082 - Train Loss: 0.2941627502441406\nBatch 1691/2082 - Train Loss: 0.301911860704422\nBatch 1701/2082 - Train Loss: 0.4312734007835388\nBatch 1711/2082 - Train Loss: 0.31275299191474915\nBatch 1721/2082 - Train Loss: 0.09701681137084961\nBatch 1731/2082 - Train Loss: 0.4603228271007538\nBatch 1741/2082 - Train Loss: 1.0402628183364868\nBatch 1751/2082 - Train Loss: 0.16590051352977753\nBatch 1761/2082 - Train Loss: 0.44610169529914856\nBatch 1771/2082 - Train Loss: 0.40443265438079834\nBatch 1781/2082 - Train Loss: 0.32091718912124634\nBatch 1791/2082 - Train Loss: 0.2236943244934082\nBatch 1801/2082 - Train Loss: 0.47647368907928467\nBatch 1811/2082 - Train Loss: 0.5433506965637207\nBatch 1821/2082 - Train Loss: 0.356540322303772\nBatch 1831/2082 - Train Loss: 0.25936955213546753\nBatch 1841/2082 - Train Loss: 0.2599044144153595\nBatch 1851/2082 - Train Loss: 0.402915358543396\nBatch 1861/2082 - Train Loss: 0.4681500792503357\nBatch 1871/2082 - Train Loss: 0.494255930185318\nBatch 1881/2082 - Train Loss: 0.5812119245529175\nBatch 1891/2082 - Train Loss: 0.35396069288253784\nBatch 1901/2082 - Train Loss: 0.4456943869590759\nBatch 1911/2082 - Train Loss: 0.6399633288383484\nBatch 1921/2082 - Train Loss: 0.1177467629313469\nBatch 1931/2082 - Train Loss: 0.6804408431053162\nBatch 1941/2082 - Train Loss: 0.7008635401725769\nBatch 1951/2082 - Train Loss: 0.6179322004318237\nBatch 1961/2082 - Train Loss: 0.4548226296901703\nBatch 1971/2082 - Train Loss: 0.43847569823265076\nBatch 1981/2082 - Train Loss: 0.3751929700374603\nBatch 1991/2082 - Train Loss: 0.299701988697052\nBatch 2001/2082 - Train Loss: 0.27743759751319885\nBatch 2011/2082 - Train Loss: 0.31265780329704285\nBatch 2021/2082 - Train Loss: 0.39750027656555176\nBatch 2031/2082 - Train Loss: 0.49957016110420227\nBatch 2041/2082 - Train Loss: 0.4455975294113159\nBatch 2051/2082 - Train Loss: 0.25028273463249207\nBatch 2061/2082 - Train Loss: 0.7297656536102295\nBatch 2071/2082 - Train Loss: 0.777664303779602\nBatch 2081/2082 - Train Loss: 0.3435782492160797\nEpoch 6 - Average Train Loss: 0.41346765365576826\nBatch 1/261 - Validation Loss: 0.0756528377532959\nBatch 11/261 - Validation Loss: 0.4586925804615021\nBatch 21/261 - Validation Loss: 0.2973417639732361\nBatch 31/261 - Validation Loss: 0.32684326171875\nBatch 41/261 - Validation Loss: 0.45325881242752075\nBatch 51/261 - Validation Loss: 0.4190300703048706\nBatch 61/261 - Validation Loss: 0.26803961396217346\nBatch 71/261 - Validation Loss: 0.15330158174037933\nBatch 81/261 - Validation Loss: 0.516111433506012\nBatch 91/261 - Validation Loss: 0.510665237903595\nBatch 101/261 - Validation Loss: 0.3785417079925537\nBatch 111/261 - Validation Loss: 0.5198478698730469\nBatch 121/261 - Validation Loss: 0.2033921778202057\nBatch 131/261 - Validation Loss: 0.7076982855796814\nBatch 141/261 - Validation Loss: 0.6369917988777161\nBatch 151/261 - Validation Loss: 0.5250734686851501\nBatch 161/261 - Validation Loss: 0.3584555685520172\nBatch 171/261 - Validation Loss: 0.6943267583847046\nBatch 181/261 - Validation Loss: 0.3967060446739197\nBatch 191/261 - Validation Loss: 0.48163965344429016\nBatch 201/261 - Validation Loss: 0.799781322479248\nBatch 211/261 - Validation Loss: 0.264640212059021\nBatch 221/261 - Validation Loss: 0.12204039096832275\nBatch 231/261 - Validation Loss: 0.15567658841609955\nBatch 241/261 - Validation Loss: 0.2553340792655945\nBatch 251/261 - Validation Loss: 0.6755772233009338\nBatch 261/261 - Validation Loss: 0.045690737664699554\nEpoch 6 - Average Validation Loss: 0.40267401611097015\nEpoch 7/10\nBatch 1/2082 - Train Loss: 0.3802099823951721\nBatch 11/2082 - Train Loss: 0.44875818490982056\nBatch 21/2082 - Train Loss: 0.15604126453399658\nBatch 31/2082 - Train Loss: 0.7050790190696716\nBatch 41/2082 - Train Loss: 0.4773466885089874\nBatch 51/2082 - Train Loss: 0.7705903649330139\nBatch 61/2082 - Train Loss: 0.09540140628814697\nBatch 71/2082 - Train Loss: 0.3050752580165863\nBatch 81/2082 - Train Loss: 0.19875407218933105\nBatch 91/2082 - Train Loss: 0.44064438343048096\nBatch 101/2082 - Train Loss: 0.3785358667373657\nBatch 111/2082 - Train Loss: 0.18067191541194916\nBatch 121/2082 - Train Loss: 0.5430150628089905\nBatch 131/2082 - Train Loss: 0.4787592589855194\nBatch 141/2082 - Train Loss: 0.06880813091993332\nBatch 151/2082 - Train Loss: 0.5142974853515625\nBatch 161/2082 - Train Loss: 0.4939159154891968\nBatch 171/2082 - Train Loss: 0.35738277435302734\nBatch 181/2082 - Train Loss: 0.22002050280570984\nBatch 191/2082 - Train Loss: 0.33159056305885315\nBatch 201/2082 - Train Loss: 0.4075681269168854\nBatch 211/2082 - Train Loss: 0.3365769684314728\nBatch 221/2082 - Train Loss: 0.435332328081131\nBatch 231/2082 - Train Loss: 0.6417242884635925\nBatch 241/2082 - Train Loss: 0.25986340641975403\nBatch 251/2082 - Train Loss: 0.568247377872467\nBatch 261/2082 - Train Loss: 0.40110334753990173\nBatch 271/2082 - Train Loss: 0.41625314950942993\nBatch 281/2082 - Train Loss: 0.5079476833343506\nBatch 291/2082 - Train Loss: 0.5409536361694336\nBatch 301/2082 - Train Loss: 0.3506203293800354\nBatch 311/2082 - Train Loss: 0.46332767605781555\nBatch 321/2082 - Train Loss: 0.22667871415615082\nBatch 331/2082 - Train Loss: 0.5372730493545532\nBatch 341/2082 - Train Loss: 0.49157124757766724\nBatch 351/2082 - Train Loss: 0.40518566966056824\nBatch 361/2082 - Train Loss: 0.5436736345291138\nBatch 371/2082 - Train Loss: 0.40926554799079895\nBatch 381/2082 - Train Loss: 0.599884569644928\nBatch 391/2082 - Train Loss: 0.4176954925060272\nBatch 401/2082 - Train Loss: 0.254658579826355\nBatch 411/2082 - Train Loss: 0.44765111804008484\nBatch 421/2082 - Train Loss: 0.4096500277519226\nBatch 431/2082 - Train Loss: 0.368374228477478\nBatch 441/2082 - Train Loss: 0.106475330889225\nBatch 451/2082 - Train Loss: 0.5393760204315186\nBatch 461/2082 - Train Loss: 0.3892478942871094\nBatch 471/2082 - Train Loss: 0.2694171071052551\nBatch 481/2082 - Train Loss: 0.3759385347366333\nBatch 491/2082 - Train Loss: 0.47684240341186523\nBatch 501/2082 - Train Loss: 0.4803483486175537\nBatch 511/2082 - Train Loss: 0.39963698387145996\nBatch 521/2082 - Train Loss: 0.3622385561466217\nBatch 531/2082 - Train Loss: 0.3312181830406189\nBatch 541/2082 - Train Loss: 0.6468634605407715\nBatch 551/2082 - Train Loss: 0.40889695286750793\nBatch 561/2082 - Train Loss: 0.2536625564098358\nBatch 571/2082 - Train Loss: 0.3700818419456482\nBatch 581/2082 - Train Loss: 0.48076844215393066\nBatch 591/2082 - Train Loss: 0.4034961462020874\nBatch 601/2082 - Train Loss: 0.45683035254478455\nBatch 611/2082 - Train Loss: 0.41011422872543335\nBatch 621/2082 - Train Loss: 0.6726272702217102\nBatch 631/2082 - Train Loss: 0.34810665249824524\nBatch 641/2082 - Train Loss: 0.47605645656585693\nBatch 651/2082 - Train Loss: 0.31442493200302124\nBatch 661/2082 - Train Loss: 0.20121166110038757\nBatch 671/2082 - Train Loss: 0.32199734449386597\nBatch 681/2082 - Train Loss: 0.5263108015060425\nBatch 691/2082 - Train Loss: 0.30634477734565735\nBatch 701/2082 - Train Loss: 0.6326286792755127\nBatch 711/2082 - Train Loss: 0.059142813086509705\nBatch 721/2082 - Train Loss: 0.4200720489025116\nBatch 731/2082 - Train Loss: 0.5388405323028564\nBatch 741/2082 - Train Loss: 0.4771496057510376\nBatch 751/2082 - Train Loss: 0.45122015476226807\nBatch 761/2082 - Train Loss: 0.4473402500152588\nBatch 771/2082 - Train Loss: 0.32201361656188965\nBatch 781/2082 - Train Loss: 0.2332049310207367\nBatch 791/2082 - Train Loss: 0.2874901592731476\nBatch 801/2082 - Train Loss: 0.18628092110157013\nBatch 811/2082 - Train Loss: 0.5060045123100281\nBatch 821/2082 - Train Loss: 0.3324126899242401\nBatch 831/2082 - Train Loss: 0.39485281705856323\nBatch 841/2082 - Train Loss: 0.3636556565761566\nBatch 851/2082 - Train Loss: 0.46344873309135437\nBatch 861/2082 - Train Loss: 0.17617569863796234\nBatch 871/2082 - Train Loss: 0.6145411729812622\nBatch 881/2082 - Train Loss: 0.31864798069000244\nBatch 891/2082 - Train Loss: 0.48166391253471375\nBatch 901/2082 - Train Loss: 0.5046214461326599\nBatch 911/2082 - Train Loss: 0.4073401093482971\nBatch 921/2082 - Train Loss: 0.3804561495780945\nBatch 931/2082 - Train Loss: 0.7396169900894165\nBatch 941/2082 - Train Loss: 0.7015597224235535\nBatch 951/2082 - Train Loss: 0.9259583353996277\nBatch 961/2082 - Train Loss: 0.27646303176879883\nBatch 971/2082 - Train Loss: 0.3979399800300598\nBatch 981/2082 - Train Loss: 0.24278974533081055\nBatch 991/2082 - Train Loss: 0.2785894274711609\nBatch 1001/2082 - Train Loss: 0.5187388062477112\nBatch 1011/2082 - Train Loss: 0.15565145015716553\nBatch 1021/2082 - Train Loss: 0.5630275011062622\nBatch 1031/2082 - Train Loss: 0.2185695618391037\nBatch 1041/2082 - Train Loss: 0.361453115940094\nBatch 1051/2082 - Train Loss: 0.24692416191101074\nBatch 1061/2082 - Train Loss: 0.42232510447502136\nBatch 1071/2082 - Train Loss: 0.23801670968532562\nBatch 1081/2082 - Train Loss: 0.3816133737564087\nBatch 1091/2082 - Train Loss: 0.3699953556060791\nBatch 1101/2082 - Train Loss: 0.15304401516914368\nBatch 1111/2082 - Train Loss: 0.4078025221824646\nBatch 1121/2082 - Train Loss: 0.49265244603157043\nBatch 1131/2082 - Train Loss: 0.5330358147621155\nBatch 1141/2082 - Train Loss: 0.25550222396850586\nBatch 1151/2082 - Train Loss: 0.55636066198349\nBatch 1161/2082 - Train Loss: 0.5246615409851074\nBatch 1171/2082 - Train Loss: 0.4561000466346741\nBatch 1181/2082 - Train Loss: 0.3059093654155731\nBatch 1191/2082 - Train Loss: 0.9586926102638245\nBatch 1201/2082 - Train Loss: 0.22938112914562225\nBatch 1211/2082 - Train Loss: 0.3526771664619446\nBatch 1221/2082 - Train Loss: 0.35948434472084045\nBatch 1231/2082 - Train Loss: 0.29476675391197205\nBatch 1241/2082 - Train Loss: 0.4242230951786041\nBatch 1251/2082 - Train Loss: 0.24658995866775513\nBatch 1261/2082 - Train Loss: 0.21321971714496613\nBatch 1271/2082 - Train Loss: 0.2839014530181885\nBatch 1281/2082 - Train Loss: 0.2780272960662842\nBatch 1291/2082 - Train Loss: 0.14031627774238586\nBatch 1301/2082 - Train Loss: 0.6740942597389221\nBatch 1311/2082 - Train Loss: 0.5563191175460815\nBatch 1321/2082 - Train Loss: 0.6909341216087341\nBatch 1331/2082 - Train Loss: 0.5866662859916687\nBatch 1341/2082 - Train Loss: 0.46671009063720703\nBatch 1351/2082 - Train Loss: 0.4063386917114258\nBatch 1361/2082 - Train Loss: 0.2353014349937439\nBatch 1371/2082 - Train Loss: 0.33003494143486023\nBatch 1381/2082 - Train Loss: 0.2825119197368622\nBatch 1391/2082 - Train Loss: 0.47012004256248474\nBatch 1401/2082 - Train Loss: 0.2885170876979828\nBatch 1411/2082 - Train Loss: 0.22979751229286194\nBatch 1421/2082 - Train Loss: 0.4812489449977875\nBatch 1431/2082 - Train Loss: 0.3750899136066437\nBatch 1441/2082 - Train Loss: 0.41625428199768066\nBatch 1451/2082 - Train Loss: 0.20318657159805298\nBatch 1461/2082 - Train Loss: 0.2796541750431061\nBatch 1471/2082 - Train Loss: 0.4661656618118286\nBatch 1481/2082 - Train Loss: 0.3540453612804413\nBatch 1491/2082 - Train Loss: 0.2721572816371918\nBatch 1501/2082 - Train Loss: 0.7590017914772034\nBatch 1511/2082 - Train Loss: 0.6120211482048035\nBatch 1521/2082 - Train Loss: 0.5090128779411316\nBatch 1531/2082 - Train Loss: 0.46036314964294434\nBatch 1541/2082 - Train Loss: 0.4267306923866272\nBatch 1551/2082 - Train Loss: 0.5423777103424072\nBatch 1561/2082 - Train Loss: 0.43697091937065125\nBatch 1571/2082 - Train Loss: 0.8628941774368286\nBatch 1581/2082 - Train Loss: 0.789269745349884\nBatch 1591/2082 - Train Loss: 0.23222684860229492\nBatch 1601/2082 - Train Loss: 0.1489057093858719\nBatch 1611/2082 - Train Loss: 0.45019811391830444\nBatch 1621/2082 - Train Loss: 0.3524807095527649\nBatch 1631/2082 - Train Loss: 0.5699573755264282\nBatch 1641/2082 - Train Loss: 0.37399283051490784\nBatch 1651/2082 - Train Loss: 0.27172088623046875\nBatch 1661/2082 - Train Loss: 0.32380619645118713\nBatch 1671/2082 - Train Loss: 0.3531593978404999\nBatch 1681/2082 - Train Loss: 0.4426538646221161\nBatch 1691/2082 - Train Loss: 0.2421671748161316\nBatch 1701/2082 - Train Loss: 0.24027857184410095\nBatch 1711/2082 - Train Loss: 0.5679981112480164\nBatch 1721/2082 - Train Loss: 0.3838125765323639\nBatch 1731/2082 - Train Loss: 0.23195011913776398\nBatch 1741/2082 - Train Loss: 0.4457569420337677\nBatch 1751/2082 - Train Loss: 0.49929946660995483\nBatch 1761/2082 - Train Loss: 0.3772517442703247\nBatch 1771/2082 - Train Loss: 0.41822925209999084\nBatch 1781/2082 - Train Loss: 0.26034867763519287\nBatch 1791/2082 - Train Loss: 0.49473562836647034\nBatch 1801/2082 - Train Loss: 0.8339957594871521\nBatch 1811/2082 - Train Loss: 0.39020460844039917\nBatch 1821/2082 - Train Loss: 0.40961626172065735\nBatch 1831/2082 - Train Loss: 0.4246878921985626\nBatch 1841/2082 - Train Loss: 0.29324543476104736\nBatch 1851/2082 - Train Loss: 0.21645741164684296\nBatch 1861/2082 - Train Loss: 0.5868297219276428\nBatch 1871/2082 - Train Loss: 0.15972156822681427\nBatch 1881/2082 - Train Loss: 0.27702784538269043\nBatch 1891/2082 - Train Loss: 0.2288915067911148\nBatch 1901/2082 - Train Loss: 0.5029907822608948\nBatch 1911/2082 - Train Loss: 0.5180477499961853\nBatch 1921/2082 - Train Loss: 0.2670075595378876\nBatch 1931/2082 - Train Loss: 0.5159579515457153\nBatch 1941/2082 - Train Loss: 0.2836930453777313\nBatch 1951/2082 - Train Loss: 0.056313883513212204\nBatch 1961/2082 - Train Loss: 0.41146427392959595\nBatch 1971/2082 - Train Loss: 0.18588195741176605\nBatch 1981/2082 - Train Loss: 0.6249545216560364\nBatch 1991/2082 - Train Loss: 0.3213546872138977\nBatch 2001/2082 - Train Loss: 0.47198206186294556\nBatch 2011/2082 - Train Loss: 0.36502179503440857\nBatch 2021/2082 - Train Loss: 0.4539943039417267\nBatch 2031/2082 - Train Loss: 0.596699059009552\nBatch 2041/2082 - Train Loss: 0.3557511270046234\nBatch 2051/2082 - Train Loss: 0.3694872260093689\nBatch 2061/2082 - Train Loss: 0.38146325945854187\nBatch 2071/2082 - Train Loss: 0.5032336711883545\nBatch 2081/2082 - Train Loss: 0.35966143012046814\nEpoch 7 - Average Train Loss: 0.40450514196333237\nBatch 1/261 - Validation Loss: 0.07617083936929703\nBatch 11/261 - Validation Loss: 0.4438263475894928\nBatch 21/261 - Validation Loss: 0.2979695796966553\nBatch 31/261 - Validation Loss: 0.317340224981308\nBatch 41/261 - Validation Loss: 0.43859755992889404\nBatch 51/261 - Validation Loss: 0.41266024112701416\nBatch 61/261 - Validation Loss: 0.25998738408088684\nBatch 71/261 - Validation Loss: 0.15407046675682068\nBatch 81/261 - Validation Loss: 0.5044587850570679\nBatch 91/261 - Validation Loss: 0.5122901201248169\nBatch 101/261 - Validation Loss: 0.3795253336429596\nBatch 111/261 - Validation Loss: 0.5149739384651184\nBatch 121/261 - Validation Loss: 0.19660520553588867\nBatch 131/261 - Validation Loss: 0.7026039958000183\nBatch 141/261 - Validation Loss: 0.6331194639205933\nBatch 151/261 - Validation Loss: 0.5179348587989807\nBatch 161/261 - Validation Loss: 0.3516862690448761\nBatch 171/261 - Validation Loss: 0.685871422290802\nBatch 181/261 - Validation Loss: 0.38903284072875977\nBatch 191/261 - Validation Loss: 0.4740661084651947\nBatch 201/261 - Validation Loss: 0.7856634855270386\nBatch 211/261 - Validation Loss: 0.26125258207321167\nBatch 221/261 - Validation Loss: 0.12281715124845505\nBatch 231/261 - Validation Loss: 0.15737701952457428\nBatch 241/261 - Validation Loss: 0.24995529651641846\nBatch 251/261 - Validation Loss: 0.6594509482383728\nBatch 261/261 - Validation Loss: 0.044867489486932755\nEpoch 7 - Average Validation Loss: 0.3971130322536518\nEpoch 8/10\nBatch 1/2082 - Train Loss: 0.33506426215171814\nBatch 11/2082 - Train Loss: 0.5728403329849243\nBatch 21/2082 - Train Loss: 0.3993198573589325\nBatch 31/2082 - Train Loss: 0.2653285264968872\nBatch 41/2082 - Train Loss: 0.21383346617221832\nBatch 51/2082 - Train Loss: 0.32703500986099243\nBatch 61/2082 - Train Loss: 0.5711817145347595\nBatch 71/2082 - Train Loss: 0.46385762095451355\nBatch 81/2082 - Train Loss: 0.6835744976997375\nBatch 91/2082 - Train Loss: 0.5357289910316467\nBatch 101/2082 - Train Loss: 0.3097621202468872\nBatch 111/2082 - Train Loss: 0.2175135612487793\nBatch 121/2082 - Train Loss: 0.35341891646385193\nBatch 131/2082 - Train Loss: 0.5563355684280396\nBatch 141/2082 - Train Loss: 0.2587374150753021\nBatch 151/2082 - Train Loss: 0.8887552618980408\nBatch 161/2082 - Train Loss: 0.19337458908557892\nBatch 171/2082 - Train Loss: 0.5330685973167419\nBatch 181/2082 - Train Loss: 0.5710975527763367\nBatch 191/2082 - Train Loss: 0.4402877390384674\nBatch 201/2082 - Train Loss: 0.26328855752944946\nBatch 211/2082 - Train Loss: 0.39470145106315613\nBatch 221/2082 - Train Loss: 0.6541751027107239\nBatch 231/2082 - Train Loss: 0.3255004584789276\nBatch 241/2082 - Train Loss: 0.25537967681884766\nBatch 251/2082 - Train Loss: 0.37779903411865234\nBatch 261/2082 - Train Loss: 0.35184088349342346\nBatch 271/2082 - Train Loss: 0.39629822969436646\nBatch 281/2082 - Train Loss: 0.2452734410762787\nBatch 291/2082 - Train Loss: 0.5985280871391296\nBatch 301/2082 - Train Loss: 0.3611336946487427\nBatch 311/2082 - Train Loss: 0.43744805455207825\nBatch 321/2082 - Train Loss: 0.325785756111145\nBatch 331/2082 - Train Loss: 0.15159402787685394\nBatch 341/2082 - Train Loss: 0.6605133414268494\nBatch 351/2082 - Train Loss: 0.4292265772819519\nBatch 361/2082 - Train Loss: 0.2963997721672058\nBatch 371/2082 - Train Loss: 0.5110187530517578\nBatch 381/2082 - Train Loss: 0.5608053803443909\nBatch 391/2082 - Train Loss: 0.5102481842041016\nBatch 401/2082 - Train Loss: 0.3654375672340393\nBatch 411/2082 - Train Loss: 0.22785621881484985\nBatch 421/2082 - Train Loss: 0.4050114154815674\nBatch 431/2082 - Train Loss: 0.15480953454971313\nBatch 441/2082 - Train Loss: 0.33839333057403564\nBatch 451/2082 - Train Loss: 0.38006123900413513\nBatch 461/2082 - Train Loss: 0.2926192581653595\nBatch 471/2082 - Train Loss: 0.3578248620033264\nBatch 481/2082 - Train Loss: 0.49068668484687805\nBatch 491/2082 - Train Loss: 0.46295884251594543\nBatch 501/2082 - Train Loss: 0.46959543228149414\nBatch 511/2082 - Train Loss: 0.3773818016052246\nBatch 521/2082 - Train Loss: 0.3557891845703125\nBatch 531/2082 - Train Loss: 0.4230242371559143\nBatch 541/2082 - Train Loss: 0.38837775588035583\nBatch 551/2082 - Train Loss: 0.4589274525642395\nBatch 561/2082 - Train Loss: 0.47287532687187195\nBatch 571/2082 - Train Loss: 0.3931181728839874\nBatch 581/2082 - Train Loss: 0.7714568376541138\nBatch 591/2082 - Train Loss: 0.40890100598335266\nBatch 601/2082 - Train Loss: 0.43844670057296753\nBatch 611/2082 - Train Loss: 0.7377867102622986\nBatch 621/2082 - Train Loss: 0.25981366634368896\nBatch 631/2082 - Train Loss: 0.47500184178352356\nBatch 641/2082 - Train Loss: 0.2588321566581726\nBatch 651/2082 - Train Loss: 0.4135728180408478\nBatch 661/2082 - Train Loss: 0.17265304923057556\nBatch 671/2082 - Train Loss: 0.24317115545272827\nBatch 681/2082 - Train Loss: 0.364253968000412\nBatch 691/2082 - Train Loss: 0.22865255177021027\nBatch 701/2082 - Train Loss: 0.4835854172706604\nBatch 711/2082 - Train Loss: 0.7297742962837219\nBatch 721/2082 - Train Loss: 0.38351190090179443\nBatch 731/2082 - Train Loss: 0.19716177880764008\nBatch 741/2082 - Train Loss: 0.35908740758895874\nBatch 751/2082 - Train Loss: 0.2928963005542755\nBatch 761/2082 - Train Loss: 0.41097503900527954\nBatch 771/2082 - Train Loss: 0.27737972140312195\nBatch 781/2082 - Train Loss: 0.26900655031204224\nBatch 791/2082 - Train Loss: 0.26421430706977844\nBatch 801/2082 - Train Loss: 0.16456300020217896\nBatch 811/2082 - Train Loss: 0.9189644455909729\nBatch 821/2082 - Train Loss: 0.6150312423706055\nBatch 831/2082 - Train Loss: 0.2928740084171295\nBatch 841/2082 - Train Loss: 0.217766672372818\nBatch 851/2082 - Train Loss: 0.2680780291557312\nBatch 861/2082 - Train Loss: 0.797897458076477\nBatch 871/2082 - Train Loss: 0.555269181728363\nBatch 881/2082 - Train Loss: 0.4267415404319763\nBatch 891/2082 - Train Loss: 0.4394850730895996\nBatch 901/2082 - Train Loss: 0.6301481127738953\nBatch 911/2082 - Train Loss: 0.23996978998184204\nBatch 921/2082 - Train Loss: 0.5757572054862976\nBatch 931/2082 - Train Loss: 0.42962345480918884\nBatch 941/2082 - Train Loss: 0.31419286131858826\nBatch 951/2082 - Train Loss: 0.16785694658756256\nBatch 961/2082 - Train Loss: 0.4892040491104126\nBatch 971/2082 - Train Loss: 0.6432861089706421\nBatch 981/2082 - Train Loss: 0.27595254778862\nBatch 991/2082 - Train Loss: 0.5685606598854065\nBatch 1001/2082 - Train Loss: 0.5293757915496826\nBatch 1011/2082 - Train Loss: 0.5972508192062378\nBatch 1021/2082 - Train Loss: 0.6935222744941711\nBatch 1031/2082 - Train Loss: 0.473209947347641\nBatch 1041/2082 - Train Loss: 0.24802568554878235\nBatch 1051/2082 - Train Loss: 0.8033135533332825\nBatch 1061/2082 - Train Loss: 0.5271926522254944\nBatch 1071/2082 - Train Loss: 0.344448059797287\nBatch 1081/2082 - Train Loss: 0.307907372713089\nBatch 1091/2082 - Train Loss: 0.6488510370254517\nBatch 1101/2082 - Train Loss: 0.2550685405731201\nBatch 1111/2082 - Train Loss: 0.3334888219833374\nBatch 1121/2082 - Train Loss: 0.2648908495903015\nBatch 1131/2082 - Train Loss: 0.4002998471260071\nBatch 1141/2082 - Train Loss: 0.2590845227241516\nBatch 1151/2082 - Train Loss: 0.4496641159057617\nBatch 1161/2082 - Train Loss: 0.7213529944419861\nBatch 1171/2082 - Train Loss: 0.17419379949569702\nBatch 1181/2082 - Train Loss: 0.5527294278144836\nBatch 1191/2082 - Train Loss: 0.42456984519958496\nBatch 1201/2082 - Train Loss: 0.7140202522277832\nBatch 1211/2082 - Train Loss: 0.18245461583137512\nBatch 1221/2082 - Train Loss: 0.2933104336261749\nBatch 1231/2082 - Train Loss: 0.1645052134990692\nBatch 1241/2082 - Train Loss: 0.35850346088409424\nBatch 1251/2082 - Train Loss: 0.4682128429412842\nBatch 1261/2082 - Train Loss: 0.4232015907764435\nBatch 1271/2082 - Train Loss: 0.49580806493759155\nBatch 1281/2082 - Train Loss: 0.5193781852722168\nBatch 1291/2082 - Train Loss: 0.3031967580318451\nBatch 1301/2082 - Train Loss: 0.2787630259990692\nBatch 1311/2082 - Train Loss: 0.25453004240989685\nBatch 1321/2082 - Train Loss: 0.5445725321769714\nBatch 1331/2082 - Train Loss: 0.4990374743938446\nBatch 1341/2082 - Train Loss: 0.4014887511730194\nBatch 1351/2082 - Train Loss: 0.5026441216468811\nBatch 1361/2082 - Train Loss: 0.17035025358200073\nBatch 1371/2082 - Train Loss: 0.41565099358558655\nBatch 1381/2082 - Train Loss: 0.21639226377010345\nBatch 1391/2082 - Train Loss: 0.29730719327926636\nBatch 1401/2082 - Train Loss: 0.2949959337711334\nBatch 1411/2082 - Train Loss: 0.4213479459285736\nBatch 1421/2082 - Train Loss: 0.8337733149528503\nBatch 1431/2082 - Train Loss: 0.354474812746048\nBatch 1441/2082 - Train Loss: 0.4424203932285309\nBatch 1451/2082 - Train Loss: 0.16129715740680695\nBatch 1461/2082 - Train Loss: 0.2541404962539673\nBatch 1471/2082 - Train Loss: 0.37424322962760925\nBatch 1481/2082 - Train Loss: 0.521382749080658\nBatch 1491/2082 - Train Loss: 0.20572645962238312\nBatch 1501/2082 - Train Loss: 0.357068806886673\nBatch 1511/2082 - Train Loss: 0.24812522530555725\nBatch 1521/2082 - Train Loss: 0.2374013215303421\nBatch 1531/2082 - Train Loss: 0.3063860237598419\nBatch 1541/2082 - Train Loss: 0.2959297001361847\nBatch 1551/2082 - Train Loss: 0.4524063766002655\nBatch 1561/2082 - Train Loss: 0.3634975254535675\nBatch 1571/2082 - Train Loss: 0.482546329498291\nBatch 1581/2082 - Train Loss: 0.49271729588508606\nBatch 1591/2082 - Train Loss: 0.2988426089286804\nBatch 1601/2082 - Train Loss: 0.252328097820282\nBatch 1611/2082 - Train Loss: 0.3648669421672821\nBatch 1621/2082 - Train Loss: 0.3793651759624481\nBatch 1631/2082 - Train Loss: 0.31310802698135376\nBatch 1641/2082 - Train Loss: 0.5672144293785095\nBatch 1651/2082 - Train Loss: 0.19292064011096954\nBatch 1661/2082 - Train Loss: 0.3079877495765686\nBatch 1671/2082 - Train Loss: 0.3418121337890625\nBatch 1681/2082 - Train Loss: 0.43141159415245056\nBatch 1691/2082 - Train Loss: 0.2635144889354706\nBatch 1701/2082 - Train Loss: 0.46672266721725464\nBatch 1711/2082 - Train Loss: 0.49413782358169556\nBatch 1721/2082 - Train Loss: 0.33650267124176025\nBatch 1731/2082 - Train Loss: 0.2507166862487793\nBatch 1741/2082 - Train Loss: 0.3159302771091461\nBatch 1751/2082 - Train Loss: 0.25106823444366455\nBatch 1761/2082 - Train Loss: 0.4065956473350525\nBatch 1771/2082 - Train Loss: 0.3493485450744629\nBatch 1781/2082 - Train Loss: 0.6139070987701416\nBatch 1791/2082 - Train Loss: 0.27677300572395325\nBatch 1801/2082 - Train Loss: 0.45874613523483276\nBatch 1811/2082 - Train Loss: 0.202696293592453\nBatch 1821/2082 - Train Loss: 0.41734111309051514\nBatch 1831/2082 - Train Loss: 0.2839565575122833\nBatch 1841/2082 - Train Loss: 0.6821807622909546\nBatch 1851/2082 - Train Loss: 0.6519755125045776\nBatch 1861/2082 - Train Loss: 0.22080130875110626\nBatch 1871/2082 - Train Loss: 0.19255821406841278\nBatch 1881/2082 - Train Loss: 0.32964009046554565\nBatch 1891/2082 - Train Loss: 0.46059533953666687\nBatch 1901/2082 - Train Loss: 0.6306039690971375\nBatch 1911/2082 - Train Loss: 0.39998921751976013\nBatch 1921/2082 - Train Loss: 0.3009699285030365\nBatch 1931/2082 - Train Loss: 0.18573719263076782\nBatch 1941/2082 - Train Loss: 0.3216552138328552\nBatch 1951/2082 - Train Loss: 0.332246869802475\nBatch 1961/2082 - Train Loss: 0.22780433297157288\nBatch 1971/2082 - Train Loss: 0.1466868370771408\nBatch 1981/2082 - Train Loss: 0.22801093757152557\nBatch 1991/2082 - Train Loss: 0.19694221019744873\nBatch 2001/2082 - Train Loss: 0.17927990853786469\nBatch 2011/2082 - Train Loss: 0.47612717747688293\nBatch 2021/2082 - Train Loss: 0.46455928683280945\nBatch 2031/2082 - Train Loss: 0.5710439085960388\nBatch 2041/2082 - Train Loss: 0.32888126373291016\nBatch 2051/2082 - Train Loss: 0.39212676882743835\nBatch 2061/2082 - Train Loss: 0.362624853849411\nBatch 2071/2082 - Train Loss: 0.27324217557907104\nBatch 2081/2082 - Train Loss: 0.46269744634628296\nEpoch 8 - Average Train Loss: 0.3973791085927963\nBatch 1/261 - Validation Loss: 0.07372908294200897\nBatch 11/261 - Validation Loss: 0.44959476590156555\nBatch 21/261 - Validation Loss: 0.2938525378704071\nBatch 31/261 - Validation Loss: 0.3131752610206604\nBatch 41/261 - Validation Loss: 0.435130774974823\nBatch 51/261 - Validation Loss: 0.4105691611766815\nBatch 61/261 - Validation Loss: 0.2636966109275818\nBatch 71/261 - Validation Loss: 0.15467554330825806\nBatch 81/261 - Validation Loss: 0.4977720081806183\nBatch 91/261 - Validation Loss: 0.5131924748420715\nBatch 101/261 - Validation Loss: 0.381019651889801\nBatch 111/261 - Validation Loss: 0.5079770088195801\nBatch 121/261 - Validation Loss: 0.19562339782714844\nBatch 131/261 - Validation Loss: 0.7029385566711426\nBatch 141/261 - Validation Loss: 0.632213294506073\nBatch 151/261 - Validation Loss: 0.5203675627708435\nBatch 161/261 - Validation Loss: 0.3535648584365845\nBatch 171/261 - Validation Loss: 0.6845202445983887\nBatch 181/261 - Validation Loss: 0.38738757371902466\nBatch 191/261 - Validation Loss: 0.472715824842453\nBatch 201/261 - Validation Loss: 0.7872429490089417\nBatch 211/261 - Validation Loss: 0.2577826678752899\nBatch 221/261 - Validation Loss: 0.11991635710000992\nBatch 231/261 - Validation Loss: 0.15354816615581512\nBatch 241/261 - Validation Loss: 0.25271207094192505\nBatch 251/261 - Validation Loss: 0.6532431840896606\nBatch 261/261 - Validation Loss: 0.04702898859977722\nEpoch 8 - Average Validation Loss: 0.3956038310438737\nEpoch 9/10\nBatch 1/2082 - Train Loss: 0.2787672281265259\nBatch 11/2082 - Train Loss: 0.3216143250465393\nBatch 21/2082 - Train Loss: 0.3342341184616089\nBatch 31/2082 - Train Loss: 0.27451291680336\nBatch 41/2082 - Train Loss: 0.5490717887878418\nBatch 51/2082 - Train Loss: 0.46450987458229065\nBatch 61/2082 - Train Loss: 0.4017361104488373\nBatch 71/2082 - Train Loss: 0.3987070620059967\nBatch 81/2082 - Train Loss: 0.4242558777332306\nBatch 91/2082 - Train Loss: 0.36393284797668457\nBatch 101/2082 - Train Loss: 0.665178120136261\nBatch 111/2082 - Train Loss: 0.12914130091667175\nBatch 121/2082 - Train Loss: 0.2834291458129883\nBatch 131/2082 - Train Loss: 0.25270307064056396\nBatch 141/2082 - Train Loss: 0.3440793752670288\nBatch 151/2082 - Train Loss: 0.6682746410369873\nBatch 161/2082 - Train Loss: 0.4976007044315338\nBatch 171/2082 - Train Loss: 0.436034232378006\nBatch 181/2082 - Train Loss: 0.35533407330513\nBatch 191/2082 - Train Loss: 0.5129241943359375\nBatch 201/2082 - Train Loss: 0.35042059421539307\nBatch 211/2082 - Train Loss: 0.43576812744140625\nBatch 221/2082 - Train Loss: 0.3446101248264313\nBatch 231/2082 - Train Loss: 0.2712509334087372\nBatch 241/2082 - Train Loss: 0.5781896114349365\nBatch 251/2082 - Train Loss: 0.2992211580276489\nBatch 261/2082 - Train Loss: 0.49997493624687195\nBatch 271/2082 - Train Loss: 0.27318787574768066\nBatch 281/2082 - Train Loss: 0.22831445932388306\nBatch 291/2082 - Train Loss: 0.3523205518722534\nBatch 301/2082 - Train Loss: 0.3230995833873749\nBatch 311/2082 - Train Loss: 0.3872780203819275\nBatch 321/2082 - Train Loss: 0.28563088178634644\nBatch 331/2082 - Train Loss: 0.16838139295578003\nBatch 341/2082 - Train Loss: 0.19121557474136353\nBatch 351/2082 - Train Loss: 0.26341161131858826\nBatch 361/2082 - Train Loss: 0.3199021518230438\nBatch 371/2082 - Train Loss: 0.2899710237979889\nBatch 381/2082 - Train Loss: 0.26342400908470154\nBatch 391/2082 - Train Loss: 0.308829128742218\nBatch 401/2082 - Train Loss: 0.2168528437614441\nBatch 411/2082 - Train Loss: 0.18762601912021637\nBatch 421/2082 - Train Loss: 0.5542232990264893\nBatch 431/2082 - Train Loss: 0.41072213649749756\nBatch 441/2082 - Train Loss: 0.269339382648468\nBatch 451/2082 - Train Loss: 0.6219049096107483\nBatch 461/2082 - Train Loss: 0.40246063470840454\nBatch 471/2082 - Train Loss: 0.7429733276367188\nBatch 481/2082 - Train Loss: 0.21293792128562927\nBatch 491/2082 - Train Loss: 0.18806177377700806\nBatch 501/2082 - Train Loss: 0.3517592251300812\nBatch 511/2082 - Train Loss: 0.4387168288230896\nBatch 521/2082 - Train Loss: 0.3759940564632416\nBatch 531/2082 - Train Loss: 0.6537545919418335\nBatch 541/2082 - Train Loss: 0.6990493535995483\nBatch 551/2082 - Train Loss: 0.39910218119621277\nBatch 561/2082 - Train Loss: 0.19273583590984344\nBatch 571/2082 - Train Loss: 0.21387222409248352\nBatch 581/2082 - Train Loss: 0.20864072442054749\nBatch 591/2082 - Train Loss: 0.7399720549583435\nBatch 601/2082 - Train Loss: 0.2610812783241272\nBatch 611/2082 - Train Loss: 0.4508970081806183\nBatch 621/2082 - Train Loss: 0.11044430732727051\nBatch 631/2082 - Train Loss: 0.3606150448322296\nBatch 641/2082 - Train Loss: 0.21937140822410583\nBatch 651/2082 - Train Loss: 0.3539693355560303\nBatch 661/2082 - Train Loss: 0.19303984940052032\nBatch 671/2082 - Train Loss: 0.2982804775238037\nBatch 681/2082 - Train Loss: 0.6049491167068481\nBatch 691/2082 - Train Loss: 0.49397221207618713\nBatch 701/2082 - Train Loss: 0.45078572630882263\nBatch 711/2082 - Train Loss: 0.3495732545852661\nBatch 721/2082 - Train Loss: 0.5957202911376953\nBatch 731/2082 - Train Loss: 0.3342595100402832\nBatch 741/2082 - Train Loss: 0.42638882994651794\nBatch 751/2082 - Train Loss: 0.1622716784477234\nBatch 761/2082 - Train Loss: 0.29653021693229675\nBatch 771/2082 - Train Loss: 0.4779755771160126\nBatch 781/2082 - Train Loss: 0.502934992313385\nBatch 791/2082 - Train Loss: 0.49431532621383667\nBatch 801/2082 - Train Loss: 0.33956047892570496\nBatch 811/2082 - Train Loss: 0.46505945920944214\nBatch 821/2082 - Train Loss: 0.47573232650756836\nBatch 831/2082 - Train Loss: 0.18710453808307648\nBatch 841/2082 - Train Loss: 0.11651927977800369\nBatch 851/2082 - Train Loss: 0.4776184558868408\nBatch 861/2082 - Train Loss: 0.684556245803833\nBatch 871/2082 - Train Loss: 0.6333801746368408\nBatch 881/2082 - Train Loss: 0.3762058913707733\nBatch 891/2082 - Train Loss: 0.6009527444839478\nBatch 901/2082 - Train Loss: 0.37070176005363464\nBatch 911/2082 - Train Loss: 0.5178853869438171\nBatch 921/2082 - Train Loss: 0.5614309906959534\nBatch 931/2082 - Train Loss: 0.3630657196044922\nBatch 941/2082 - Train Loss: 0.37295666337013245\nBatch 951/2082 - Train Loss: 0.2787688076496124\nBatch 961/2082 - Train Loss: 0.3299485743045807\nBatch 971/2082 - Train Loss: 0.27512261271476746\nBatch 981/2082 - Train Loss: 0.43316850066185\nBatch 991/2082 - Train Loss: 0.5436193943023682\nBatch 1001/2082 - Train Loss: 0.5473867654800415\nBatch 1011/2082 - Train Loss: 0.4973471164703369\nBatch 1021/2082 - Train Loss: 0.4116405248641968\nBatch 1031/2082 - Train Loss: 0.44158726930618286\nBatch 1041/2082 - Train Loss: 0.26031965017318726\nBatch 1051/2082 - Train Loss: 0.6203669905662537\nBatch 1061/2082 - Train Loss: 0.2885565459728241\nBatch 1071/2082 - Train Loss: 0.38874876499176025\nBatch 1081/2082 - Train Loss: 0.32221531867980957\nBatch 1091/2082 - Train Loss: 0.4368263781070709\nBatch 1101/2082 - Train Loss: 0.5342974066734314\nBatch 1111/2082 - Train Loss: 0.32104626297950745\nBatch 1121/2082 - Train Loss: 0.2564278841018677\nBatch 1131/2082 - Train Loss: 0.085725799202919\nBatch 1141/2082 - Train Loss: 0.4671502709388733\nBatch 1151/2082 - Train Loss: 0.3716372549533844\nBatch 1161/2082 - Train Loss: 0.3669404983520508\nBatch 1171/2082 - Train Loss: 0.37361669540405273\nBatch 1181/2082 - Train Loss: 0.4255301058292389\nBatch 1191/2082 - Train Loss: 0.513758659362793\nBatch 1201/2082 - Train Loss: 0.4043557643890381\nBatch 1211/2082 - Train Loss: 0.22503788769245148\nBatch 1221/2082 - Train Loss: 0.39684489369392395\nBatch 1231/2082 - Train Loss: 0.3910852074623108\nBatch 1241/2082 - Train Loss: 0.481047660112381\nBatch 1251/2082 - Train Loss: 0.3071330189704895\nBatch 1261/2082 - Train Loss: 0.4001128077507019\nBatch 1271/2082 - Train Loss: 0.30792152881622314\nBatch 1281/2082 - Train Loss: 0.6076457500457764\nBatch 1291/2082 - Train Loss: 0.35321083664894104\nBatch 1301/2082 - Train Loss: 0.4185573160648346\nBatch 1311/2082 - Train Loss: 0.45278576016426086\nBatch 1321/2082 - Train Loss: 0.12169802188873291\nBatch 1331/2082 - Train Loss: 0.41940632462501526\nBatch 1341/2082 - Train Loss: 0.39397555589675903\nBatch 1351/2082 - Train Loss: 0.4610450267791748\nBatch 1361/2082 - Train Loss: 0.6283820867538452\nBatch 1371/2082 - Train Loss: 0.3888111412525177\nBatch 1381/2082 - Train Loss: 0.7138843536376953\nBatch 1391/2082 - Train Loss: 0.3078594505786896\nBatch 1401/2082 - Train Loss: 0.6551178097724915\nBatch 1411/2082 - Train Loss: 0.2386888563632965\nBatch 1421/2082 - Train Loss: 0.18615061044692993\nBatch 1431/2082 - Train Loss: 0.36072057485580444\nBatch 1441/2082 - Train Loss: 0.5458954572677612\nBatch 1451/2082 - Train Loss: 0.649786651134491\nBatch 1461/2082 - Train Loss: 0.452781081199646\nBatch 1471/2082 - Train Loss: 0.3874851167201996\nBatch 1481/2082 - Train Loss: 0.36931923031806946\nBatch 1491/2082 - Train Loss: 0.23158414661884308\nBatch 1501/2082 - Train Loss: 0.3288213610649109\nBatch 1511/2082 - Train Loss: 0.2856459319591522\nBatch 1521/2082 - Train Loss: 0.5106617212295532\nBatch 1531/2082 - Train Loss: 0.3232121467590332\nBatch 1541/2082 - Train Loss: 0.46250757575035095\nBatch 1551/2082 - Train Loss: 0.26699984073638916\nBatch 1561/2082 - Train Loss: 0.24223166704177856\nBatch 1571/2082 - Train Loss: 0.3488854169845581\nBatch 1581/2082 - Train Loss: 0.40940022468566895\nBatch 1591/2082 - Train Loss: 0.3105349540710449\nBatch 1601/2082 - Train Loss: 0.3232046961784363\nBatch 1611/2082 - Train Loss: 0.678254246711731\nBatch 1621/2082 - Train Loss: 0.23502492904663086\nBatch 1631/2082 - Train Loss: 0.419025719165802\nBatch 1641/2082 - Train Loss: 0.5487425327301025\nBatch 1651/2082 - Train Loss: 0.34126031398773193\nBatch 1661/2082 - Train Loss: 0.18073523044586182\nBatch 1671/2082 - Train Loss: 0.30113041400909424\nBatch 1681/2082 - Train Loss: 0.4134478271007538\nBatch 1691/2082 - Train Loss: 0.3112601637840271\nBatch 1701/2082 - Train Loss: 0.6200467348098755\nBatch 1711/2082 - Train Loss: 0.34834691882133484\nBatch 1721/2082 - Train Loss: 0.40882083773612976\nBatch 1731/2082 - Train Loss: 0.4745182394981384\nBatch 1741/2082 - Train Loss: 0.36319026350975037\nBatch 1751/2082 - Train Loss: 0.08004990965127945\nBatch 1761/2082 - Train Loss: 0.5312496423721313\nBatch 1771/2082 - Train Loss: 0.1263413280248642\nBatch 1781/2082 - Train Loss: 0.528833270072937\nBatch 1791/2082 - Train Loss: 0.4247414171695709\nBatch 1801/2082 - Train Loss: 0.3712441027164459\nBatch 1811/2082 - Train Loss: 0.5368484258651733\nBatch 1821/2082 - Train Loss: 0.12261586636304855\nBatch 1831/2082 - Train Loss: 0.1952863186597824\nBatch 1841/2082 - Train Loss: 0.3425118327140808\nBatch 1851/2082 - Train Loss: 0.3516903519630432\nBatch 1861/2082 - Train Loss: 0.26980060338974\nBatch 1871/2082 - Train Loss: 0.5967458486557007\nBatch 1881/2082 - Train Loss: 0.15226510167121887\nBatch 1891/2082 - Train Loss: 0.24234749376773834\nBatch 1901/2082 - Train Loss: 0.39018669724464417\nBatch 1911/2082 - Train Loss: 0.48292481899261475\nBatch 1921/2082 - Train Loss: 0.7026514410972595\nBatch 1931/2082 - Train Loss: 0.46388769149780273\nBatch 1941/2082 - Train Loss: 0.5139310956001282\nBatch 1951/2082 - Train Loss: 0.5777840614318848\nBatch 1961/2082 - Train Loss: 0.28442564606666565\nBatch 1971/2082 - Train Loss: 0.5574192404747009\nBatch 1981/2082 - Train Loss: 0.17531421780586243\nBatch 1991/2082 - Train Loss: 0.2227887213230133\nBatch 2001/2082 - Train Loss: 0.23360562324523926\nBatch 2011/2082 - Train Loss: 0.20559608936309814\nBatch 2021/2082 - Train Loss: 0.17312069237232208\nBatch 2031/2082 - Train Loss: 0.6356765627861023\nBatch 2041/2082 - Train Loss: 0.40142759680747986\nBatch 2051/2082 - Train Loss: 0.5240480899810791\nBatch 2061/2082 - Train Loss: 0.26058244705200195\nBatch 2071/2082 - Train Loss: 0.2658786475658417\nBatch 2081/2082 - Train Loss: 0.27670562267303467\nEpoch 9 - Average Train Loss: 0.39027578016522985\nBatch 1/261 - Validation Loss: 0.07536181807518005\nBatch 11/261 - Validation Loss: 0.44012826681137085\nBatch 21/261 - Validation Loss: 0.28464624285697937\nBatch 31/261 - Validation Loss: 0.3134967088699341\nBatch 41/261 - Validation Loss: 0.43065816164016724\nBatch 51/261 - Validation Loss: 0.40317782759666443\nBatch 61/261 - Validation Loss: 0.2665136754512787\nBatch 71/261 - Validation Loss: 0.15351907908916473\nBatch 81/261 - Validation Loss: 0.49033990502357483\nBatch 91/261 - Validation Loss: 0.5045315623283386\nBatch 101/261 - Validation Loss: 0.38265353441238403\nBatch 111/261 - Validation Loss: 0.5032081604003906\nBatch 121/261 - Validation Loss: 0.1938391923904419\nBatch 131/261 - Validation Loss: 0.6964850425720215\nBatch 141/261 - Validation Loss: 0.6204541325569153\nBatch 151/261 - Validation Loss: 0.5108550786972046\nBatch 161/261 - Validation Loss: 0.3487814664840698\nBatch 171/261 - Validation Loss: 0.6731017827987671\nBatch 181/261 - Validation Loss: 0.3924441337585449\nBatch 191/261 - Validation Loss: 0.4675160348415375\nBatch 201/261 - Validation Loss: 0.776881992816925\nBatch 211/261 - Validation Loss: 0.24912899732589722\nBatch 221/261 - Validation Loss: 0.11749127507209778\nBatch 231/261 - Validation Loss: 0.1538405865430832\nBatch 241/261 - Validation Loss: 0.2458827644586563\nBatch 251/261 - Validation Loss: 0.6482174396514893\nBatch 261/261 - Validation Loss: 0.043537672609090805\nEpoch 9 - Average Validation Loss: 0.3914139580509672\nEpoch 10/10\nBatch 1/2082 - Train Loss: 0.27357739210128784\nBatch 11/2082 - Train Loss: 0.45859742164611816\nBatch 21/2082 - Train Loss: 0.6780551075935364\nBatch 31/2082 - Train Loss: 0.5682100653648376\nBatch 41/2082 - Train Loss: 0.4249641001224518\nBatch 51/2082 - Train Loss: 0.4031778872013092\nBatch 61/2082 - Train Loss: 0.36146432161331177\nBatch 71/2082 - Train Loss: 0.2290761023759842\nBatch 81/2082 - Train Loss: 0.6336041688919067\nBatch 91/2082 - Train Loss: 0.55946946144104\nBatch 101/2082 - Train Loss: 0.22724352777004242\nBatch 111/2082 - Train Loss: 0.1100086197257042\nBatch 121/2082 - Train Loss: 0.2490491420030594\nBatch 131/2082 - Train Loss: 0.4008016586303711\nBatch 141/2082 - Train Loss: 0.1995551735162735\nBatch 151/2082 - Train Loss: 0.2773033380508423\nBatch 161/2082 - Train Loss: 0.6274704337120056\nBatch 171/2082 - Train Loss: 0.3100401759147644\nBatch 181/2082 - Train Loss: 0.5290768146514893\nBatch 191/2082 - Train Loss: 0.2200363725423813\nBatch 201/2082 - Train Loss: 0.3199780583381653\nBatch 211/2082 - Train Loss: 0.621258020401001\nBatch 221/2082 - Train Loss: 0.16439315676689148\nBatch 231/2082 - Train Loss: 0.5570346713066101\nBatch 241/2082 - Train Loss: 0.3174356520175934\nBatch 251/2082 - Train Loss: 0.31594252586364746\nBatch 261/2082 - Train Loss: 0.5794103145599365\nBatch 271/2082 - Train Loss: 0.1238640695810318\nBatch 281/2082 - Train Loss: 0.26136311888694763\nBatch 291/2082 - Train Loss: 0.4141373932361603\nBatch 301/2082 - Train Loss: 0.4816945791244507\nBatch 311/2082 - Train Loss: 0.6577973961830139\nBatch 321/2082 - Train Loss: 0.272522509098053\nBatch 331/2082 - Train Loss: 0.3506172299385071\nBatch 341/2082 - Train Loss: 0.4022281765937805\nBatch 351/2082 - Train Loss: 0.3824423849582672\nBatch 361/2082 - Train Loss: 0.5753546357154846\nBatch 371/2082 - Train Loss: 0.4407622814178467\nBatch 381/2082 - Train Loss: 0.2796500325202942\nBatch 391/2082 - Train Loss: 0.31085672974586487\nBatch 401/2082 - Train Loss: 0.2973938584327698\nBatch 411/2082 - Train Loss: 0.4472396969795227\nBatch 421/2082 - Train Loss: 0.3657780587673187\nBatch 431/2082 - Train Loss: 0.17206630110740662\nBatch 441/2082 - Train Loss: 0.5779293775558472\nBatch 451/2082 - Train Loss: 0.14631514251232147\nBatch 461/2082 - Train Loss: 0.44426316022872925\nBatch 471/2082 - Train Loss: 0.17661991715431213\nBatch 481/2082 - Train Loss: 0.5884760618209839\nBatch 491/2082 - Train Loss: 0.3300178647041321\nBatch 501/2082 - Train Loss: 0.5806040167808533\nBatch 511/2082 - Train Loss: 0.40647661685943604\nBatch 521/2082 - Train Loss: 0.5391721129417419\nBatch 531/2082 - Train Loss: 0.4744459092617035\nBatch 541/2082 - Train Loss: 0.328644335269928\nBatch 551/2082 - Train Loss: 0.3152886629104614\nBatch 561/2082 - Train Loss: 0.5771534442901611\nBatch 571/2082 - Train Loss: 0.3061771094799042\nBatch 581/2082 - Train Loss: 0.7339872121810913\nBatch 591/2082 - Train Loss: 0.7136755585670471\nBatch 601/2082 - Train Loss: 0.3337974548339844\nBatch 611/2082 - Train Loss: 0.264543354511261\nBatch 621/2082 - Train Loss: 0.7799108624458313\nBatch 631/2082 - Train Loss: 0.29208484292030334\nBatch 641/2082 - Train Loss: 0.5399143099784851\nBatch 651/2082 - Train Loss: 0.23008428514003754\nBatch 661/2082 - Train Loss: 0.5191463828086853\nBatch 671/2082 - Train Loss: 0.2766144871711731\nBatch 681/2082 - Train Loss: 0.23418332636356354\nBatch 691/2082 - Train Loss: 0.2931859493255615\nBatch 701/2082 - Train Loss: 0.31499385833740234\nBatch 711/2082 - Train Loss: 0.5591457486152649\nBatch 721/2082 - Train Loss: 0.636146068572998\nBatch 731/2082 - Train Loss: 0.47469985485076904\nBatch 741/2082 - Train Loss: 0.34287700057029724\nBatch 751/2082 - Train Loss: 0.2314499169588089\nBatch 761/2082 - Train Loss: 0.4914286434650421\nBatch 771/2082 - Train Loss: 0.07436325401067734\nBatch 781/2082 - Train Loss: 0.42500147223472595\nBatch 791/2082 - Train Loss: 0.30412307381629944\nBatch 801/2082 - Train Loss: 0.6338406205177307\nBatch 811/2082 - Train Loss: 0.09032895416021347\nBatch 821/2082 - Train Loss: 0.26264631748199463\nBatch 831/2082 - Train Loss: 0.2876436114311218\nBatch 841/2082 - Train Loss: 0.25061529874801636\nBatch 851/2082 - Train Loss: 0.5157391428947449\nBatch 861/2082 - Train Loss: 0.2728070616722107\nBatch 871/2082 - Train Loss: 0.41189849376678467\nBatch 881/2082 - Train Loss: 0.32365846633911133\nBatch 891/2082 - Train Loss: 0.4120188057422638\nBatch 901/2082 - Train Loss: 0.4569328725337982\nBatch 911/2082 - Train Loss: 0.36402419209480286\nBatch 921/2082 - Train Loss: 0.5658552646636963\nBatch 931/2082 - Train Loss: 0.3832298219203949\nBatch 941/2082 - Train Loss: 0.5122736692428589\nBatch 951/2082 - Train Loss: 0.42708754539489746\nBatch 961/2082 - Train Loss: 0.30370965600013733\nBatch 971/2082 - Train Loss: 0.15608082711696625\nBatch 981/2082 - Train Loss: 0.3252786695957184\nBatch 991/2082 - Train Loss: 0.4580499827861786\nBatch 1001/2082 - Train Loss: 0.5573984980583191\nBatch 1011/2082 - Train Loss: 0.33804622292518616\nBatch 1021/2082 - Train Loss: 0.4110896587371826\nBatch 1031/2082 - Train Loss: 0.4730986952781677\nBatch 1041/2082 - Train Loss: 0.28717240691185\nBatch 1051/2082 - Train Loss: 0.3359884023666382\nBatch 1061/2082 - Train Loss: 0.42902469635009766\nBatch 1071/2082 - Train Loss: 0.2627847492694855\nBatch 1081/2082 - Train Loss: 0.4290340542793274\nBatch 1091/2082 - Train Loss: 0.3500151038169861\nBatch 1101/2082 - Train Loss: 0.5334616899490356\nBatch 1111/2082 - Train Loss: 0.23011866211891174\nBatch 1121/2082 - Train Loss: 0.13405749201774597\nBatch 1131/2082 - Train Loss: 0.39465469121932983\nBatch 1141/2082 - Train Loss: 0.42332297563552856\nBatch 1151/2082 - Train Loss: 0.6352646946907043\nBatch 1161/2082 - Train Loss: 0.3735020160675049\nBatch 1171/2082 - Train Loss: 0.4828137159347534\nBatch 1181/2082 - Train Loss: 0.13398762047290802\nBatch 1191/2082 - Train Loss: 0.46815475821495056\nBatch 1201/2082 - Train Loss: 0.20267561078071594\nBatch 1211/2082 - Train Loss: 0.41034552454948425\nBatch 1221/2082 - Train Loss: 0.6194000244140625\nBatch 1231/2082 - Train Loss: 0.14324182271957397\nBatch 1241/2082 - Train Loss: 0.26352426409721375\nBatch 1251/2082 - Train Loss: 0.18789997696876526\nBatch 1261/2082 - Train Loss: 0.19217222929000854\nBatch 1271/2082 - Train Loss: 0.25481268763542175\nBatch 1281/2082 - Train Loss: 0.39772671461105347\nBatch 1291/2082 - Train Loss: 0.25785860419273376\nBatch 1301/2082 - Train Loss: 0.5208883285522461\nBatch 1311/2082 - Train Loss: 0.2932329773902893\nBatch 1321/2082 - Train Loss: 0.09673090279102325\nBatch 1331/2082 - Train Loss: 0.24733716249465942\nBatch 1341/2082 - Train Loss: 0.49151676893234253\nBatch 1351/2082 - Train Loss: 0.1650928109884262\nBatch 1361/2082 - Train Loss: 0.360337495803833\nBatch 1371/2082 - Train Loss: 0.35935768485069275\nBatch 1381/2082 - Train Loss: 0.3196447193622589\nBatch 1391/2082 - Train Loss: 0.3062514364719391\nBatch 1401/2082 - Train Loss: 0.347394734621048\nBatch 1411/2082 - Train Loss: 0.2270045429468155\nBatch 1421/2082 - Train Loss: 0.21532748639583588\nBatch 1431/2082 - Train Loss: 0.35580387711524963\nBatch 1441/2082 - Train Loss: 0.6357222199440002\nBatch 1451/2082 - Train Loss: 0.615138590335846\nBatch 1461/2082 - Train Loss: 0.21603423357009888\nBatch 1471/2082 - Train Loss: 0.3002515137195587\nBatch 1481/2082 - Train Loss: 0.5271048545837402\nBatch 1491/2082 - Train Loss: 0.3192479610443115\nBatch 1501/2082 - Train Loss: 0.5273820757865906\nBatch 1511/2082 - Train Loss: 0.6448691487312317\nBatch 1521/2082 - Train Loss: 0.14011450111865997\nBatch 1531/2082 - Train Loss: 0.3360743522644043\nBatch 1541/2082 - Train Loss: 0.42104393243789673\nBatch 1551/2082 - Train Loss: 0.1814383715391159\nBatch 1561/2082 - Train Loss: 0.4591498374938965\nBatch 1571/2082 - Train Loss: 0.278438925743103\nBatch 1581/2082 - Train Loss: 0.6580458879470825\nBatch 1591/2082 - Train Loss: 0.3211863040924072\nBatch 1601/2082 - Train Loss: 0.4708441197872162\nBatch 1611/2082 - Train Loss: 0.4719882607460022\nBatch 1621/2082 - Train Loss: 0.24178609251976013\nBatch 1631/2082 - Train Loss: 0.42702606320381165\nBatch 1641/2082 - Train Loss: 0.4234713912010193\nBatch 1651/2082 - Train Loss: 0.2539091408252716\nBatch 1661/2082 - Train Loss: 0.2431086003780365\nBatch 1671/2082 - Train Loss: 0.3112501800060272\nBatch 1681/2082 - Train Loss: 0.7072368264198303\nBatch 1691/2082 - Train Loss: 0.18213556706905365\nBatch 1701/2082 - Train Loss: 0.5169842839241028\nBatch 1711/2082 - Train Loss: 0.511719822883606\nBatch 1721/2082 - Train Loss: 0.372033953666687\nBatch 1731/2082 - Train Loss: 0.5117978453636169\nBatch 1741/2082 - Train Loss: 0.4412653148174286\nBatch 1751/2082 - Train Loss: 0.39405515789985657\nBatch 1761/2082 - Train Loss: 0.20838122069835663\nBatch 1771/2082 - Train Loss: 0.3940083384513855\nBatch 1781/2082 - Train Loss: 0.8032177686691284\nBatch 1791/2082 - Train Loss: 0.5697720050811768\nBatch 1801/2082 - Train Loss: 0.5834964513778687\nBatch 1811/2082 - Train Loss: 0.5088937878608704\nBatch 1821/2082 - Train Loss: 0.19854359328746796\nBatch 1831/2082 - Train Loss: 0.3602886199951172\nBatch 1841/2082 - Train Loss: 0.5833160877227783\nBatch 1851/2082 - Train Loss: 0.45631805062294006\nBatch 1861/2082 - Train Loss: 0.6021640300750732\nBatch 1871/2082 - Train Loss: 0.11250485479831696\nBatch 1881/2082 - Train Loss: 0.3768447935581207\nBatch 1891/2082 - Train Loss: 0.7743999361991882\nBatch 1901/2082 - Train Loss: 0.24738316237926483\nBatch 1911/2082 - Train Loss: 0.4187415838241577\nBatch 1921/2082 - Train Loss: 0.46616464853286743\nBatch 1931/2082 - Train Loss: 0.19770044088363647\nBatch 1941/2082 - Train Loss: 0.31587764620780945\nBatch 1951/2082 - Train Loss: 0.47185298800468445\nBatch 1961/2082 - Train Loss: 0.24455906450748444\nBatch 1971/2082 - Train Loss: 0.20339496433734894\nBatch 1981/2082 - Train Loss: 0.21816030144691467\nBatch 1991/2082 - Train Loss: 0.41593411564826965\nBatch 2001/2082 - Train Loss: 0.06812115013599396\nBatch 2011/2082 - Train Loss: 0.24964594841003418\nBatch 2021/2082 - Train Loss: 0.35298657417297363\nBatch 2031/2082 - Train Loss: 0.11958163976669312\nBatch 2041/2082 - Train Loss: 0.4169929027557373\nBatch 2051/2082 - Train Loss: 0.40209269523620605\nBatch 2061/2082 - Train Loss: 0.3841745853424072\nBatch 2071/2082 - Train Loss: 0.6320202350616455\nBatch 2081/2082 - Train Loss: 0.24949976801872253\nEpoch 10 - Average Train Loss: 0.38356291421301714\nBatch 1/261 - Validation Loss: 0.07416611164808273\nBatch 11/261 - Validation Loss: 0.4402827322483063\nBatch 21/261 - Validation Loss: 0.2780008316040039\nBatch 31/261 - Validation Loss: 0.30609849095344543\nBatch 41/261 - Validation Loss: 0.4283697009086609\nBatch 51/261 - Validation Loss: 0.402431458234787\nBatch 61/261 - Validation Loss: 0.2601269483566284\nBatch 71/261 - Validation Loss: 0.1482926309108734\nBatch 81/261 - Validation Loss: 0.48443302512168884\nBatch 91/261 - Validation Loss: 0.49621105194091797\nBatch 101/261 - Validation Loss: 0.37221115827560425\nBatch 111/261 - Validation Loss: 0.4979952573776245\nBatch 121/261 - Validation Loss: 0.1891344040632248\nBatch 131/261 - Validation Loss: 0.6870245933532715\nBatch 141/261 - Validation Loss: 0.6171460151672363\nBatch 151/261 - Validation Loss: 0.49958497285842896\nBatch 161/261 - Validation Loss: 0.3455868363380432\nBatch 171/261 - Validation Loss: 0.6697016954421997\nBatch 181/261 - Validation Loss: 0.388113409280777\nBatch 191/261 - Validation Loss: 0.4655334949493408\nBatch 201/261 - Validation Loss: 0.7714602947235107\nBatch 211/261 - Validation Loss: 0.251080721616745\nBatch 221/261 - Validation Loss: 0.11525902152061462\nBatch 231/261 - Validation Loss: 0.1512536257505417\nBatch 241/261 - Validation Loss: 0.2462960034608841\nBatch 251/261 - Validation Loss: 0.6425280570983887\nBatch 261/261 - Validation Loss: 0.0479605570435524\nEpoch 10 - Average Validation Loss: 0.38732730547301614\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"model_path = \"/kaggle/working/clean_t5_model_lower\"\n\n# Simpan model dan tokenizer\nmodel.save_pretrained(model_path)\ntokenizer.save_pretrained(model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T13:58:54.287271Z","iopub.execute_input":"2024-12-16T13:58:54.287558Z","iopub.status.idle":"2024-12-16T13:58:56.744165Z","shell.execute_reply.started":"2024-12-16T13:58:54.287533Z","shell.execute_reply":"2024-12-16T13:58:56.743302Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/clean_t5_model_lower/tokenizer_config.json',\n '/kaggle/working/clean_t5_model_lower/special_tokens_map.json',\n '/kaggle/working/clean_t5_model_lower/spiece.model',\n '/kaggle/working/clean_t5_model_lower/added_tokens.json')"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"!zip -r clean_indot5_model_non_lower.zip /kaggle/working/clean_t5_model_lower","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T13:58:56.745188Z","iopub.execute_input":"2024-12-16T13:58:56.745459Z","iopub.status.idle":"2024-12-16T13:59:53.107927Z","shell.execute_reply.started":"2024-12-16T13:58:56.745434Z","shell.execute_reply":"2024-12-16T13:59:53.106823Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"  adding: kaggle/working/clean_t5_model_lower/ (stored 0%)\n  adding: kaggle/working/clean_t5_model_lower/spiece.model (deflated 48%)\n  adding: kaggle/working/clean_t5_model_lower/generation_config.json (deflated 29%)\n  adding: kaggle/working/clean_t5_model_lower/tokenizer_config.json (deflated 94%)\n  adding: kaggle/working/clean_t5_model_lower/special_tokens_map.json (deflated 85%)\n  adding: kaggle/working/clean_t5_model_lower/model.safetensors (deflated 11%)\n  adding: kaggle/working/clean_t5_model_lower/config.json (deflated 63%)\n  adding: kaggle/working/clean_t5_model_lower/added_tokens.json (deflated 83%)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, num_epochs + 1), train_losses, label=\"Training Loss\", color='b', marker='o')\nplt.plot(range(1, num_epochs + 1), val_losses, label=\"Validation Loss\", color='r', marker='o')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training and Validation Loss\")\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T13:59:53.109316Z","iopub.execute_input":"2024-12-16T13:59:53.109590Z","iopub.status.idle":"2024-12-16T13:59:53.455663Z","shell.execute_reply.started":"2024-12-16T13:59:53.109565Z","shell.execute_reply":"2024-12-16T13:59:53.454764Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\nimport re\n\n\n# Generate predictions and evaluate\nmodel.eval()\npredictions, references, inputs = [], [], []\n\nfor batch in test_dataloader:\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n\n    # Generate summary\n    outputs = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_length=150,\n        min_length=30,\n        num_beams=4,\n        no_repeat_ngram_size=2\n    )\n\n    # Decode predictions and references, remove special tokens\n    predictions.extend([re.sub(r\"<extra_id_\\d+>\", \"\", tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)).strip()\n                        for output in outputs])\n    references.extend([tokenizer.decode(label, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n                       for label in batch['labels']])\n    inputs.extend([tokenizer.decode(input_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n                   for input_id in input_ids])\n\n# Create a DataFrame to store inputs, predictions, and references\ndf_results = pd.DataFrame({\n    'input': inputs,\n    'references': references,\n    'predictions': predictions\n})\ndf_results.to_csv('generated_summaries1_t5.csv', index=False)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:08:03.290756Z","iopub.execute_input":"2024-12-16T14:08:03.291129Z","iopub.status.idle":"2024-12-16T14:19:56.909707Z","shell.execute_reply.started":"2024-12-16T14:08:03.291098Z","shell.execute_reply":"2024-12-16T14:19:56.908987Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Evaluasi ROUGE\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\nrouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n\nfor ref, pred in zip(references, predictions):\n    score = scorer.score(ref, pred)\n    rouge_scores[\"rouge1\"].append(score[\"rouge1\"].fmeasure)\n    rouge_scores[\"rouge2\"].append(score[\"rouge2\"].fmeasure)\n    rouge_scores[\"rougeL\"].append(score[\"rougeL\"].fmeasure)\n\navg_rouge1 = sum(rouge_scores[\"rouge1\"]) / len(rouge_scores[\"rouge1\"])\navg_rouge2 = sum(rouge_scores[\"rouge2\"]) / len(rouge_scores[\"rouge2\"])\navg_rougeL = sum(rouge_scores[\"rougeL\"]) / len(rouge_scores[\"rougeL\"])\n\nprint(f\"Average ROUGE-1: {avg_rouge1}\")\nprint(f\"Average ROUGE-2: {avg_rouge2}\")\nprint(f\"Average ROUGE-L: {avg_rougeL}\")\n\n# Evaluasi BERTScore\nP, R, F1 = bert_score(predictions, references, lang=\"en\")\nprint(f\"Average BERTScore Precision: {P.mean().item()}\")\nprint(f\"Average BERTScore Recall: {R.mean().item()}\")\nprint(f\"Average BERTScore F1: {F1.mean().item()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:19:56.911205Z","iopub.execute_input":"2024-12-16T14:19:56.911473Z","iopub.status.idle":"2024-12-16T14:20:24.198939Z","shell.execute_reply.started":"2024-12-16T14:19:56.911447Z","shell.execute_reply":"2024-12-16T14:20:24.198038Z"}},"outputs":[{"name":"stdout","text":"Average ROUGE-1: 0.37961906283131513\nAverage ROUGE-2: 0.23962414542201166\nAverage ROUGE-L: 0.35002436343912313\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c82bbcb120d64b1b8f38291dfc39e072"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"829417fddd5144669ba0b39b8bb32de4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e045d2754f540a2824bf5f86eb29cf0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccff353be44c4760a14cececc5544a3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeba88a5f3ed4ed6b2da1db40dfd40e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80ac9c6d0f834bd7963bbc5e4c7297df"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Average BERTScore Precision: 0.8722087740898132\nAverage BERTScore Recall: 0.8694022297859192\nAverage BERTScore F1: 0.8706243634223938\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import pandas as pd\nimport re\n\n\n# Generate predictions and evaluate\nmodel.eval()\npredictions, references, inputs = [], [], []\n\nfor batch in test_dataloader:\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n\n    # Generate summary\n    outputs = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        min_length=30,\n        max_length=150,\n        num_beams=10,\n        no_repeat_ngram_size=2,\n        repetition_penalty=3.0,\n        length_penalty=1.2,\n        early_stopping=True,\n        use_cache=True,\n        do_sample=True,\n        temperature=1.0,\n        top_k=50,\n        top_p=0.9\n    )\n\n    # Decode predictions and references, remove special tokens\n    predictions.extend([re.sub(r\"<extra_id_\\d+>\", \"\", tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)).strip()\n                        for output in outputs])\n    references.extend([tokenizer.decode(label, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n                       for label in batch['labels']])\n    inputs.extend([tokenizer.decode(input_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n                   for input_id in input_ids])\n\n# Create a DataFrame to store inputs, predictions, and references\ndf_results = pd.DataFrame({\n    'input': inputs,\n    'references': references,\n    'predictions': predictions\n})\ndf_results.to_csv('generated_summaries2_t5.csv', index=False)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:20:24.200207Z","iopub.execute_input":"2024-12-16T14:20:24.200952Z","iopub.status.idle":"2024-12-16T14:35:37.374187Z","shell.execute_reply.started":"2024-12-16T14:20:24.200909Z","shell.execute_reply":"2024-12-16T14:35:37.372636Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Evaluasi ROUGE\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\nrouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n\nfor ref, pred in zip(references, predictions):\n    score = scorer.score(ref, pred)\n    rouge_scores[\"rouge1\"].append(score[\"rouge1\"].fmeasure)\n    rouge_scores[\"rouge2\"].append(score[\"rouge2\"].fmeasure)\n    rouge_scores[\"rougeL\"].append(score[\"rougeL\"].fmeasure)\n\navg_rouge1 = sum(rouge_scores[\"rouge1\"]) / len(rouge_scores[\"rouge1\"])\navg_rouge2 = sum(rouge_scores[\"rouge2\"]) / len(rouge_scores[\"rouge2\"])\navg_rougeL = sum(rouge_scores[\"rougeL\"]) / len(rouge_scores[\"rougeL\"])\n\nprint(f\"Average ROUGE-1: {avg_rouge1}\")\nprint(f\"Average ROUGE-2: {avg_rouge2}\")\nprint(f\"Average ROUGE-L: {avg_rougeL}\")\n\n# Evaluasi BERTScore\nP, R, F1 = bert_score(predictions, references, lang=\"en\")\nprint(f\"Average BERTScore Precision: {P.mean().item()}\")\nprint(f\"Average BERTScore Recall: {R.mean().item()}\")\nprint(f\"Average BERTScore F1: {F1.mean().item()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:35:37.376717Z","iopub.execute_input":"2024-12-16T14:35:37.377718Z","iopub.status.idle":"2024-12-16T14:35:54.998384Z","shell.execute_reply.started":"2024-12-16T14:35:37.377665Z","shell.execute_reply":"2024-12-16T14:35:54.997470Z"}},"outputs":[{"name":"stdout","text":"Average ROUGE-1: 0.34051121095836245\nAverage ROUGE-2: 0.2083903226272127\nAverage ROUGE-L: 0.315776273029487\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Average BERTScore Precision: 0.8706821799278259\nAverage BERTScore Recall: 0.8586987853050232\nAverage BERTScore F1: 0.8645060062408447\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"!pip install huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:35:54.999611Z","iopub.execute_input":"2024-12-16T14:35:55.000361Z","iopub.status.idle":"2024-12-16T14:36:03.091426Z","shell.execute_reply.started":"2024-12-16T14:35:55.000319Z","shell.execute_reply":"2024-12-16T14:36:03.090308Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.26.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.6.2)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from huggingface_hub import login\n\n# Masukkan token akses Anda (pastikan token memiliki izin \"write\")\nlogin(token=\"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\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:36:03.093232Z","iopub.execute_input":"2024-12-16T14:36:03.093636Z","iopub.status.idle":"2024-12-16T14:36:03.178664Z","shell.execute_reply.started":"2024-12-16T14:36:03.093596Z","shell.execute_reply":"2024-12-16T14:36:03.177890Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from huggingface_hub import create_repo\n\n# Membuat repository\nrepo_name = 'clean-model-t5-10epoch-lower'\ncreate_repo(repo_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:36:03.179845Z","iopub.execute_input":"2024-12-16T14:36:03.180402Z","iopub.status.idle":"2024-12-16T14:36:03.518472Z","shell.execute_reply.started":"2024-12-16T14:36:03.180364Z","shell.execute_reply":"2024-12-16T14:36:03.517705Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"RepoUrl('https://huggingface.co/intanutami/clean-model-t5-10epoch-lower', endpoint='https://huggingface.co', repo_type='model', repo_id='intanutami/clean-model-t5-10epoch-lower')"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"from huggingface_hub import upload_folder\n\n# Upload model ke repository yang sudah ada\nupload_folder(\n    repo_id='intanutami/clean-model-t5-10epoch-lower',\n    folder_path='/kaggle/working/clean_t5_model_lower',\n    commit_message=\"Upload my trained model\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T14:36:03.519653Z","iopub.execute_input":"2024-12-16T14:36:03.519970Z","iopub.status.idle":"2024-12-16T14:36:26.505426Z","shell.execute_reply.started":"2024-12-16T14:36:03.519934Z","shell.execute_reply":"2024-12-16T14:36:26.504589Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d1b842e72a34c9d8b19cad1686c2246"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f62f16cdfbd47cf9f9dd8dd59d919d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fd8ee8234934c3c90eb0498b6e910ce"}},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/intanutami/clean-model-t5-10epoch-lower/commit/936b86c5df3abec2da6f60b077216bf4f44653f9', commit_message='Upload my trained\\xa0model', commit_description='', oid='936b86c5df3abec2da6f60b077216bf4f44653f9', pr_url=None, repo_url=RepoUrl('https://huggingface.co/intanutami/clean-model-t5-10epoch-lower', endpoint='https://huggingface.co', repo_type='model', repo_id='intanutami/clean-model-t5-10epoch-lower'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":21}]}