{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 10094241,
          "sourceType": "datasetVersion",
          "datasetId": 6224823
        },
        {
          "sourceType": "datasetVersion",
          "sourceId": 10208527,
          "datasetId": 6309140
        }
      ],
      "dockerImageVersionId": 30805,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers rouge-score bert-score"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T08:34:50.087515Z",
          "iopub.execute_input": "2024-12-16T08:34:50.088166Z",
          "iopub.status.idle": "2024-12-16T08:35:02.049950Z",
          "shell.execute_reply.started": "2024-12-16T08:34:50.088133Z",
          "shell.execute_reply": "2024-12-16T08:35:02.048971Z"
        },
        "id": "7ldljE0VjYoe",
        "outputId": "a9c36234-6d41-434f-eff7-d92db8fd70fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.46.3)\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting bert-score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.26.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.4.0)\nRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.2.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert-score) (3.7.5)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2024.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.1.4)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (10.3.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.6.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=7711339785dc1aa705c9a8ec3267c61fae292fb49f5b42627841f19449b070ea\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score, bert-score\nSuccessfully installed bert-score-0.3.13 rouge-score-0.1.2\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T08:35:02.051989Z",
          "iopub.execute_input": "2024-12-16T08:35:02.052276Z",
          "iopub.status.idle": "2024-12-16T08:35:07.505413Z",
          "shell.execute_reply.started": "2024-12-16T08:35:02.052249Z",
          "shell.execute_reply": "2024-12-16T08:35:07.504774Z"
        },
        "id": "SAElNgR6jYoi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T08:35:07.506349Z",
          "iopub.execute_input": "2024-12-16T08:35:07.506753Z",
          "iopub.status.idle": "2024-12-16T08:35:18.947175Z",
          "shell.execute_reply.started": "2024-12-16T08:35:07.506726Z",
          "shell.execute_reply": "2024-12-16T08:35:18.946259Z"
        },
        "id": "sIB2PLB_jYoj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"cahya/t5-base-indonesian-summarization-cased\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T08:35:18.948276Z",
          "iopub.execute_input": "2024-12-16T08:35:18.948794Z",
          "iopub.status.idle": "2024-12-16T08:35:24.502634Z",
          "shell.execute_reply.started": "2024-12-16T08:35:18.948766Z",
          "shell.execute_reply": "2024-12-16T08:35:24.501881Z"
        },
        "id": "HRacdSCgjYoj",
        "outputId": "976098f1-b19d-4d7c-c8d1-ae30465e35d5",
        "colab": {
          "referenced_widgets": [
            "d0ea363cb2e5490f9324dd39ffaa38cf",
            "a9595aae56c5417fa90d392480f5b9d4",
            "11b96767b50849359440044f9e696d75",
            "759ca16bf7464e508744a1401051822d",
            "fb983442d5734c2dbda594265e2da340",
            "80bc192fb296469c9ef88f9e503ac1d1",
            "d79fba65046f4515a21f5ec7f1ccd499",
            "e4d2d69f7a214cb98fb0c4443a335f9d",
            "b4fc075aac1b4fe8b9e893ab97862444",
            "7029f26adf8d4a099fcc565b601cb542"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/2.07k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80bc192fb296469c9ef88f9e503ac1d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "spiece.model:   0%|          | 0.00/793k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d79fba65046f4515a21f5ec7f1ccd499"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4d2d69f7a214cb98fb0c4443a335f9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/657 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b4fc075aac1b4fe8b9e893ab97862444"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/892M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7029f26adf8d4a099fcc565b601cb542"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# File paths\n",
        "file_paths = {\n",
        "    'train': '/kaggle/input/dataset-lowercase-clean/train_data (1).csv',\n",
        "    'test': '/kaggle/input/dataset-lowercase-clean/test_data (1).csv',\n",
        "    'val': '/kaggle/input/dataset-lowercase-clean/val_data.csv'\n",
        "}\n",
        "\n",
        "# Fungsi untuk membaca dan melakukan tokenisasi pada data\n",
        "def process_data(file_path):\n",
        "    # Membaca CSV\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # Mengganti NaN dengan string kosong agar tokenizer bisa bekerja dengan data yang valid\n",
        "    data['content'] = data['content'].fillna('')\n",
        "    data['summary'] = data['summary'].fillna('')\n",
        "\n",
        "    # Tokenisasi kolom 'content' dan 'summary' dengan padding dan attention mask otomatis\n",
        "    input_encodings = tokenizer(\n",
        "        data['content'].tolist(),\n",
        "        max_length=512,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    target_encodings = tokenizer(\n",
        "        data['summary'].tolist(),\n",
        "        max_length=150,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Menambahkan kolom tokenisasi ke DataFrame\n",
        "    data['input_ids'] = input_encodings['input_ids'].tolist()\n",
        "    data['attention_mask'] = input_encodings['attention_mask'].tolist()\n",
        "    data['labels'] = target_encodings['input_ids'].tolist()\n",
        "\n",
        "    # Menampilkan beberapa baris pertama setelah tokenisasi\n",
        "    return data[['content', 'summary', 'input_ids', 'attention_mask', 'labels']]\n",
        "\n",
        "# Proses data untuk train, test, dan val\n",
        "train_data = process_data(file_paths['train'])\n",
        "test_data = process_data(file_paths['test'])\n",
        "val_data = process_data(file_paths['val'])\n",
        "\n",
        "# Tampilkan hasil tokenisasi sebagai DataFrame tanpa print\n",
        "train_data.head()  # Tampilkan 5 baris pertama dari data pelatihan"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T08:37:18.681540Z",
          "iopub.execute_input": "2024-12-16T08:37:18.681956Z",
          "iopub.status.idle": "2024-12-16T08:37:46.360873Z",
          "shell.execute_reply.started": "2024-12-16T08:37:18.681924Z",
          "shell.execute_reply": "2024-12-16T08:37:46.359951Z"
        },
        "id": "JFa7y_kE8hG_",
        "outputId": "11a62d31-e46c-4e26-c5d5-6820141a33cd"
      },
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                             content  \\\n0  ketua dewan pimpinan wilayah psi jakarta, elva...   \n1  pt pelayaran nasional indonesia (persero) atau...   \n2  shahnaz haque bercerita tentang kronologi meni...   \n3  bank indonesia (bi) mengajak investor china me...   \n4  ketua bidang pelayanan pusat kedokteran dan ke...   \n\n                                             summary  \\\n0  psi menargetkan ridwan kamil-suswono menang sa...   \n1  pt pelni (persero) sedang membuka lowongan ker...   \n2  adik marissa haque, shahnaz haque mengungkapka...   \n3  bi mengajak investor china memanfaatkan peluan...   \n4  pusdokkes polri masih kesulitan mengidentifika...   \n\n                                           input_ids  \\\n0  [1103, 2759, 2000, 180, 15, 9655, 15, 19704, 1...   \n1  [15, 6091, 8020, 2030, 15, 27478, 15, 4, 912, ...   \n2  [15, 14138, 13785, 5419, 3289, 16852, 406, 222...   \n3  [1536, 15, 27478, 15, 4, 901, 5, 6024, 8562, 1...   \n4  [1103, 399, 14041, 461, 30, 6734, 778, 36, 16,...   \n\n                                      attention_mask  \\\n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n\n                                              labels  \n0  [15, 9655, 24928, 15, 6476, 1517, 328, 363, 7,...  \n1  [15, 6091, 15, 3979, 794, 15, 4, 912, 25, 4671...  \n2  [2811, 8268, 8967, 13, 5419, 3289, 14, 15, 141...  \n3  [3543, 6024, 8562, 15, 10093, 13, 2787, 678, 5...  \n4  [15, 5998, 6734, 2637, 15, 5001, 572, 145, 797...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>content</th>\n      <th>summary</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ketua dewan pimpinan wilayah psi jakarta, elva...</td>\n      <td>psi menargetkan ridwan kamil-suswono menang sa...</td>\n      <td>[1103, 2759, 2000, 180, 15, 9655, 15, 19704, 1...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[15, 9655, 24928, 15, 6476, 1517, 328, 363, 7,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>pt pelayaran nasional indonesia (persero) atau...</td>\n      <td>pt pelni (persero) sedang membuka lowongan ker...</td>\n      <td>[15, 6091, 8020, 2030, 15, 27478, 15, 4, 912, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[15, 6091, 15, 3979, 794, 15, 4, 912, 25, 4671...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>shahnaz haque bercerita tentang kronologi meni...</td>\n      <td>adik marissa haque, shahnaz haque mengungkapka...</td>\n      <td>[15, 14138, 13785, 5419, 3289, 16852, 406, 222...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[2811, 8268, 8967, 13, 5419, 3289, 14, 15, 141...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bank indonesia (bi) mengajak investor china me...</td>\n      <td>bi mengajak investor china memanfaatkan peluan...</td>\n      <td>[1536, 15, 27478, 15, 4, 901, 5, 6024, 8562, 1...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[3543, 6024, 8562, 15, 10093, 13, 2787, 678, 5...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ketua bidang pelayanan pusat kedokteran dan ke...</td>\n      <td>pusdokkes polri masih kesulitan mengidentifika...</td>\n      <td>[1103, 399, 14041, 461, 30, 6734, 778, 36, 16,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n      <td>[15, 5998, 6734, 2637, 15, 5001, 572, 145, 797...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.to_csv('train_data_tokenized_indot5.csv', index=False)\n",
        "test_data.to_csv('test_data_tokenized_indot5.csv', index=False)\n",
        "val_data.to_csv('val_data_tokenized_indot5.csv', index=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T08:37:46.362416Z",
          "iopub.execute_input": "2024-12-16T08:37:46.362714Z",
          "iopub.status.idle": "2024-12-16T08:37:48.894412Z",
          "shell.execute_reply.started": "2024-12-16T08:37:46.362681Z",
          "shell.execute_reply": "2024-12-16T08:37:48.893664Z"
        },
        "id": "edD3Ox2J8hHA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Class (menggunakan token yang sudah ada)\n",
        "class SummaryDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.data = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': torch.tensor(self.data.iloc[idx]['input_ids']),\n",
        "            'attention_mask': torch.tensor(self.data.iloc[idx]['attention_mask']),\n",
        "            'labels': torch.tensor(self.data.iloc[idx]['labels'])\n",
        "        }\n",
        "\n",
        "# Membuat dataset untuk train, validation, dan test set\n",
        "train_dataset = SummaryDataset(train_data)\n",
        "val_dataset = SummaryDataset(val_data)\n",
        "test_dataset = SummaryDataset(test_data)\n",
        "\n",
        "# Membuat DataLoader untuk batch training\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T08:37:48.895553Z",
          "iopub.execute_input": "2024-12-16T08:37:48.895909Z",
          "iopub.status.idle": "2024-12-16T08:37:48.903036Z",
          "shell.execute_reply.started": "2024-12-16T08:37:48.895871Z",
          "shell.execute_reply": "2024-12-16T08:37:48.902095Z"
        },
        "id": "cD2PSryS8hHA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(\"cahya/t5-base-indonesian-summarization-cased\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T08:37:48.904202Z",
          "iopub.execute_input": "2024-12-16T08:37:48.904512Z",
          "iopub.status.idle": "2024-12-16T08:37:49.877842Z",
          "shell.execute_reply.started": "2024-12-16T08:37:48.904479Z",
          "shell.execute_reply": "2024-12-16T08:37:49.876985Z"
        },
        "id": "glDpciJLjYom",
        "outputId": "8f58b592-18de-4ff7-a558-a4b02480769a"
      },
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "T5ForConditionalGeneration(\n  (shared): Embedding(32128, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=768, out_features=3072, bias=False)\n              (wo): Linear(in_features=3072, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "# Menggunakan Adam optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
        "\n",
        "# Tentukan jumlah epoch untuk pelatihan\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T08:37:49.880730Z",
          "iopub.execute_input": "2024-12-16T08:37:49.881002Z",
          "iopub.status.idle": "2024-12-16T08:37:49.898596Z",
          "shell.execute_reply.started": "2024-12-16T08:37:49.880979Z",
          "shell.execute_reply": "2024-12-16T08:37:49.897682Z"
        },
        "id": "vOSg7gLdjYom",
        "outputId": "b410da07-fd93-4b51-b1d5-80bca628e5ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "train_losses_per_batch = []\n",
        "val_losses_per_batch = []\n",
        "train_losses = []  # Initialize train_losses here\n",
        "val_losses = []  # Initialize val_losses here\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    # Training phase\n",
        "    model.train()  # Pastikan model dalam mode pelatihan\n",
        "    total_train_loss = 0\n",
        "    for batch_idx, batch in enumerate(train_dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        train_losses_per_batch.append(loss.item())  # Menyimpan loss per batch\n",
        "\n",
        "        # Menampilkan loss per batch\n",
        "        if batch_idx % 10 == 0:  # Menampilkan setiap 10 batch\n",
        "            print(f\"Batch {batch_idx+1}/{len(train_dataloader)} - Train Loss: {loss.item()}\")\n",
        "\n",
        "    # Menampilkan loss rata-rata per epoch (Training Loss)\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    print(f\"Epoch {epoch+1} - Average Train Loss: {avg_train_loss}\")\n",
        "\n",
        "    # Clear GPU memory after training epoch\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()  # Pastikan model dalam mode evaluasi\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(val_dataloader):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_val_loss += loss.item()\n",
        "            val_losses_per_batch.append(loss.item())  # Menyimpan loss per batch\n",
        "\n",
        "            # Menampilkan loss per batch pada validation set\n",
        "            if batch_idx % 10 == 0:  # Menampilkan setiap 10 batch\n",
        "                print(f\"Batch {batch_idx+1}/{len(val_dataloader)} - Validation Loss: {loss.item()}\")\n",
        "\n",
        "    # Menampilkan loss rata-rata per epoch (Validation Loss)\n",
        "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "    val_losses.append(avg_val_loss)  # Append to val_losses list\n",
        "    print(f\"Epoch {epoch+1} - Average Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "    # Clear GPU memory after validation epoch\n",
        "    torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T08:37:49.899653Z",
          "iopub.execute_input": "2024-12-16T08:37:49.900049Z",
          "iopub.status.idle": "2024-12-16T13:02:27.267019Z",
          "shell.execute_reply.started": "2024-12-16T08:37:49.900004Z",
          "shell.execute_reply": "2024-12-16T13:02:27.266112Z"
        },
        "id": "WX_k-xFS8hHB",
        "outputId": "685f23cc-6a07-4eb8-9ffc-16ac0b5508bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/10\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Batch 1/2082 - Train Loss: 12.289971351623535\nBatch 11/2082 - Train Loss: 7.797990798950195\nBatch 21/2082 - Train Loss: 6.181206226348877\nBatch 31/2082 - Train Loss: 3.686300754547119\nBatch 41/2082 - Train Loss: 2.890085458755493\nBatch 51/2082 - Train Loss: 2.396743059158325\nBatch 61/2082 - Train Loss: 2.0215559005737305\nBatch 71/2082 - Train Loss: 1.3652716875076294\nBatch 81/2082 - Train Loss: 1.1880683898925781\nBatch 91/2082 - Train Loss: 0.9508769512176514\nBatch 101/2082 - Train Loss: 0.756712794303894\nBatch 111/2082 - Train Loss: 0.8428658843040466\nBatch 121/2082 - Train Loss: 0.8178764581680298\nBatch 131/2082 - Train Loss: 0.838151216506958\nBatch 141/2082 - Train Loss: 0.6605752110481262\nBatch 151/2082 - Train Loss: 0.7837121486663818\nBatch 161/2082 - Train Loss: 0.5633265376091003\nBatch 171/2082 - Train Loss: 0.8131298422813416\nBatch 181/2082 - Train Loss: 1.0137743949890137\nBatch 191/2082 - Train Loss: 0.475291907787323\nBatch 201/2082 - Train Loss: 0.678084135055542\nBatch 211/2082 - Train Loss: 0.5603020787239075\nBatch 221/2082 - Train Loss: 0.4268997609615326\nBatch 231/2082 - Train Loss: 0.512617290019989\nBatch 241/2082 - Train Loss: 0.45046523213386536\nBatch 251/2082 - Train Loss: 0.4735352098941803\nBatch 261/2082 - Train Loss: 0.6019735932350159\nBatch 271/2082 - Train Loss: 0.44034549593925476\nBatch 281/2082 - Train Loss: 0.34409403800964355\nBatch 291/2082 - Train Loss: 0.4114032983779907\nBatch 301/2082 - Train Loss: 0.4380492568016052\nBatch 311/2082 - Train Loss: 0.3705364465713501\nBatch 321/2082 - Train Loss: 0.4964435398578644\nBatch 331/2082 - Train Loss: 0.4003055989742279\nBatch 341/2082 - Train Loss: 0.4304307997226715\nBatch 351/2082 - Train Loss: 0.2379002869129181\nBatch 361/2082 - Train Loss: 0.49442505836486816\nBatch 371/2082 - Train Loss: 0.7266979217529297\nBatch 381/2082 - Train Loss: 0.3979828655719757\nBatch 391/2082 - Train Loss: 0.24066047370433807\nBatch 401/2082 - Train Loss: 0.4145511984825134\nBatch 411/2082 - Train Loss: 0.317079097032547\nBatch 421/2082 - Train Loss: 0.7446762323379517\nBatch 431/2082 - Train Loss: 0.42397060990333557\nBatch 441/2082 - Train Loss: 0.4155740737915039\nBatch 451/2082 - Train Loss: 0.41468244791030884\nBatch 461/2082 - Train Loss: 0.7161523103713989\nBatch 471/2082 - Train Loss: 0.33865177631378174\nBatch 481/2082 - Train Loss: 0.3034040629863739\nBatch 491/2082 - Train Loss: 0.3624570667743683\nBatch 501/2082 - Train Loss: 0.3240172564983368\nBatch 511/2082 - Train Loss: 0.6138221621513367\nBatch 521/2082 - Train Loss: 0.4992824196815491\nBatch 531/2082 - Train Loss: 0.14551237225532532\nBatch 541/2082 - Train Loss: 0.2526172399520874\nBatch 551/2082 - Train Loss: 0.4927191138267517\nBatch 561/2082 - Train Loss: 0.4656887352466583\nBatch 571/2082 - Train Loss: 0.3927682936191559\nBatch 581/2082 - Train Loss: 0.40255385637283325\nBatch 591/2082 - Train Loss: 0.3808565139770508\nBatch 601/2082 - Train Loss: 0.2975172996520996\nBatch 611/2082 - Train Loss: 0.470302015542984\nBatch 621/2082 - Train Loss: 0.2563430666923523\nBatch 631/2082 - Train Loss: 0.44557759165763855\nBatch 641/2082 - Train Loss: 0.16200567781925201\nBatch 651/2082 - Train Loss: 0.28999635577201843\nBatch 661/2082 - Train Loss: 0.316547155380249\nBatch 671/2082 - Train Loss: 0.4531318247318268\nBatch 681/2082 - Train Loss: 0.187839075922966\nBatch 691/2082 - Train Loss: 0.31898483633995056\nBatch 701/2082 - Train Loss: 0.38186773657798767\nBatch 711/2082 - Train Loss: 0.3252187967300415\nBatch 721/2082 - Train Loss: 0.25346115231513977\nBatch 731/2082 - Train Loss: 0.30946123600006104\nBatch 741/2082 - Train Loss: 0.4691087305545807\nBatch 751/2082 - Train Loss: 0.3614518940448761\nBatch 761/2082 - Train Loss: 0.3223186135292053\nBatch 771/2082 - Train Loss: 0.5484934449195862\nBatch 781/2082 - Train Loss: 0.42686569690704346\nBatch 791/2082 - Train Loss: 0.26198551058769226\nBatch 801/2082 - Train Loss: 0.27090030908584595\nBatch 811/2082 - Train Loss: 0.35197538137435913\nBatch 821/2082 - Train Loss: 0.3475134074687958\nBatch 831/2082 - Train Loss: 0.38312557339668274\nBatch 841/2082 - Train Loss: 0.3361767530441284\nBatch 851/2082 - Train Loss: 0.35076674818992615\nBatch 861/2082 - Train Loss: 0.32264742255210876\nBatch 871/2082 - Train Loss: 0.2808401882648468\nBatch 881/2082 - Train Loss: 0.38982656598091125\nBatch 891/2082 - Train Loss: 0.4507421851158142\nBatch 901/2082 - Train Loss: 0.14590635895729065\nBatch 911/2082 - Train Loss: 0.27045148611068726\nBatch 921/2082 - Train Loss: 0.5072057843208313\nBatch 931/2082 - Train Loss: 0.5023919939994812\nBatch 941/2082 - Train Loss: 0.3327089548110962\nBatch 951/2082 - Train Loss: 0.45526957511901855\nBatch 961/2082 - Train Loss: 0.575745165348053\nBatch 971/2082 - Train Loss: 0.24055129289627075\nBatch 981/2082 - Train Loss: 0.23788109421730042\nBatch 991/2082 - Train Loss: 0.33395010232925415\nBatch 1001/2082 - Train Loss: 0.15853476524353027\nBatch 1011/2082 - Train Loss: 0.3521762192249298\nBatch 1021/2082 - Train Loss: 1.1706387996673584\nBatch 1031/2082 - Train Loss: 0.4311447739601135\nBatch 1041/2082 - Train Loss: 0.2110964059829712\nBatch 1051/2082 - Train Loss: 0.1683056801557541\nBatch 1061/2082 - Train Loss: 0.29222771525382996\nBatch 1071/2082 - Train Loss: 0.3194267749786377\nBatch 1081/2082 - Train Loss: 0.42424532771110535\nBatch 1091/2082 - Train Loss: 0.16872961819171906\nBatch 1101/2082 - Train Loss: 0.1411503404378891\nBatch 1111/2082 - Train Loss: 0.20940764248371124\nBatch 1121/2082 - Train Loss: 0.47945258021354675\nBatch 1131/2082 - Train Loss: 0.22452254593372345\nBatch 1141/2082 - Train Loss: 0.41684696078300476\nBatch 1151/2082 - Train Loss: 0.1282419115304947\nBatch 1161/2082 - Train Loss: 0.2260548174381256\nBatch 1171/2082 - Train Loss: 0.2798561751842499\nBatch 1181/2082 - Train Loss: 0.40900498628616333\nBatch 1191/2082 - Train Loss: 0.2615726590156555\nBatch 1201/2082 - Train Loss: 0.5961005687713623\nBatch 1211/2082 - Train Loss: 0.22642062604427338\nBatch 1221/2082 - Train Loss: 0.3120206594467163\nBatch 1231/2082 - Train Loss: 0.4198232889175415\nBatch 1241/2082 - Train Loss: 0.3151204288005829\nBatch 1251/2082 - Train Loss: 0.14531585574150085\nBatch 1261/2082 - Train Loss: 0.21487513184547424\nBatch 1271/2082 - Train Loss: 0.3723934590816498\nBatch 1281/2082 - Train Loss: 0.2399500012397766\nBatch 1291/2082 - Train Loss: 0.4073614776134491\nBatch 1301/2082 - Train Loss: 0.41520678997039795\nBatch 1311/2082 - Train Loss: 0.26179563999176025\nBatch 1321/2082 - Train Loss: 0.2039012461900711\nBatch 1331/2082 - Train Loss: 0.3962153494358063\nBatch 1341/2082 - Train Loss: 0.26465919613838196\nBatch 1351/2082 - Train Loss: 0.42540186643600464\nBatch 1361/2082 - Train Loss: 0.2902683913707733\nBatch 1371/2082 - Train Loss: 0.35611867904663086\nBatch 1381/2082 - Train Loss: 0.32493752241134644\nBatch 1391/2082 - Train Loss: 0.30739298462867737\nBatch 1401/2082 - Train Loss: 0.19343596696853638\nBatch 1411/2082 - Train Loss: 0.2687542736530304\nBatch 1421/2082 - Train Loss: 0.27613821625709534\nBatch 1431/2082 - Train Loss: 0.2814406454563141\nBatch 1441/2082 - Train Loss: 0.19426751136779785\nBatch 1451/2082 - Train Loss: 0.3960653245449066\nBatch 1461/2082 - Train Loss: 0.3510259985923767\nBatch 1471/2082 - Train Loss: 0.3489571511745453\nBatch 1481/2082 - Train Loss: 0.30024996399879456\nBatch 1491/2082 - Train Loss: 0.2825753092765808\nBatch 1501/2082 - Train Loss: 0.5136685371398926\nBatch 1511/2082 - Train Loss: 0.4477074146270752\nBatch 1521/2082 - Train Loss: 0.230329692363739\nBatch 1531/2082 - Train Loss: 0.30377310514450073\nBatch 1541/2082 - Train Loss: 0.41685202717781067\nBatch 1551/2082 - Train Loss: 0.2433430254459381\nBatch 1561/2082 - Train Loss: 0.24333645403385162\nBatch 1571/2082 - Train Loss: 0.35167166590690613\nBatch 1581/2082 - Train Loss: 0.14072519540786743\nBatch 1591/2082 - Train Loss: 0.34941357374191284\nBatch 1601/2082 - Train Loss: 0.27303797006607056\nBatch 1611/2082 - Train Loss: 0.32715097069740295\nBatch 1621/2082 - Train Loss: 0.4065302014350891\nBatch 1631/2082 - Train Loss: 0.3130054473876953\nBatch 1641/2082 - Train Loss: 0.4388745129108429\nBatch 1651/2082 - Train Loss: 0.31680774688720703\nBatch 1661/2082 - Train Loss: 0.1789344698190689\nBatch 1671/2082 - Train Loss: 0.37310242652893066\nBatch 1681/2082 - Train Loss: 0.44930189847946167\nBatch 1691/2082 - Train Loss: 0.37993699312210083\nBatch 1701/2082 - Train Loss: 0.3655693531036377\nBatch 1711/2082 - Train Loss: 0.2624976634979248\nBatch 1721/2082 - Train Loss: 0.3063738942146301\nBatch 1731/2082 - Train Loss: 0.20792227983474731\nBatch 1741/2082 - Train Loss: 0.20080235600471497\nBatch 1751/2082 - Train Loss: 0.29948359727859497\nBatch 1761/2082 - Train Loss: 0.2765962779521942\nBatch 1771/2082 - Train Loss: 0.3151560425758362\nBatch 1781/2082 - Train Loss: 0.3020589053630829\nBatch 1791/2082 - Train Loss: 0.21422070264816284\nBatch 1801/2082 - Train Loss: 0.5359252691268921\nBatch 1811/2082 - Train Loss: 0.1642594188451767\nBatch 1821/2082 - Train Loss: 0.24272702634334564\nBatch 1831/2082 - Train Loss: 0.3295121490955353\nBatch 1841/2082 - Train Loss: 0.3161245286464691\nBatch 1851/2082 - Train Loss: 0.28672003746032715\nBatch 1861/2082 - Train Loss: 0.36319270730018616\nBatch 1871/2082 - Train Loss: 0.21365363895893097\nBatch 1881/2082 - Train Loss: 0.24145245552062988\nBatch 1891/2082 - Train Loss: 0.29147157073020935\nBatch 1901/2082 - Train Loss: 0.10898053646087646\nBatch 1911/2082 - Train Loss: 0.24089792370796204\nBatch 1921/2082 - Train Loss: 0.3610542416572571\nBatch 1931/2082 - Train Loss: 0.21838867664337158\nBatch 1941/2082 - Train Loss: 0.43629902601242065\nBatch 1951/2082 - Train Loss: 0.2820732891559601\nBatch 1961/2082 - Train Loss: 0.3291696310043335\nBatch 1971/2082 - Train Loss: 0.37183210253715515\nBatch 1981/2082 - Train Loss: 0.28887513279914856\nBatch 1991/2082 - Train Loss: 0.2852233052253723\nBatch 2001/2082 - Train Loss: 0.466654509305954\nBatch 2011/2082 - Train Loss: 0.2057112455368042\nBatch 2021/2082 - Train Loss: 0.19665656983852386\nBatch 2031/2082 - Train Loss: 0.27480190992355347\nBatch 2041/2082 - Train Loss: 0.18272146582603455\nBatch 2051/2082 - Train Loss: 0.1995210349559784\nBatch 2061/2082 - Train Loss: 0.23839184641838074\nBatch 2071/2082 - Train Loss: 0.12325139343738556\nBatch 2081/2082 - Train Loss: 0.2467872053384781\nEpoch 1 - Average Train Loss: 0.5243458485840949\nBatch 1/261 - Validation Loss: 0.10215852409601212\nBatch 11/261 - Validation Loss: 0.383421391248703\nBatch 21/261 - Validation Loss: 0.184590682387352\nBatch 31/261 - Validation Loss: 0.2990325689315796\nBatch 41/261 - Validation Loss: 0.3083021938800812\nBatch 51/261 - Validation Loss: 0.17219838500022888\nBatch 61/261 - Validation Loss: 0.19033727049827576\nBatch 71/261 - Validation Loss: 0.18215183913707733\nBatch 81/261 - Validation Loss: 0.33622145652770996\nBatch 91/261 - Validation Loss: 0.3593539893627167\nBatch 101/261 - Validation Loss: 0.1630130559206009\nBatch 111/261 - Validation Loss: 0.3639222979545593\nBatch 121/261 - Validation Loss: 0.21748028695583344\nBatch 131/261 - Validation Loss: 0.5170674324035645\nBatch 141/261 - Validation Loss: 0.3950423300266266\nBatch 151/261 - Validation Loss: 0.3811992108821869\nBatch 161/261 - Validation Loss: 0.21208225190639496\nBatch 171/261 - Validation Loss: 0.42385974526405334\nBatch 181/261 - Validation Loss: 0.3357117474079132\nBatch 191/261 - Validation Loss: 0.30226367712020874\nBatch 201/261 - Validation Loss: 0.6000118851661682\nBatch 211/261 - Validation Loss: 0.20325376093387604\nBatch 221/261 - Validation Loss: 0.12351023405790329\nBatch 231/261 - Validation Loss: 0.16087952256202698\nBatch 241/261 - Validation Loss: 0.2025577872991562\nBatch 251/261 - Validation Loss: 0.43345245718955994\nBatch 261/261 - Validation Loss: 0.08271098881959915\nEpoch 1 - Average Validation Loss: 0.26938062092696113\nEpoch 2/10\nBatch 1/2082 - Train Loss: 0.1851607710123062\nBatch 11/2082 - Train Loss: 0.3471841812133789\nBatch 21/2082 - Train Loss: 0.18717741966247559\nBatch 31/2082 - Train Loss: 0.3098929822444916\nBatch 41/2082 - Train Loss: 0.44094008207321167\nBatch 51/2082 - Train Loss: 0.20690572261810303\nBatch 61/2082 - Train Loss: 0.20536461472511292\nBatch 71/2082 - Train Loss: 0.34521228075027466\nBatch 81/2082 - Train Loss: 0.18947093188762665\nBatch 91/2082 - Train Loss: 0.1654689460992813\nBatch 101/2082 - Train Loss: 0.21611040830612183\nBatch 111/2082 - Train Loss: 0.24294644594192505\nBatch 121/2082 - Train Loss: 0.2116841822862625\nBatch 131/2082 - Train Loss: 0.2414989173412323\nBatch 141/2082 - Train Loss: 0.2154788225889206\nBatch 151/2082 - Train Loss: 0.29299575090408325\nBatch 161/2082 - Train Loss: 0.2689203917980194\nBatch 171/2082 - Train Loss: 0.36017388105392456\nBatch 181/2082 - Train Loss: 0.3358975946903229\nBatch 191/2082 - Train Loss: 0.22063064575195312\nBatch 201/2082 - Train Loss: 0.4794901907444\nBatch 211/2082 - Train Loss: 0.2671459913253784\nBatch 221/2082 - Train Loss: 0.25728216767311096\nBatch 231/2082 - Train Loss: 0.21862788498401642\nBatch 241/2082 - Train Loss: 0.47511595487594604\nBatch 251/2082 - Train Loss: 0.24170246720314026\nBatch 261/2082 - Train Loss: 0.30526086688041687\nBatch 271/2082 - Train Loss: 0.4699175953865051\nBatch 281/2082 - Train Loss: 0.2900221347808838\nBatch 291/2082 - Train Loss: 0.3501274585723877\nBatch 301/2082 - Train Loss: 0.21025662124156952\nBatch 311/2082 - Train Loss: 0.24327965080738068\nBatch 321/2082 - Train Loss: 0.19512814283370972\nBatch 331/2082 - Train Loss: 0.2702464461326599\nBatch 341/2082 - Train Loss: 0.19524790346622467\nBatch 351/2082 - Train Loss: 0.18228396773338318\nBatch 361/2082 - Train Loss: 0.33269262313842773\nBatch 371/2082 - Train Loss: 0.15716060996055603\nBatch 381/2082 - Train Loss: 0.37494441866874695\nBatch 391/2082 - Train Loss: 0.24601662158966064\nBatch 401/2082 - Train Loss: 0.2123880833387375\nBatch 411/2082 - Train Loss: 0.12647752463817596\nBatch 421/2082 - Train Loss: 0.14985819160938263\nBatch 431/2082 - Train Loss: 0.21520675718784332\nBatch 441/2082 - Train Loss: 0.33257144689559937\nBatch 451/2082 - Train Loss: 0.2852417230606079\nBatch 461/2082 - Train Loss: 0.1054367795586586\nBatch 471/2082 - Train Loss: 0.26587975025177\nBatch 481/2082 - Train Loss: 0.49943095445632935\nBatch 491/2082 - Train Loss: 0.33359456062316895\nBatch 501/2082 - Train Loss: 0.17804525792598724\nBatch 511/2082 - Train Loss: 0.3242177367210388\nBatch 521/2082 - Train Loss: 0.4276273548603058\nBatch 531/2082 - Train Loss: 0.16726428270339966\nBatch 541/2082 - Train Loss: 0.4105581045150757\nBatch 551/2082 - Train Loss: 0.27798426151275635\nBatch 561/2082 - Train Loss: 0.2365061193704605\nBatch 571/2082 - Train Loss: 0.24597719311714172\nBatch 581/2082 - Train Loss: 0.21803270280361176\nBatch 591/2082 - Train Loss: 0.21045900881290436\nBatch 601/2082 - Train Loss: 0.3383834660053253\nBatch 611/2082 - Train Loss: 0.31515002250671387\nBatch 621/2082 - Train Loss: 0.3962658941745758\nBatch 631/2082 - Train Loss: 0.4478690028190613\nBatch 641/2082 - Train Loss: 0.22433796525001526\nBatch 651/2082 - Train Loss: 0.17609114944934845\nBatch 661/2082 - Train Loss: 0.19639721512794495\nBatch 671/2082 - Train Loss: 0.2590976059436798\nBatch 681/2082 - Train Loss: 0.34854620695114136\nBatch 691/2082 - Train Loss: 0.2691088318824768\nBatch 701/2082 - Train Loss: 0.16898201406002045\nBatch 711/2082 - Train Loss: 0.2663232088088989\nBatch 721/2082 - Train Loss: 0.2365761250257492\nBatch 731/2082 - Train Loss: 0.25778234004974365\nBatch 741/2082 - Train Loss: 0.3464047610759735\nBatch 751/2082 - Train Loss: 0.15747588872909546\nBatch 761/2082 - Train Loss: 0.37764862179756165\nBatch 771/2082 - Train Loss: 0.26413530111312866\nBatch 781/2082 - Train Loss: 0.29996028542518616\nBatch 791/2082 - Train Loss: 0.3623087704181671\nBatch 801/2082 - Train Loss: 0.45036208629608154\nBatch 811/2082 - Train Loss: 0.3066907823085785\nBatch 821/2082 - Train Loss: 0.33111387491226196\nBatch 831/2082 - Train Loss: 0.2813393473625183\nBatch 841/2082 - Train Loss: 0.4799138009548187\nBatch 851/2082 - Train Loss: 0.27247002720832825\nBatch 861/2082 - Train Loss: 0.2584320902824402\nBatch 871/2082 - Train Loss: 0.4120975732803345\nBatch 881/2082 - Train Loss: 0.325332373380661\nBatch 891/2082 - Train Loss: 0.3710811734199524\nBatch 901/2082 - Train Loss: 0.2666788697242737\nBatch 911/2082 - Train Loss: 0.18116381764411926\nBatch 921/2082 - Train Loss: 0.36138641834259033\nBatch 931/2082 - Train Loss: 0.3472704291343689\nBatch 941/2082 - Train Loss: 0.3274095952510834\nBatch 951/2082 - Train Loss: 0.31260475516319275\nBatch 961/2082 - Train Loss: 0.14897359907627106\nBatch 971/2082 - Train Loss: 0.17576557397842407\nBatch 981/2082 - Train Loss: 0.2384980320930481\nBatch 991/2082 - Train Loss: 0.45736220479011536\nBatch 1001/2082 - Train Loss: 0.29170745611190796\nBatch 1011/2082 - Train Loss: 0.17729619145393372\nBatch 1021/2082 - Train Loss: 0.38826894760131836\nBatch 1031/2082 - Train Loss: 0.3395494818687439\nBatch 1041/2082 - Train Loss: 0.4065815210342407\nBatch 1051/2082 - Train Loss: 0.18108722567558289\nBatch 1061/2082 - Train Loss: 0.2479773759841919\nBatch 1071/2082 - Train Loss: 0.22247843444347382\nBatch 1081/2082 - Train Loss: 0.24590115249156952\nBatch 1091/2082 - Train Loss: 0.1315196454524994\nBatch 1101/2082 - Train Loss: 0.2868388891220093\nBatch 1111/2082 - Train Loss: 0.1835402250289917\nBatch 1121/2082 - Train Loss: 0.11431017518043518\nBatch 1131/2082 - Train Loss: 0.22625383734703064\nBatch 1141/2082 - Train Loss: 0.4592355787754059\nBatch 1151/2082 - Train Loss: 0.1525748074054718\nBatch 1161/2082 - Train Loss: 0.3660311996936798\nBatch 1171/2082 - Train Loss: 0.3332512676715851\nBatch 1181/2082 - Train Loss: 0.18928326666355133\nBatch 1191/2082 - Train Loss: 0.16560252010822296\nBatch 1201/2082 - Train Loss: 0.15749311447143555\nBatch 1211/2082 - Train Loss: 0.24173840880393982\nBatch 1221/2082 - Train Loss: 0.4147219955921173\nBatch 1231/2082 - Train Loss: 0.3862490952014923\nBatch 1241/2082 - Train Loss: 0.3050166666507721\nBatch 1251/2082 - Train Loss: 0.2610808312892914\nBatch 1261/2082 - Train Loss: 0.0900113433599472\nBatch 1271/2082 - Train Loss: 0.3545726239681244\nBatch 1281/2082 - Train Loss: 0.2442081719636917\nBatch 1291/2082 - Train Loss: 0.13519026339054108\nBatch 1301/2082 - Train Loss: 0.2671738862991333\nBatch 1311/2082 - Train Loss: 0.4874178171157837\nBatch 1321/2082 - Train Loss: 0.4556986391544342\nBatch 1331/2082 - Train Loss: 0.26086559891700745\nBatch 1341/2082 - Train Loss: 0.34238162636756897\nBatch 1351/2082 - Train Loss: 0.3198118507862091\nBatch 1361/2082 - Train Loss: 0.3112909197807312\nBatch 1371/2082 - Train Loss: 0.2445421665906906\nBatch 1381/2082 - Train Loss: 0.18557599186897278\nBatch 1391/2082 - Train Loss: 0.10788785666227341\nBatch 1401/2082 - Train Loss: 0.30393317341804504\nBatch 1411/2082 - Train Loss: 0.22692164778709412\nBatch 1421/2082 - Train Loss: 0.28692132234573364\nBatch 1431/2082 - Train Loss: 0.26325953006744385\nBatch 1441/2082 - Train Loss: 0.4301099181175232\nBatch 1451/2082 - Train Loss: 0.3067440390586853\nBatch 1461/2082 - Train Loss: 0.196335107088089\nBatch 1471/2082 - Train Loss: 0.1990249752998352\nBatch 1481/2082 - Train Loss: 0.08295591175556183\nBatch 1491/2082 - Train Loss: 0.38588011264801025\nBatch 1501/2082 - Train Loss: 0.1703370213508606\nBatch 1511/2082 - Train Loss: 0.14716804027557373\nBatch 1521/2082 - Train Loss: 0.15235644578933716\nBatch 1531/2082 - Train Loss: 0.34284353256225586\nBatch 1541/2082 - Train Loss: 0.38658180832862854\nBatch 1551/2082 - Train Loss: 0.20768405497074127\nBatch 1561/2082 - Train Loss: 0.30631130933761597\nBatch 1571/2082 - Train Loss: 0.22086988389492035\nBatch 1581/2082 - Train Loss: 0.3022100627422333\nBatch 1591/2082 - Train Loss: 0.3687897026538849\nBatch 1601/2082 - Train Loss: 0.3865598142147064\nBatch 1611/2082 - Train Loss: 0.22117596864700317\nBatch 1621/2082 - Train Loss: 0.4360488951206207\nBatch 1631/2082 - Train Loss: 0.31104424595832825\nBatch 1641/2082 - Train Loss: 0.2893116772174835\nBatch 1651/2082 - Train Loss: 0.24921132624149323\nBatch 1661/2082 - Train Loss: 0.3941670358181\nBatch 1671/2082 - Train Loss: 0.3569643497467041\nBatch 1681/2082 - Train Loss: 0.1441357433795929\nBatch 1691/2082 - Train Loss: 0.2918839156627655\nBatch 1701/2082 - Train Loss: 0.2722977101802826\nBatch 1711/2082 - Train Loss: 0.3039664328098297\nBatch 1721/2082 - Train Loss: 0.3534095287322998\nBatch 1731/2082 - Train Loss: 0.11920065432786942\nBatch 1741/2082 - Train Loss: 0.31889626383781433\nBatch 1751/2082 - Train Loss: 0.21205653250217438\nBatch 1761/2082 - Train Loss: 0.3433898389339447\nBatch 1771/2082 - Train Loss: 0.3019971251487732\nBatch 1781/2082 - Train Loss: 0.34477779269218445\nBatch 1791/2082 - Train Loss: 0.22776035964488983\nBatch 1801/2082 - Train Loss: 0.16527745127677917\nBatch 1811/2082 - Train Loss: 0.2085888832807541\nBatch 1821/2082 - Train Loss: 0.26441678404808044\nBatch 1831/2082 - Train Loss: 0.2385568767786026\nBatch 1841/2082 - Train Loss: 0.16951100528240204\nBatch 1851/2082 - Train Loss: 0.2712548077106476\nBatch 1861/2082 - Train Loss: 0.35808929800987244\nBatch 1871/2082 - Train Loss: 0.3476528823375702\nBatch 1881/2082 - Train Loss: 0.09829355776309967\nBatch 1891/2082 - Train Loss: 0.31733188033103943\nBatch 1901/2082 - Train Loss: 0.11830224096775055\nBatch 1911/2082 - Train Loss: 0.24470682442188263\nBatch 1921/2082 - Train Loss: 0.1795273721218109\nBatch 1931/2082 - Train Loss: 0.29844775795936584\nBatch 1941/2082 - Train Loss: 0.23914550244808197\nBatch 1951/2082 - Train Loss: 0.28024885058403015\nBatch 1961/2082 - Train Loss: 0.3276258409023285\nBatch 1971/2082 - Train Loss: 0.23753324151039124\nBatch 1981/2082 - Train Loss: 0.30142688751220703\nBatch 1991/2082 - Train Loss: 0.2799893319606781\nBatch 2001/2082 - Train Loss: 0.19363632798194885\nBatch 2011/2082 - Train Loss: 0.32363080978393555\nBatch 2021/2082 - Train Loss: 0.37002649903297424\nBatch 2031/2082 - Train Loss: 0.23243838548660278\nBatch 2041/2082 - Train Loss: 0.2350323498249054\nBatch 2051/2082 - Train Loss: 0.30189675092697144\nBatch 2061/2082 - Train Loss: 0.33501461148262024\nBatch 2071/2082 - Train Loss: 0.3793121874332428\nBatch 2081/2082 - Train Loss: 0.2942133843898773\nEpoch 2 - Average Train Loss: 0.2808260899788622\nBatch 1/261 - Validation Loss: 0.09758830815553665\nBatch 11/261 - Validation Loss: 0.34762170910835266\nBatch 21/261 - Validation Loss: 0.167835533618927\nBatch 31/261 - Validation Loss: 0.2874995768070221\nBatch 41/261 - Validation Loss: 0.27954810857772827\nBatch 51/261 - Validation Loss: 0.16338296234607697\nBatch 61/261 - Validation Loss: 0.18058066070079803\nBatch 71/261 - Validation Loss: 0.1666106879711151\nBatch 81/261 - Validation Loss: 0.30988913774490356\nBatch 91/261 - Validation Loss: 0.3474139869213104\nBatch 101/261 - Validation Loss: 0.15744268894195557\nBatch 111/261 - Validation Loss: 0.33811965584754944\nBatch 121/261 - Validation Loss: 0.19482740759849548\nBatch 131/261 - Validation Loss: 0.5062735676765442\nBatch 141/261 - Validation Loss: 0.3713485896587372\nBatch 151/261 - Validation Loss: 0.35955318808555603\nBatch 161/261 - Validation Loss: 0.19717593491077423\nBatch 171/261 - Validation Loss: 0.40655800700187683\nBatch 181/261 - Validation Loss: 0.31275808811187744\nBatch 191/261 - Validation Loss: 0.29621678590774536\nBatch 201/261 - Validation Loss: 0.5658741593360901\nBatch 211/261 - Validation Loss: 0.19267159700393677\nBatch 221/261 - Validation Loss: 0.10834846645593643\nBatch 231/261 - Validation Loss: 0.15757670998573303\nBatch 241/261 - Validation Loss: 0.19375993311405182\nBatch 251/261 - Validation Loss: 0.4119945168495178\nBatch 261/261 - Validation Loss: 0.06510880589485168\nEpoch 2 - Average Validation Loss: 0.25404751533464\nEpoch 3/10\nBatch 1/2082 - Train Loss: 0.15864279866218567\nBatch 11/2082 - Train Loss: 0.3076990246772766\nBatch 21/2082 - Train Loss: 0.3106383681297302\nBatch 31/2082 - Train Loss: 0.1247747391462326\nBatch 41/2082 - Train Loss: 0.25527164340019226\nBatch 51/2082 - Train Loss: 0.35503196716308594\nBatch 61/2082 - Train Loss: 0.13247710466384888\nBatch 71/2082 - Train Loss: 0.4663339853286743\nBatch 81/2082 - Train Loss: 0.238213911652565\nBatch 91/2082 - Train Loss: 0.21072252094745636\nBatch 101/2082 - Train Loss: 0.4333164393901825\nBatch 111/2082 - Train Loss: 0.3270416259765625\nBatch 121/2082 - Train Loss: 0.3128597140312195\nBatch 131/2082 - Train Loss: 0.2127409279346466\nBatch 141/2082 - Train Loss: 0.2897910177707672\nBatch 151/2082 - Train Loss: 0.214979350566864\nBatch 161/2082 - Train Loss: 0.43894901871681213\nBatch 171/2082 - Train Loss: 0.33042991161346436\nBatch 181/2082 - Train Loss: 0.48531869053840637\nBatch 191/2082 - Train Loss: 0.12854573130607605\nBatch 201/2082 - Train Loss: 0.3205786645412445\nBatch 211/2082 - Train Loss: 0.25491681694984436\nBatch 221/2082 - Train Loss: 0.24490152299404144\nBatch 231/2082 - Train Loss: 0.2935073971748352\nBatch 241/2082 - Train Loss: 0.3960497975349426\nBatch 251/2082 - Train Loss: 0.3010188937187195\nBatch 261/2082 - Train Loss: 0.4526338279247284\nBatch 271/2082 - Train Loss: 0.4085525572299957\nBatch 281/2082 - Train Loss: 0.1622786521911621\nBatch 291/2082 - Train Loss: 0.2962154150009155\nBatch 301/2082 - Train Loss: 0.23311974108219147\nBatch 311/2082 - Train Loss: 0.2902081310749054\nBatch 321/2082 - Train Loss: 0.33035892248153687\nBatch 331/2082 - Train Loss: 0.26649847626686096\nBatch 341/2082 - Train Loss: 0.20915116369724274\nBatch 351/2082 - Train Loss: 0.24601849913597107\nBatch 361/2082 - Train Loss: 0.3743392825126648\nBatch 371/2082 - Train Loss: 0.354224294424057\nBatch 381/2082 - Train Loss: 0.15346145629882812\nBatch 391/2082 - Train Loss: 0.3180460035800934\nBatch 401/2082 - Train Loss: 0.20214927196502686\nBatch 411/2082 - Train Loss: 0.32802748680114746\nBatch 421/2082 - Train Loss: 0.32438504695892334\nBatch 431/2082 - Train Loss: 0.15024331212043762\nBatch 441/2082 - Train Loss: 0.16391637921333313\nBatch 451/2082 - Train Loss: 0.2767478823661804\nBatch 461/2082 - Train Loss: 0.33805251121520996\nBatch 471/2082 - Train Loss: 0.34487447142601013\nBatch 481/2082 - Train Loss: 0.1710556596517563\nBatch 491/2082 - Train Loss: 0.2924833297729492\nBatch 501/2082 - Train Loss: 0.22554358839988708\nBatch 511/2082 - Train Loss: 0.44527167081832886\nBatch 521/2082 - Train Loss: 0.24139584600925446\nBatch 531/2082 - Train Loss: 0.43461912870407104\nBatch 541/2082 - Train Loss: 0.13888855278491974\nBatch 551/2082 - Train Loss: 0.17747336626052856\nBatch 561/2082 - Train Loss: 0.12544216215610504\nBatch 571/2082 - Train Loss: 0.3142922818660736\nBatch 581/2082 - Train Loss: 0.19987724721431732\nBatch 591/2082 - Train Loss: 0.329317182302475\nBatch 601/2082 - Train Loss: 0.16481676697731018\nBatch 611/2082 - Train Loss: 0.1608651578426361\nBatch 621/2082 - Train Loss: 0.22122932970523834\nBatch 631/2082 - Train Loss: 0.34684911370277405\nBatch 641/2082 - Train Loss: 0.21666260063648224\nBatch 651/2082 - Train Loss: 0.4938455820083618\nBatch 661/2082 - Train Loss: 0.2883854806423187\nBatch 671/2082 - Train Loss: 0.3077164590358734\nBatch 681/2082 - Train Loss: 0.2536540627479553\nBatch 691/2082 - Train Loss: 0.39418983459472656\nBatch 701/2082 - Train Loss: 0.18008460104465485\nBatch 711/2082 - Train Loss: 0.17554692924022675\nBatch 721/2082 - Train Loss: 0.2315564751625061\nBatch 731/2082 - Train Loss: 0.18052536249160767\nBatch 741/2082 - Train Loss: 0.2449120134115219\nBatch 751/2082 - Train Loss: 0.17249783873558044\nBatch 761/2082 - Train Loss: 0.4554632008075714\nBatch 771/2082 - Train Loss: 0.090859554708004\nBatch 781/2082 - Train Loss: 0.24382083117961884\nBatch 791/2082 - Train Loss: 0.17640942335128784\nBatch 801/2082 - Train Loss: 0.28259894251823425\nBatch 811/2082 - Train Loss: 0.11832737177610397\nBatch 821/2082 - Train Loss: 0.2529826760292053\nBatch 831/2082 - Train Loss: 0.24483738839626312\nBatch 841/2082 - Train Loss: 0.23932737112045288\nBatch 851/2082 - Train Loss: 0.37232762575149536\nBatch 861/2082 - Train Loss: 0.27230727672576904\nBatch 871/2082 - Train Loss: 0.2104000449180603\nBatch 881/2082 - Train Loss: 0.25389766693115234\nBatch 891/2082 - Train Loss: 0.13591304421424866\nBatch 901/2082 - Train Loss: 0.24811626970767975\nBatch 911/2082 - Train Loss: 0.3433302938938141\nBatch 921/2082 - Train Loss: 0.365136981010437\nBatch 931/2082 - Train Loss: 0.3714297115802765\nBatch 941/2082 - Train Loss: 0.24939073622226715\nBatch 951/2082 - Train Loss: 0.2992667257785797\nBatch 961/2082 - Train Loss: 0.255588173866272\nBatch 971/2082 - Train Loss: 0.36376264691352844\nBatch 981/2082 - Train Loss: 0.2909087538719177\nBatch 991/2082 - Train Loss: 0.2319623827934265\nBatch 1001/2082 - Train Loss: 0.1971045881509781\nBatch 1011/2082 - Train Loss: 0.2406846582889557\nBatch 1021/2082 - Train Loss: 0.2815948724746704\nBatch 1031/2082 - Train Loss: 0.21586479246616364\nBatch 1041/2082 - Train Loss: 0.2931367754936218\nBatch 1051/2082 - Train Loss: 0.289859414100647\nBatch 1061/2082 - Train Loss: 0.22837036848068237\nBatch 1071/2082 - Train Loss: 0.290444552898407\nBatch 1081/2082 - Train Loss: 0.1448844075202942\nBatch 1091/2082 - Train Loss: 0.16353707015514374\nBatch 1101/2082 - Train Loss: 0.31080421805381775\nBatch 1111/2082 - Train Loss: 0.23066368699073792\nBatch 1121/2082 - Train Loss: 0.2561321258544922\nBatch 1131/2082 - Train Loss: 0.29327574372291565\nBatch 1141/2082 - Train Loss: 0.29668834805488586\nBatch 1151/2082 - Train Loss: 0.1649986207485199\nBatch 1161/2082 - Train Loss: 0.19606828689575195\nBatch 1171/2082 - Train Loss: 0.441561222076416\nBatch 1181/2082 - Train Loss: 0.2541361451148987\nBatch 1191/2082 - Train Loss: 0.2944871485233307\nBatch 1201/2082 - Train Loss: 0.45815515518188477\nBatch 1211/2082 - Train Loss: 0.26661545038223267\nBatch 1221/2082 - Train Loss: 0.1833847165107727\nBatch 1231/2082 - Train Loss: 0.11324325203895569\nBatch 1241/2082 - Train Loss: 0.172345831990242\nBatch 1251/2082 - Train Loss: 0.20435470342636108\nBatch 1261/2082 - Train Loss: 0.3743269443511963\nBatch 1271/2082 - Train Loss: 0.3299722671508789\nBatch 1281/2082 - Train Loss: 0.2442018836736679\nBatch 1291/2082 - Train Loss: 0.409207284450531\nBatch 1301/2082 - Train Loss: 0.3099571466445923\nBatch 1311/2082 - Train Loss: 0.22229179739952087\nBatch 1321/2082 - Train Loss: 0.23272906243801117\nBatch 1331/2082 - Train Loss: 0.18337848782539368\nBatch 1341/2082 - Train Loss: 0.24220037460327148\nBatch 1351/2082 - Train Loss: 0.15284371376037598\nBatch 1361/2082 - Train Loss: 0.2508678734302521\nBatch 1371/2082 - Train Loss: 0.13057713210582733\nBatch 1381/2082 - Train Loss: 0.21385625004768372\nBatch 1391/2082 - Train Loss: 0.14727476239204407\nBatch 1401/2082 - Train Loss: 0.19433103501796722\nBatch 1411/2082 - Train Loss: 0.15069818496704102\nBatch 1421/2082 - Train Loss: 0.2664651870727539\nBatch 1431/2082 - Train Loss: 0.13575860857963562\nBatch 1441/2082 - Train Loss: 0.14758965373039246\nBatch 1451/2082 - Train Loss: 0.3793455958366394\nBatch 1461/2082 - Train Loss: 0.2548893690109253\nBatch 1471/2082 - Train Loss: 0.453819215297699\nBatch 1481/2082 - Train Loss: 0.07081902772188187\nBatch 1491/2082 - Train Loss: 0.1000487208366394\nBatch 1501/2082 - Train Loss: 0.23064394295215607\nBatch 1511/2082 - Train Loss: 0.11465810239315033\nBatch 1521/2082 - Train Loss: 0.27513387799263\nBatch 1531/2082 - Train Loss: 0.25645682215690613\nBatch 1541/2082 - Train Loss: 0.27431851625442505\nBatch 1551/2082 - Train Loss: 0.17029337584972382\nBatch 1561/2082 - Train Loss: 0.20776276290416718\nBatch 1571/2082 - Train Loss: 0.21070189774036407\nBatch 1581/2082 - Train Loss: 0.45863696932792664\nBatch 1591/2082 - Train Loss: 0.20369817316532135\nBatch 1601/2082 - Train Loss: 0.2220921367406845\nBatch 1611/2082 - Train Loss: 0.3278519809246063\nBatch 1621/2082 - Train Loss: 0.3414590656757355\nBatch 1631/2082 - Train Loss: 0.29917848110198975\nBatch 1641/2082 - Train Loss: 0.3283988833427429\nBatch 1651/2082 - Train Loss: 0.2087297886610031\nBatch 1661/2082 - Train Loss: 0.19064375758171082\nBatch 1671/2082 - Train Loss: 0.2079961895942688\nBatch 1681/2082 - Train Loss: 0.3227985203266144\nBatch 1691/2082 - Train Loss: 0.059294041246175766\nBatch 1701/2082 - Train Loss: 0.14153948426246643\nBatch 1711/2082 - Train Loss: 0.22703343629837036\nBatch 1721/2082 - Train Loss: 0.3199478089809418\nBatch 1731/2082 - Train Loss: 0.2858484983444214\nBatch 1741/2082 - Train Loss: 0.3184264898300171\nBatch 1751/2082 - Train Loss: 0.31364503502845764\nBatch 1761/2082 - Train Loss: 0.1997331976890564\nBatch 1771/2082 - Train Loss: 0.33327096700668335\nBatch 1781/2082 - Train Loss: 0.17387565970420837\nBatch 1791/2082 - Train Loss: 0.21097585558891296\nBatch 1801/2082 - Train Loss: 0.11875999718904495\nBatch 1811/2082 - Train Loss: 0.3047758936882019\nBatch 1821/2082 - Train Loss: 0.32660752534866333\nBatch 1831/2082 - Train Loss: 0.09811542183160782\nBatch 1841/2082 - Train Loss: 0.1766970008611679\nBatch 1851/2082 - Train Loss: 0.29380857944488525\nBatch 1861/2082 - Train Loss: 0.10986831784248352\nBatch 1871/2082 - Train Loss: 0.12348788231611252\nBatch 1881/2082 - Train Loss: 0.10534815490245819\nBatch 1891/2082 - Train Loss: 0.43647873401641846\nBatch 1901/2082 - Train Loss: 0.3934529721736908\nBatch 1911/2082 - Train Loss: 0.38069599866867065\nBatch 1921/2082 - Train Loss: 0.2263871282339096\nBatch 1931/2082 - Train Loss: 0.32243314385414124\nBatch 1941/2082 - Train Loss: 0.2237391173839569\nBatch 1951/2082 - Train Loss: 0.2260066270828247\nBatch 1961/2082 - Train Loss: 0.25165584683418274\nBatch 1971/2082 - Train Loss: 0.2512206435203552\nBatch 1981/2082 - Train Loss: 0.4276832640171051\nBatch 1991/2082 - Train Loss: 0.18879713118076324\nBatch 2001/2082 - Train Loss: 0.2801288068294525\nBatch 2011/2082 - Train Loss: 0.28300148248672485\nBatch 2021/2082 - Train Loss: 0.24039506912231445\nBatch 2031/2082 - Train Loss: 0.29283761978149414\nBatch 2041/2082 - Train Loss: 0.23742049932479858\nBatch 2051/2082 - Train Loss: 0.23664772510528564\nBatch 2061/2082 - Train Loss: 0.37825459241867065\nBatch 2071/2082 - Train Loss: 0.22539401054382324\nBatch 2081/2082 - Train Loss: 0.18203766644001007\nEpoch 3 - Average Train Loss: 0.2606275948507069\nBatch 1/261 - Validation Loss: 0.0945405587553978\nBatch 11/261 - Validation Loss: 0.3377266228199005\nBatch 21/261 - Validation Loss: 0.15883001685142517\nBatch 31/261 - Validation Loss: 0.27637049555778503\nBatch 41/261 - Validation Loss: 0.27253633737564087\nBatch 51/261 - Validation Loss: 0.16201381385326385\nBatch 61/261 - Validation Loss: 0.1809748262166977\nBatch 71/261 - Validation Loss: 0.16029396653175354\nBatch 81/261 - Validation Loss: 0.29751068353652954\nBatch 91/261 - Validation Loss: 0.3396949768066406\nBatch 101/261 - Validation Loss: 0.15793639421463013\nBatch 111/261 - Validation Loss: 0.3275163769721985\nBatch 121/261 - Validation Loss: 0.19137859344482422\nBatch 131/261 - Validation Loss: 0.5095641016960144\nBatch 141/261 - Validation Loss: 0.36353015899658203\nBatch 151/261 - Validation Loss: 0.34655529260635376\nBatch 161/261 - Validation Loss: 0.19295592606067657\nBatch 171/261 - Validation Loss: 0.4000031054019928\nBatch 181/261 - Validation Loss: 0.3017340302467346\nBatch 191/261 - Validation Loss: 0.2924615740776062\nBatch 201/261 - Validation Loss: 0.5511792302131653\nBatch 211/261 - Validation Loss: 0.1814425140619278\nBatch 221/261 - Validation Loss: 0.09937575459480286\nBatch 231/261 - Validation Loss: 0.15774349868297577\nBatch 241/261 - Validation Loss: 0.19055776298046112\nBatch 251/261 - Validation Loss: 0.4098545014858246\nBatch 261/261 - Validation Loss: 0.07061001658439636\nEpoch 3 - Average Validation Loss: 0.24825545852839717\nEpoch 4/10\nBatch 1/2082 - Train Loss: 0.4091660976409912\nBatch 11/2082 - Train Loss: 0.26388657093048096\nBatch 21/2082 - Train Loss: 0.21658815443515778\nBatch 31/2082 - Train Loss: 0.30232304334640503\nBatch 41/2082 - Train Loss: 0.337051123380661\nBatch 51/2082 - Train Loss: 0.2315727025270462\nBatch 61/2082 - Train Loss: 0.30063846707344055\nBatch 71/2082 - Train Loss: 0.12385019659996033\nBatch 81/2082 - Train Loss: 0.3652498424053192\nBatch 91/2082 - Train Loss: 0.1241006851196289\nBatch 101/2082 - Train Loss: 0.300900936126709\nBatch 111/2082 - Train Loss: 0.22387143969535828\nBatch 121/2082 - Train Loss: 0.10028915107250214\nBatch 131/2082 - Train Loss: 0.2484380602836609\nBatch 141/2082 - Train Loss: 0.14983372390270233\nBatch 151/2082 - Train Loss: 0.30423837900161743\nBatch 161/2082 - Train Loss: 0.35145506262779236\nBatch 171/2082 - Train Loss: 0.31163156032562256\nBatch 181/2082 - Train Loss: 0.14002642035484314\nBatch 191/2082 - Train Loss: 0.1444484144449234\nBatch 201/2082 - Train Loss: 0.2657402753829956\nBatch 211/2082 - Train Loss: 0.20876751840114594\nBatch 221/2082 - Train Loss: 0.1773877590894699\nBatch 231/2082 - Train Loss: 0.29453009366989136\nBatch 241/2082 - Train Loss: 0.33002132177352905\nBatch 251/2082 - Train Loss: 0.1273244023323059\nBatch 261/2082 - Train Loss: 0.30626821517944336\nBatch 271/2082 - Train Loss: 0.14924190938472748\nBatch 281/2082 - Train Loss: 0.21020573377609253\nBatch 291/2082 - Train Loss: 0.17186607420444489\nBatch 301/2082 - Train Loss: 0.2744632959365845\nBatch 311/2082 - Train Loss: 0.2210637629032135\nBatch 321/2082 - Train Loss: 0.3636062443256378\nBatch 331/2082 - Train Loss: 0.1046530231833458\nBatch 341/2082 - Train Loss: 0.220047727227211\nBatch 351/2082 - Train Loss: 0.1301218867301941\nBatch 361/2082 - Train Loss: 0.17839166522026062\nBatch 371/2082 - Train Loss: 0.3398537337779999\nBatch 381/2082 - Train Loss: 0.13899078965187073\nBatch 391/2082 - Train Loss: 0.24318277835845947\nBatch 401/2082 - Train Loss: 0.19074080884456635\nBatch 411/2082 - Train Loss: 0.46636155247688293\nBatch 421/2082 - Train Loss: 0.41410669684410095\nBatch 431/2082 - Train Loss: 0.1810525357723236\nBatch 441/2082 - Train Loss: 0.2254926860332489\nBatch 451/2082 - Train Loss: 0.2351292371749878\nBatch 461/2082 - Train Loss: 0.3231421709060669\nBatch 471/2082 - Train Loss: 0.21718159317970276\nBatch 481/2082 - Train Loss: 0.1673273742198944\nBatch 491/2082 - Train Loss: 0.2777892053127289\nBatch 501/2082 - Train Loss: 0.20139840245246887\nBatch 511/2082 - Train Loss: 0.18791668117046356\nBatch 521/2082 - Train Loss: 0.14845514297485352\nBatch 531/2082 - Train Loss: 0.10453855991363525\nBatch 541/2082 - Train Loss: 0.23716868460178375\nBatch 551/2082 - Train Loss: 0.1900712102651596\nBatch 561/2082 - Train Loss: 0.3456214964389801\nBatch 571/2082 - Train Loss: 0.12810595333576202\nBatch 581/2082 - Train Loss: 0.1392073780298233\nBatch 591/2082 - Train Loss: 0.24157315492630005\nBatch 601/2082 - Train Loss: 0.1722046583890915\nBatch 611/2082 - Train Loss: 0.31938666105270386\nBatch 621/2082 - Train Loss: 0.22553609311580658\nBatch 631/2082 - Train Loss: 0.47342702746391296\nBatch 641/2082 - Train Loss: 0.2743362784385681\nBatch 651/2082 - Train Loss: 0.19296236336231232\nBatch 661/2082 - Train Loss: 0.18670088052749634\nBatch 671/2082 - Train Loss: 0.12267032265663147\nBatch 681/2082 - Train Loss: 0.16192178428173065\nBatch 691/2082 - Train Loss: 0.3462112247943878\nBatch 701/2082 - Train Loss: 0.1654214859008789\nBatch 711/2082 - Train Loss: 0.16466595232486725\nBatch 721/2082 - Train Loss: 0.13509134948253632\nBatch 731/2082 - Train Loss: 0.1365385502576828\nBatch 741/2082 - Train Loss: 0.305203914642334\nBatch 751/2082 - Train Loss: 0.19516563415527344\nBatch 761/2082 - Train Loss: 0.35992631316185\nBatch 771/2082 - Train Loss: 0.1936071813106537\nBatch 781/2082 - Train Loss: 0.26050999760627747\nBatch 791/2082 - Train Loss: 0.32581281661987305\nBatch 801/2082 - Train Loss: 0.30066168308258057\nBatch 811/2082 - Train Loss: 0.31492361426353455\nBatch 821/2082 - Train Loss: 0.1778213083744049\nBatch 831/2082 - Train Loss: 0.3270883560180664\nBatch 841/2082 - Train Loss: 0.1328965276479721\nBatch 851/2082 - Train Loss: 0.44272086024284363\nBatch 861/2082 - Train Loss: 0.265577107667923\nBatch 871/2082 - Train Loss: 0.16212564706802368\nBatch 881/2082 - Train Loss: 0.1665724813938141\nBatch 891/2082 - Train Loss: 0.315026193857193\nBatch 901/2082 - Train Loss: 0.20865830779075623\nBatch 911/2082 - Train Loss: 0.2569204568862915\nBatch 921/2082 - Train Loss: 0.17267952859401703\nBatch 931/2082 - Train Loss: 0.2597792446613312\nBatch 941/2082 - Train Loss: 0.23241932690143585\nBatch 951/2082 - Train Loss: 0.2969937324523926\nBatch 961/2082 - Train Loss: 0.44031062722206116\nBatch 971/2082 - Train Loss: 0.36053717136383057\nBatch 981/2082 - Train Loss: 0.3322184681892395\nBatch 991/2082 - Train Loss: 0.22464756667613983\nBatch 1001/2082 - Train Loss: 0.2516772150993347\nBatch 1011/2082 - Train Loss: 0.23979614675045013\nBatch 1021/2082 - Train Loss: 0.43689799308776855\nBatch 1031/2082 - Train Loss: 0.3382817804813385\nBatch 1041/2082 - Train Loss: 0.332738995552063\nBatch 1051/2082 - Train Loss: 0.2071569859981537\nBatch 1061/2082 - Train Loss: 0.1336510181427002\nBatch 1071/2082 - Train Loss: 0.432806134223938\nBatch 1081/2082 - Train Loss: 0.16594217717647552\nBatch 1091/2082 - Train Loss: 0.2687826454639435\nBatch 1101/2082 - Train Loss: 0.139949232339859\nBatch 1111/2082 - Train Loss: 0.16900204122066498\nBatch 1121/2082 - Train Loss: 0.18393681943416595\nBatch 1131/2082 - Train Loss: 0.14575070142745972\nBatch 1141/2082 - Train Loss: 0.38806024193763733\nBatch 1151/2082 - Train Loss: 0.3716839551925659\nBatch 1161/2082 - Train Loss: 0.24635764956474304\nBatch 1171/2082 - Train Loss: 0.16513419151306152\nBatch 1181/2082 - Train Loss: 0.23824939131736755\nBatch 1191/2082 - Train Loss: 0.16722461581230164\nBatch 1201/2082 - Train Loss: 0.37336423993110657\nBatch 1211/2082 - Train Loss: 0.22887258231639862\nBatch 1221/2082 - Train Loss: 0.3008081912994385\nBatch 1231/2082 - Train Loss: 0.13243509829044342\nBatch 1241/2082 - Train Loss: 0.2918301522731781\nBatch 1251/2082 - Train Loss: 0.24985460937023163\nBatch 1261/2082 - Train Loss: 0.1845652461051941\nBatch 1271/2082 - Train Loss: 0.18670441210269928\nBatch 1281/2082 - Train Loss: 0.3436000645160675\nBatch 1291/2082 - Train Loss: 0.4832020699977875\nBatch 1301/2082 - Train Loss: 0.22051534056663513\nBatch 1311/2082 - Train Loss: 0.18077626824378967\nBatch 1321/2082 - Train Loss: 0.1927884966135025\nBatch 1331/2082 - Train Loss: 0.21847257018089294\nBatch 1341/2082 - Train Loss: 0.2816198468208313\nBatch 1351/2082 - Train Loss: 0.3698247969150543\nBatch 1361/2082 - Train Loss: 0.17965686321258545\nBatch 1371/2082 - Train Loss: 0.3460749685764313\nBatch 1381/2082 - Train Loss: 0.25098302960395813\nBatch 1391/2082 - Train Loss: 0.6728254556655884\nBatch 1401/2082 - Train Loss: 0.187316432595253\nBatch 1411/2082 - Train Loss: 0.26583796739578247\nBatch 1421/2082 - Train Loss: 0.25624319911003113\nBatch 1431/2082 - Train Loss: 0.24695289134979248\nBatch 1441/2082 - Train Loss: 0.25338590145111084\nBatch 1451/2082 - Train Loss: 0.16747230291366577\nBatch 1461/2082 - Train Loss: 0.16200071573257446\nBatch 1471/2082 - Train Loss: 0.23409754037857056\nBatch 1481/2082 - Train Loss: 0.33195069432258606\nBatch 1491/2082 - Train Loss: 0.26166167855262756\nBatch 1501/2082 - Train Loss: 0.13690224289894104\nBatch 1511/2082 - Train Loss: 0.36822301149368286\nBatch 1521/2082 - Train Loss: 0.17896725237369537\nBatch 1531/2082 - Train Loss: 0.35967326164245605\nBatch 1541/2082 - Train Loss: 0.16463184356689453\nBatch 1551/2082 - Train Loss: 0.399632066488266\nBatch 1561/2082 - Train Loss: 0.16984505951404572\nBatch 1571/2082 - Train Loss: 0.3210790753364563\nBatch 1581/2082 - Train Loss: 0.28658246994018555\nBatch 1591/2082 - Train Loss: 0.11575648933649063\nBatch 1601/2082 - Train Loss: 0.27220389246940613\nBatch 1611/2082 - Train Loss: 0.20072589814662933\nBatch 1621/2082 - Train Loss: 0.26650571823120117\nBatch 1631/2082 - Train Loss: 0.15534810721874237\nBatch 1641/2082 - Train Loss: 0.17244209349155426\nBatch 1651/2082 - Train Loss: 0.20169515907764435\nBatch 1661/2082 - Train Loss: 0.21429763734340668\nBatch 1671/2082 - Train Loss: 0.3313421905040741\nBatch 1681/2082 - Train Loss: 0.1647258698940277\nBatch 1691/2082 - Train Loss: 0.2673799991607666\nBatch 1701/2082 - Train Loss: 0.21565623581409454\nBatch 1711/2082 - Train Loss: 0.274577796459198\nBatch 1721/2082 - Train Loss: 0.20618665218353271\nBatch 1731/2082 - Train Loss: 0.3291322588920593\nBatch 1741/2082 - Train Loss: 0.15939921140670776\nBatch 1751/2082 - Train Loss: 0.4102538228034973\nBatch 1761/2082 - Train Loss: 0.16778266429901123\nBatch 1771/2082 - Train Loss: 0.3832223117351532\nBatch 1781/2082 - Train Loss: 0.184406578540802\nBatch 1791/2082 - Train Loss: 0.24476124346256256\nBatch 1801/2082 - Train Loss: 0.20242784917354584\nBatch 1811/2082 - Train Loss: 0.11300729215145111\nBatch 1821/2082 - Train Loss: 0.22818662226200104\nBatch 1831/2082 - Train Loss: 0.2044091373682022\nBatch 1841/2082 - Train Loss: 0.21910972893238068\nBatch 1851/2082 - Train Loss: 0.2648558020591736\nBatch 1861/2082 - Train Loss: 0.20470571517944336\nBatch 1871/2082 - Train Loss: 0.22047394514083862\nBatch 1881/2082 - Train Loss: 0.11958570033311844\nBatch 1891/2082 - Train Loss: 0.42828887701034546\nBatch 1901/2082 - Train Loss: 0.3981737196445465\nBatch 1911/2082 - Train Loss: 0.2730828821659088\nBatch 1921/2082 - Train Loss: 0.18880553543567657\nBatch 1931/2082 - Train Loss: 0.10933584719896317\nBatch 1941/2082 - Train Loss: 0.32752543687820435\nBatch 1951/2082 - Train Loss: 0.31154659390449524\nBatch 1961/2082 - Train Loss: 0.05763756483793259\nBatch 1971/2082 - Train Loss: 0.23673182725906372\nBatch 1981/2082 - Train Loss: 0.1834437996149063\nBatch 1991/2082 - Train Loss: 0.2265220433473587\nBatch 2001/2082 - Train Loss: 0.13803377747535706\nBatch 2011/2082 - Train Loss: 0.41854405403137207\nBatch 2021/2082 - Train Loss: 0.43338343501091003\nBatch 2031/2082 - Train Loss: 0.2123834192752838\nBatch 2041/2082 - Train Loss: 0.4032045602798462\nBatch 2051/2082 - Train Loss: 0.1426657885313034\nBatch 2061/2082 - Train Loss: 0.19599629938602448\nBatch 2071/2082 - Train Loss: 0.21834975481033325\nBatch 2081/2082 - Train Loss: 0.3641083240509033\nEpoch 4 - Average Train Loss: 0.24669585086219192\nBatch 1/261 - Validation Loss: 0.09858138114213943\nBatch 11/261 - Validation Loss: 0.33000463247299194\nBatch 21/261 - Validation Loss: 0.15729111433029175\nBatch 31/261 - Validation Loss: 0.27096280455589294\nBatch 41/261 - Validation Loss: 0.26651468873023987\nBatch 51/261 - Validation Loss: 0.1548590213060379\nBatch 61/261 - Validation Loss: 0.18312512338161469\nBatch 71/261 - Validation Loss: 0.16039739549160004\nBatch 81/261 - Validation Loss: 0.2892448902130127\nBatch 91/261 - Validation Loss: 0.3320760726928711\nBatch 101/261 - Validation Loss: 0.15662401914596558\nBatch 111/261 - Validation Loss: 0.32111936807632446\nBatch 121/261 - Validation Loss: 0.18566668033599854\nBatch 131/261 - Validation Loss: 0.5081120729446411\nBatch 141/261 - Validation Loss: 0.35575857758522034\nBatch 151/261 - Validation Loss: 0.3417401909828186\nBatch 161/261 - Validation Loss: 0.19042564928531647\nBatch 171/261 - Validation Loss: 0.397279292345047\nBatch 181/261 - Validation Loss: 0.3008350729942322\nBatch 191/261 - Validation Loss: 0.2902948260307312\nBatch 201/261 - Validation Loss: 0.5489572286605835\nBatch 211/261 - Validation Loss: 0.18223384022712708\nBatch 221/261 - Validation Loss: 0.09245935082435608\nBatch 231/261 - Validation Loss: 0.15913307666778564\nBatch 241/261 - Validation Loss: 0.19026145339012146\nBatch 251/261 - Validation Loss: 0.41115084290504456\nBatch 261/261 - Validation Loss: 0.08542031794786453\nEpoch 4 - Average Validation Loss: 0.2444338432274102\nEpoch 5/10\nBatch 1/2082 - Train Loss: 0.29099029302597046\nBatch 11/2082 - Train Loss: 0.16231949627399445\nBatch 21/2082 - Train Loss: 0.39497727155685425\nBatch 31/2082 - Train Loss: 0.19395841658115387\nBatch 41/2082 - Train Loss: 0.1942552626132965\nBatch 51/2082 - Train Loss: 0.22038380801677704\nBatch 61/2082 - Train Loss: 0.10378891974687576\nBatch 71/2082 - Train Loss: 0.09711664915084839\nBatch 81/2082 - Train Loss: 0.2669217884540558\nBatch 91/2082 - Train Loss: 0.23467928171157837\nBatch 101/2082 - Train Loss: 0.38803762197494507\nBatch 111/2082 - Train Loss: 0.1675063669681549\nBatch 121/2082 - Train Loss: 0.1291860193014145\nBatch 131/2082 - Train Loss: 0.22754748165607452\nBatch 141/2082 - Train Loss: 0.17464317381381989\nBatch 151/2082 - Train Loss: 0.2872489094734192\nBatch 161/2082 - Train Loss: 0.13767988979816437\nBatch 171/2082 - Train Loss: 0.246097132563591\nBatch 181/2082 - Train Loss: 0.43078938126564026\nBatch 191/2082 - Train Loss: 0.13860975205898285\nBatch 201/2082 - Train Loss: 0.24149401485919952\nBatch 211/2082 - Train Loss: 0.24194419384002686\nBatch 221/2082 - Train Loss: 0.23996683955192566\nBatch 231/2082 - Train Loss: 0.30526047945022583\nBatch 241/2082 - Train Loss: 0.2354021668434143\nBatch 251/2082 - Train Loss: 0.3145694434642792\nBatch 261/2082 - Train Loss: 0.3703370988368988\nBatch 271/2082 - Train Loss: 0.23567433655261993\nBatch 281/2082 - Train Loss: 0.31927362084388733\nBatch 291/2082 - Train Loss: 0.5733345150947571\nBatch 301/2082 - Train Loss: 0.14475665986537933\nBatch 311/2082 - Train Loss: 0.12674857676029205\nBatch 321/2082 - Train Loss: 0.27694010734558105\nBatch 331/2082 - Train Loss: 0.27637818455696106\nBatch 341/2082 - Train Loss: 0.2879501283168793\nBatch 351/2082 - Train Loss: 0.22194544970989227\nBatch 361/2082 - Train Loss: 0.15079545974731445\nBatch 371/2082 - Train Loss: 0.12116217613220215\nBatch 381/2082 - Train Loss: 0.22591674327850342\nBatch 391/2082 - Train Loss: 0.4116440415382385\nBatch 401/2082 - Train Loss: 0.1632295548915863\nBatch 411/2082 - Train Loss: 0.20224860310554504\nBatch 421/2082 - Train Loss: 0.26376351714134216\nBatch 431/2082 - Train Loss: 0.3372306227684021\nBatch 441/2082 - Train Loss: 0.31821197271347046\nBatch 451/2082 - Train Loss: 0.19359010457992554\nBatch 461/2082 - Train Loss: 0.30049148201942444\nBatch 471/2082 - Train Loss: 0.17547273635864258\nBatch 481/2082 - Train Loss: 0.2868538498878479\nBatch 491/2082 - Train Loss: 0.3496468663215637\nBatch 501/2082 - Train Loss: 0.4521218240261078\nBatch 511/2082 - Train Loss: 0.1308014988899231\nBatch 521/2082 - Train Loss: 0.22165493667125702\nBatch 531/2082 - Train Loss: 0.1673460453748703\nBatch 541/2082 - Train Loss: 0.19168564677238464\nBatch 551/2082 - Train Loss: 0.1836586743593216\nBatch 561/2082 - Train Loss: 0.5079577565193176\nBatch 571/2082 - Train Loss: 0.21402890980243683\nBatch 581/2082 - Train Loss: 0.09450706094503403\nBatch 591/2082 - Train Loss: 0.3176875114440918\nBatch 601/2082 - Train Loss: 0.32496464252471924\nBatch 611/2082 - Train Loss: 0.45321232080459595\nBatch 621/2082 - Train Loss: 0.29438459873199463\nBatch 631/2082 - Train Loss: 0.4320828318595886\nBatch 641/2082 - Train Loss: 0.41021546721458435\nBatch 651/2082 - Train Loss: 0.3485027253627777\nBatch 661/2082 - Train Loss: 0.14878933131694794\nBatch 671/2082 - Train Loss: 0.17374996840953827\nBatch 681/2082 - Train Loss: 0.3088832199573517\nBatch 691/2082 - Train Loss: 0.13180261850357056\nBatch 701/2082 - Train Loss: 0.211265429854393\nBatch 711/2082 - Train Loss: 0.1956995725631714\nBatch 721/2082 - Train Loss: 0.21687087416648865\nBatch 731/2082 - Train Loss: 0.2535032629966736\nBatch 741/2082 - Train Loss: 0.6417355537414551\nBatch 751/2082 - Train Loss: 0.3076998293399811\nBatch 761/2082 - Train Loss: 0.23447774350643158\nBatch 771/2082 - Train Loss: 0.3555891513824463\nBatch 781/2082 - Train Loss: 0.23609425127506256\nBatch 791/2082 - Train Loss: 0.162350133061409\nBatch 801/2082 - Train Loss: 0.3009105920791626\nBatch 811/2082 - Train Loss: 0.30433520674705505\nBatch 821/2082 - Train Loss: 0.36577463150024414\nBatch 831/2082 - Train Loss: 0.145847886800766\nBatch 841/2082 - Train Loss: 0.5964840054512024\nBatch 851/2082 - Train Loss: 0.2091744989156723\nBatch 861/2082 - Train Loss: 0.22655512392520905\nBatch 871/2082 - Train Loss: 0.37497517466545105\nBatch 881/2082 - Train Loss: 0.11141611635684967\nBatch 891/2082 - Train Loss: 0.14999228715896606\nBatch 901/2082 - Train Loss: 0.22811853885650635\nBatch 911/2082 - Train Loss: 0.2662583887577057\nBatch 921/2082 - Train Loss: 0.2718447744846344\nBatch 931/2082 - Train Loss: 0.35564905405044556\nBatch 941/2082 - Train Loss: 0.3566990792751312\nBatch 951/2082 - Train Loss: 0.1525951474905014\nBatch 961/2082 - Train Loss: 0.23135678470134735\nBatch 971/2082 - Train Loss: 0.20672212541103363\nBatch 981/2082 - Train Loss: 0.18627937138080597\nBatch 991/2082 - Train Loss: 0.15771891176700592\nBatch 1001/2082 - Train Loss: 0.2576240003108978\nBatch 1011/2082 - Train Loss: 0.23769620060920715\nBatch 1021/2082 - Train Loss: 0.16320805251598358\nBatch 1031/2082 - Train Loss: 0.13140016794204712\nBatch 1041/2082 - Train Loss: 0.28505370020866394\nBatch 1051/2082 - Train Loss: 0.12053786963224411\nBatch 1061/2082 - Train Loss: 0.2798352539539337\nBatch 1071/2082 - Train Loss: 0.39048030972480774\nBatch 1081/2082 - Train Loss: 0.19963251054286957\nBatch 1091/2082 - Train Loss: 0.45322391390800476\nBatch 1101/2082 - Train Loss: 0.3238896429538727\nBatch 1111/2082 - Train Loss: 0.2134087085723877\nBatch 1121/2082 - Train Loss: 0.14753222465515137\nBatch 1131/2082 - Train Loss: 0.23757131397724152\nBatch 1141/2082 - Train Loss: 0.22507566213607788\nBatch 1151/2082 - Train Loss: 0.16060934960842133\nBatch 1161/2082 - Train Loss: 0.256430059671402\nBatch 1171/2082 - Train Loss: 0.20320650935173035\nBatch 1181/2082 - Train Loss: 0.3534126579761505\nBatch 1191/2082 - Train Loss: 0.27884751558303833\nBatch 1201/2082 - Train Loss: 0.3670285642147064\nBatch 1211/2082 - Train Loss: 0.2108321487903595\nBatch 1221/2082 - Train Loss: 0.30191782116889954\nBatch 1231/2082 - Train Loss: 0.11517917364835739\nBatch 1241/2082 - Train Loss: 0.2627786099910736\nBatch 1251/2082 - Train Loss: 0.28230026364326477\nBatch 1261/2082 - Train Loss: 0.36426642537117004\nBatch 1271/2082 - Train Loss: 0.2025160938501358\nBatch 1281/2082 - Train Loss: 0.1175231784582138\nBatch 1291/2082 - Train Loss: 0.08762166649103165\nBatch 1301/2082 - Train Loss: 0.09123751521110535\nBatch 1311/2082 - Train Loss: 0.18883922696113586\nBatch 1321/2082 - Train Loss: 0.18993829190731049\nBatch 1331/2082 - Train Loss: 0.4697475731372833\nBatch 1341/2082 - Train Loss: 0.16619271039962769\nBatch 1351/2082 - Train Loss: 0.5061016082763672\nBatch 1361/2082 - Train Loss: 0.16951902210712433\nBatch 1371/2082 - Train Loss: 0.24597182869911194\nBatch 1381/2082 - Train Loss: 0.3241182565689087\nBatch 1391/2082 - Train Loss: 0.12371804565191269\nBatch 1401/2082 - Train Loss: 0.32969263195991516\nBatch 1411/2082 - Train Loss: 0.2023295909166336\nBatch 1421/2082 - Train Loss: 0.2002636343240738\nBatch 1431/2082 - Train Loss: 0.35299691557884216\nBatch 1441/2082 - Train Loss: 0.21718528866767883\nBatch 1451/2082 - Train Loss: 0.2140970379114151\nBatch 1461/2082 - Train Loss: 0.056507408618927\nBatch 1471/2082 - Train Loss: 0.12052883207798004\nBatch 1481/2082 - Train Loss: 0.13058479130268097\nBatch 1491/2082 - Train Loss: 0.11001290380954742\nBatch 1501/2082 - Train Loss: 0.17791986465454102\nBatch 1511/2082 - Train Loss: 0.13473393023014069\nBatch 1521/2082 - Train Loss: 0.37479478120803833\nBatch 1531/2082 - Train Loss: 0.19338947534561157\nBatch 1541/2082 - Train Loss: 0.41448071599006653\nBatch 1551/2082 - Train Loss: 0.15955877304077148\nBatch 1561/2082 - Train Loss: 0.2235090136528015\nBatch 1571/2082 - Train Loss: 0.19293953478336334\nBatch 1581/2082 - Train Loss: 0.14505235850811005\nBatch 1591/2082 - Train Loss: 0.134828582406044\nBatch 1601/2082 - Train Loss: 0.25295042991638184\nBatch 1611/2082 - Train Loss: 0.12891696393489838\nBatch 1621/2082 - Train Loss: 0.21326637268066406\nBatch 1631/2082 - Train Loss: 0.15446621179580688\nBatch 1641/2082 - Train Loss: 0.15702709555625916\nBatch 1651/2082 - Train Loss: 0.23236066102981567\nBatch 1661/2082 - Train Loss: 0.13143302500247955\nBatch 1671/2082 - Train Loss: 0.16788837313652039\nBatch 1681/2082 - Train Loss: 0.1514049470424652\nBatch 1691/2082 - Train Loss: 0.18311132490634918\nBatch 1701/2082 - Train Loss: 0.2870149612426758\nBatch 1711/2082 - Train Loss: 0.27895382046699524\nBatch 1721/2082 - Train Loss: 0.16346335411071777\nBatch 1731/2082 - Train Loss: 0.1641717106103897\nBatch 1741/2082 - Train Loss: 0.46125951409339905\nBatch 1751/2082 - Train Loss: 0.3174103796482086\nBatch 1761/2082 - Train Loss: 0.2786557972431183\nBatch 1771/2082 - Train Loss: 0.21027682721614838\nBatch 1781/2082 - Train Loss: 0.19792558252811432\nBatch 1791/2082 - Train Loss: 0.07932478189468384\nBatch 1801/2082 - Train Loss: 0.12716421484947205\nBatch 1811/2082 - Train Loss: 0.12440966069698334\nBatch 1821/2082 - Train Loss: 0.1075000986456871\nBatch 1831/2082 - Train Loss: 0.17810140550136566\nBatch 1841/2082 - Train Loss: 0.25796160101890564\nBatch 1851/2082 - Train Loss: 0.19922003149986267\nBatch 1861/2082 - Train Loss: 0.22705452144145966\nBatch 1871/2082 - Train Loss: 0.25548359751701355\nBatch 1881/2082 - Train Loss: 0.22454430162906647\nBatch 1891/2082 - Train Loss: 0.150096595287323\nBatch 1901/2082 - Train Loss: 0.12582416832447052\nBatch 1911/2082 - Train Loss: 0.2160724699497223\nBatch 1921/2082 - Train Loss: 0.07312624901533127\nBatch 1931/2082 - Train Loss: 0.32736736536026\nBatch 1941/2082 - Train Loss: 0.21096019446849823\nBatch 1951/2082 - Train Loss: 0.15698225796222687\nBatch 1961/2082 - Train Loss: 0.2846826910972595\nBatch 1971/2082 - Train Loss: 0.17311102151870728\nBatch 1981/2082 - Train Loss: 0.2629389762878418\nBatch 1991/2082 - Train Loss: 0.15892335772514343\nBatch 2001/2082 - Train Loss: 0.22911769151687622\nBatch 2011/2082 - Train Loss: 0.18788623809814453\nBatch 2021/2082 - Train Loss: 0.1629609763622284\nBatch 2031/2082 - Train Loss: 0.21059606969356537\nBatch 2041/2082 - Train Loss: 0.28901827335357666\nBatch 2051/2082 - Train Loss: 0.21029168367385864\nBatch 2061/2082 - Train Loss: 0.2837215065956116\nBatch 2071/2082 - Train Loss: 0.18355464935302734\nBatch 2081/2082 - Train Loss: 0.2599572539329529\nEpoch 5 - Average Train Loss: 0.23523839668520188\nBatch 1/261 - Validation Loss: 0.0970388874411583\nBatch 11/261 - Validation Loss: 0.32860276103019714\nBatch 21/261 - Validation Loss: 0.1525786817073822\nBatch 31/261 - Validation Loss: 0.2642430365085602\nBatch 41/261 - Validation Loss: 0.26700350642204285\nBatch 51/261 - Validation Loss: 0.15453769266605377\nBatch 61/261 - Validation Loss: 0.18398970365524292\nBatch 71/261 - Validation Loss: 0.15849484503269196\nBatch 81/261 - Validation Loss: 0.2844010591506958\nBatch 91/261 - Validation Loss: 0.33023929595947266\nBatch 101/261 - Validation Loss: 0.15828295052051544\nBatch 111/261 - Validation Loss: 0.3214682936668396\nBatch 121/261 - Validation Loss: 0.1776973158121109\nBatch 131/261 - Validation Loss: 0.5039969086647034\nBatch 141/261 - Validation Loss: 0.3525300621986389\nBatch 151/261 - Validation Loss: 0.33860620856285095\nBatch 161/261 - Validation Loss: 0.18770155310630798\nBatch 171/261 - Validation Loss: 0.3974212408065796\nBatch 181/261 - Validation Loss: 0.29675498604774475\nBatch 191/261 - Validation Loss: 0.29200631380081177\nBatch 201/261 - Validation Loss: 0.5432080030441284\nBatch 211/261 - Validation Loss: 0.17908117175102234\nBatch 221/261 - Validation Loss: 0.09250186383724213\nBatch 231/261 - Validation Loss: 0.1592664271593094\nBatch 241/261 - Validation Loss: 0.19244788587093353\nBatch 251/261 - Validation Loss: 0.4100077152252197\nBatch 261/261 - Validation Loss: 0.07746993005275726\nEpoch 5 - Average Validation Loss: 0.24238037771877202\nEpoch 6/10\nBatch 1/2082 - Train Loss: 0.09859751164913177\nBatch 11/2082 - Train Loss: 0.34626448154449463\nBatch 21/2082 - Train Loss: 0.09131494164466858\nBatch 31/2082 - Train Loss: 0.20182479918003082\nBatch 41/2082 - Train Loss: 0.24602846801280975\nBatch 51/2082 - Train Loss: 0.1361243724822998\nBatch 61/2082 - Train Loss: 0.2784872055053711\nBatch 71/2082 - Train Loss: 0.2673042118549347\nBatch 81/2082 - Train Loss: 0.17161111533641815\nBatch 91/2082 - Train Loss: 0.21040275692939758\nBatch 101/2082 - Train Loss: 0.16468654572963715\nBatch 111/2082 - Train Loss: 0.20480650663375854\nBatch 121/2082 - Train Loss: 0.27900809049606323\nBatch 131/2082 - Train Loss: 0.3492003083229065\nBatch 141/2082 - Train Loss: 0.2479255199432373\nBatch 151/2082 - Train Loss: 0.1728382706642151\nBatch 161/2082 - Train Loss: 0.26641350984573364\nBatch 171/2082 - Train Loss: 0.18383151292800903\nBatch 181/2082 - Train Loss: 0.3562384843826294\nBatch 191/2082 - Train Loss: 0.1374284327030182\nBatch 201/2082 - Train Loss: 0.21153676509857178\nBatch 211/2082 - Train Loss: 0.29231783747673035\nBatch 221/2082 - Train Loss: 0.29279419779777527\nBatch 231/2082 - Train Loss: 0.15460173785686493\nBatch 241/2082 - Train Loss: 0.210634246468544\nBatch 251/2082 - Train Loss: 0.30256980657577515\nBatch 261/2082 - Train Loss: 0.21128416061401367\nBatch 271/2082 - Train Loss: 0.19278503954410553\nBatch 281/2082 - Train Loss: 0.2371220886707306\nBatch 291/2082 - Train Loss: 0.3273546099662781\nBatch 301/2082 - Train Loss: 0.15488015115261078\nBatch 311/2082 - Train Loss: 0.10403931140899658\nBatch 321/2082 - Train Loss: 0.29835954308509827\nBatch 331/2082 - Train Loss: 0.18761630356311798\nBatch 341/2082 - Train Loss: 0.2776002287864685\nBatch 351/2082 - Train Loss: 0.1780083179473877\nBatch 361/2082 - Train Loss: 0.14054280519485474\nBatch 371/2082 - Train Loss: 0.4389004111289978\nBatch 381/2082 - Train Loss: 0.14538167417049408\nBatch 391/2082 - Train Loss: 0.18555909395217896\nBatch 401/2082 - Train Loss: 0.2137986570596695\nBatch 411/2082 - Train Loss: 0.11492502689361572\nBatch 421/2082 - Train Loss: 0.14592345058918\nBatch 431/2082 - Train Loss: 0.4235609471797943\nBatch 441/2082 - Train Loss: 0.15600275993347168\nBatch 451/2082 - Train Loss: 0.10623633116483688\nBatch 461/2082 - Train Loss: 0.3262005150318146\nBatch 471/2082 - Train Loss: 0.34121572971343994\nBatch 481/2082 - Train Loss: 0.10588707029819489\nBatch 491/2082 - Train Loss: 0.20394951105117798\nBatch 501/2082 - Train Loss: 0.08367626368999481\nBatch 511/2082 - Train Loss: 0.18736056983470917\nBatch 521/2082 - Train Loss: 0.2064417004585266\nBatch 531/2082 - Train Loss: 0.15328608453273773\nBatch 541/2082 - Train Loss: 0.14417314529418945\nBatch 551/2082 - Train Loss: 0.282004714012146\nBatch 561/2082 - Train Loss: 0.22161094844341278\nBatch 571/2082 - Train Loss: 0.1967441886663437\nBatch 581/2082 - Train Loss: 0.22231793403625488\nBatch 591/2082 - Train Loss: 0.13608139753341675\nBatch 601/2082 - Train Loss: 0.28032374382019043\nBatch 611/2082 - Train Loss: 0.2350223809480667\nBatch 621/2082 - Train Loss: 0.2375345677137375\nBatch 631/2082 - Train Loss: 0.1647687554359436\nBatch 641/2082 - Train Loss: 0.305864542722702\nBatch 651/2082 - Train Loss: 0.2133050560951233\nBatch 661/2082 - Train Loss: 0.2678980529308319\nBatch 671/2082 - Train Loss: 0.28875532746315\nBatch 681/2082 - Train Loss: 0.3026059865951538\nBatch 691/2082 - Train Loss: 0.29659727215766907\nBatch 701/2082 - Train Loss: 0.23058877885341644\nBatch 711/2082 - Train Loss: 0.34235575795173645\nBatch 721/2082 - Train Loss: 0.1504637598991394\nBatch 731/2082 - Train Loss: 0.15816468000411987\nBatch 741/2082 - Train Loss: 0.22147171199321747\nBatch 751/2082 - Train Loss: 0.3047470152378082\nBatch 761/2082 - Train Loss: 0.16907669603824615\nBatch 771/2082 - Train Loss: 0.26476767659187317\nBatch 781/2082 - Train Loss: 0.2935735285282135\nBatch 791/2082 - Train Loss: 0.31265270709991455\nBatch 801/2082 - Train Loss: 0.18382935225963593\nBatch 811/2082 - Train Loss: 0.20228451490402222\nBatch 821/2082 - Train Loss: 0.24636004865169525\nBatch 831/2082 - Train Loss: 0.3137918710708618\nBatch 841/2082 - Train Loss: 0.4610634446144104\nBatch 851/2082 - Train Loss: 0.0675233006477356\nBatch 861/2082 - Train Loss: 0.16404469311237335\nBatch 871/2082 - Train Loss: 0.2712392807006836\nBatch 881/2082 - Train Loss: 0.130992129445076\nBatch 891/2082 - Train Loss: 0.08098728209733963\nBatch 901/2082 - Train Loss: 0.17407941818237305\nBatch 911/2082 - Train Loss: 0.2707456946372986\nBatch 921/2082 - Train Loss: 0.3112528622150421\nBatch 931/2082 - Train Loss: 0.29475757479667664\nBatch 941/2082 - Train Loss: 0.6484553813934326\nBatch 951/2082 - Train Loss: 0.12999646365642548\nBatch 961/2082 - Train Loss: 0.08486740291118622\nBatch 971/2082 - Train Loss: 0.12551908195018768\nBatch 981/2082 - Train Loss: 0.19809073209762573\nBatch 991/2082 - Train Loss: 0.16258437931537628\nBatch 1001/2082 - Train Loss: 0.18163156509399414\nBatch 1011/2082 - Train Loss: 0.13646887242794037\nBatch 1021/2082 - Train Loss: 0.1766432672739029\nBatch 1031/2082 - Train Loss: 0.10547181963920593\nBatch 1041/2082 - Train Loss: 0.3202301561832428\nBatch 1051/2082 - Train Loss: 0.08911342173814774\nBatch 1061/2082 - Train Loss: 0.1254977434873581\nBatch 1071/2082 - Train Loss: 0.1157846599817276\nBatch 1081/2082 - Train Loss: 0.28900623321533203\nBatch 1091/2082 - Train Loss: 0.12495508790016174\nBatch 1101/2082 - Train Loss: 0.18659283220767975\nBatch 1111/2082 - Train Loss: 0.07873073220252991\nBatch 1121/2082 - Train Loss: 0.29395008087158203\nBatch 1131/2082 - Train Loss: 0.19668550789356232\nBatch 1141/2082 - Train Loss: 0.20832230150699615\nBatch 1151/2082 - Train Loss: 0.1593361347913742\nBatch 1161/2082 - Train Loss: 0.3468882441520691\nBatch 1171/2082 - Train Loss: 0.11385069787502289\nBatch 1181/2082 - Train Loss: 0.2540859580039978\nBatch 1191/2082 - Train Loss: 0.12670469284057617\nBatch 1201/2082 - Train Loss: 0.18840371072292328\nBatch 1211/2082 - Train Loss: 0.23438914120197296\nBatch 1221/2082 - Train Loss: 0.1657445728778839\nBatch 1231/2082 - Train Loss: 0.16636987030506134\nBatch 1241/2082 - Train Loss: 0.17778703570365906\nBatch 1251/2082 - Train Loss: 0.06389448791742325\nBatch 1261/2082 - Train Loss: 0.16806237399578094\nBatch 1271/2082 - Train Loss: 0.18592222034931183\nBatch 1281/2082 - Train Loss: 0.2659089267253876\nBatch 1291/2082 - Train Loss: 0.24348655343055725\nBatch 1301/2082 - Train Loss: 0.15758535265922546\nBatch 1311/2082 - Train Loss: 0.4314314126968384\nBatch 1321/2082 - Train Loss: 0.28049737215042114\nBatch 1331/2082 - Train Loss: 0.23512746393680573\nBatch 1341/2082 - Train Loss: 0.2785329818725586\nBatch 1351/2082 - Train Loss: 0.18861021101474762\nBatch 1361/2082 - Train Loss: 0.3809967637062073\nBatch 1371/2082 - Train Loss: 0.21271148324012756\nBatch 1381/2082 - Train Loss: 0.3455955982208252\nBatch 1391/2082 - Train Loss: 0.18074442446231842\nBatch 1401/2082 - Train Loss: 0.29179003834724426\nBatch 1411/2082 - Train Loss: 0.41981905698776245\nBatch 1421/2082 - Train Loss: 0.2671533524990082\nBatch 1431/2082 - Train Loss: 0.18437092006206512\nBatch 1441/2082 - Train Loss: 0.12999378144741058\nBatch 1451/2082 - Train Loss: 0.39008229970932007\nBatch 1461/2082 - Train Loss: 0.144028440117836\nBatch 1471/2082 - Train Loss: 0.254286527633667\nBatch 1481/2082 - Train Loss: 0.1283360719680786\nBatch 1491/2082 - Train Loss: 0.4002901017665863\nBatch 1501/2082 - Train Loss: 0.24694229662418365\nBatch 1511/2082 - Train Loss: 0.3484133780002594\nBatch 1521/2082 - Train Loss: 0.23427142202854156\nBatch 1531/2082 - Train Loss: 0.27484649419784546\nBatch 1541/2082 - Train Loss: 0.3088773190975189\nBatch 1551/2082 - Train Loss: 0.22833704948425293\nBatch 1561/2082 - Train Loss: 0.2350580096244812\nBatch 1571/2082 - Train Loss: 0.1892530620098114\nBatch 1581/2082 - Train Loss: 0.22791233658790588\nBatch 1591/2082 - Train Loss: 0.190058171749115\nBatch 1601/2082 - Train Loss: 0.19592273235321045\nBatch 1611/2082 - Train Loss: 0.1332041621208191\nBatch 1621/2082 - Train Loss: 0.2400989532470703\nBatch 1631/2082 - Train Loss: 0.15431496500968933\nBatch 1641/2082 - Train Loss: 0.23554891347885132\nBatch 1651/2082 - Train Loss: 0.2390889972448349\nBatch 1661/2082 - Train Loss: 0.063448466360569\nBatch 1671/2082 - Train Loss: 0.2987527847290039\nBatch 1681/2082 - Train Loss: 0.1183275580406189\nBatch 1691/2082 - Train Loss: 0.27683645486831665\nBatch 1701/2082 - Train Loss: 0.3327937424182892\nBatch 1711/2082 - Train Loss: 0.07157886773347855\nBatch 1721/2082 - Train Loss: 0.29604020714759827\nBatch 1731/2082 - Train Loss: 0.20876085758209229\nBatch 1741/2082 - Train Loss: 0.12074976414442062\nBatch 1751/2082 - Train Loss: 0.30762767791748047\nBatch 1761/2082 - Train Loss: 0.07214411348104477\nBatch 1771/2082 - Train Loss: 0.21887116134166718\nBatch 1781/2082 - Train Loss: 0.27810823917388916\nBatch 1791/2082 - Train Loss: 0.17983761429786682\nBatch 1801/2082 - Train Loss: 0.44178029894828796\nBatch 1811/2082 - Train Loss: 0.5187268257141113\nBatch 1821/2082 - Train Loss: 0.15877249836921692\nBatch 1831/2082 - Train Loss: 0.20770929753780365\nBatch 1841/2082 - Train Loss: 0.2779640853404999\nBatch 1851/2082 - Train Loss: 0.21574045717716217\nBatch 1861/2082 - Train Loss: 0.22429202497005463\nBatch 1871/2082 - Train Loss: 0.21921981871128082\nBatch 1881/2082 - Train Loss: 0.2094990611076355\nBatch 1891/2082 - Train Loss: 0.1500813364982605\nBatch 1901/2082 - Train Loss: 0.1382596641778946\nBatch 1911/2082 - Train Loss: 0.19234231114387512\nBatch 1921/2082 - Train Loss: 0.1936541050672531\nBatch 1931/2082 - Train Loss: 0.3669576644897461\nBatch 1941/2082 - Train Loss: 0.10545191168785095\nBatch 1951/2082 - Train Loss: 0.16937614977359772\nBatch 1961/2082 - Train Loss: 0.1916976124048233\nBatch 1971/2082 - Train Loss: 0.2855969965457916\nBatch 1981/2082 - Train Loss: 0.38587990403175354\nBatch 1991/2082 - Train Loss: 0.2308369129896164\nBatch 2001/2082 - Train Loss: 0.16887955367565155\nBatch 2011/2082 - Train Loss: 0.20815333724021912\nBatch 2021/2082 - Train Loss: 0.17917248606681824\nBatch 2031/2082 - Train Loss: 0.1864539235830307\nBatch 2041/2082 - Train Loss: 0.20958255231380463\nBatch 2051/2082 - Train Loss: 0.3083896040916443\nBatch 2061/2082 - Train Loss: 0.21451565623283386\nBatch 2071/2082 - Train Loss: 0.21583934128284454\nBatch 2081/2082 - Train Loss: 0.22610829770565033\nEpoch 6 - Average Train Loss: 0.2249669255447233\nBatch 1/261 - Validation Loss: 0.09833349287509918\nBatch 11/261 - Validation Loss: 0.33335942029953003\nBatch 21/261 - Validation Loss: 0.15087175369262695\nBatch 31/261 - Validation Loss: 0.259747713804245\nBatch 41/261 - Validation Loss: 0.2695809602737427\nBatch 51/261 - Validation Loss: 0.15589432418346405\nBatch 61/261 - Validation Loss: 0.18430155515670776\nBatch 71/261 - Validation Loss: 0.1602935642004013\nBatch 81/261 - Validation Loss: 0.28659743070602417\nBatch 91/261 - Validation Loss: 0.3315427005290985\nBatch 101/261 - Validation Loss: 0.1612025648355484\nBatch 111/261 - Validation Loss: 0.3187089264392853\nBatch 121/261 - Validation Loss: 0.17877452075481415\nBatch 131/261 - Validation Loss: 0.5111151337623596\nBatch 141/261 - Validation Loss: 0.35341864824295044\nBatch 151/261 - Validation Loss: 0.33672553300857544\nBatch 161/261 - Validation Loss: 0.18234361708164215\nBatch 171/261 - Validation Loss: 0.3973104953765869\nBatch 181/261 - Validation Loss: 0.30022525787353516\nBatch 191/261 - Validation Loss: 0.2983994781970978\nBatch 201/261 - Validation Loss: 0.5377715826034546\nBatch 211/261 - Validation Loss: 0.17666950821876526\nBatch 221/261 - Validation Loss: 0.09184486418962479\nBatch 231/261 - Validation Loss: 0.1603373885154724\nBatch 241/261 - Validation Loss: 0.19201326370239258\nBatch 251/261 - Validation Loss: 0.41929808259010315\nBatch 261/261 - Validation Loss: 0.062026455998420715\nEpoch 6 - Average Validation Loss: 0.24315978326902535\nEpoch 7/10\nBatch 1/2082 - Train Loss: 0.2757946252822876\nBatch 11/2082 - Train Loss: 0.22547218203544617\nBatch 21/2082 - Train Loss: 0.4222477674484253\nBatch 31/2082 - Train Loss: 0.18379366397857666\nBatch 41/2082 - Train Loss: 0.23154427111148834\nBatch 51/2082 - Train Loss: 0.3129141330718994\nBatch 61/2082 - Train Loss: 0.2164754718542099\nBatch 71/2082 - Train Loss: 0.46448370814323425\nBatch 81/2082 - Train Loss: 0.3013293445110321\nBatch 91/2082 - Train Loss: 0.14855527877807617\nBatch 101/2082 - Train Loss: 0.22142700850963593\nBatch 111/2082 - Train Loss: 0.27158719301223755\nBatch 121/2082 - Train Loss: 0.22125966846942902\nBatch 131/2082 - Train Loss: 0.1540064513683319\nBatch 141/2082 - Train Loss: 0.205452099442482\nBatch 151/2082 - Train Loss: 0.2895309329032898\nBatch 161/2082 - Train Loss: 0.2460671216249466\nBatch 171/2082 - Train Loss: 0.15594226121902466\nBatch 181/2082 - Train Loss: 0.1265740543603897\nBatch 191/2082 - Train Loss: 0.23388244211673737\nBatch 201/2082 - Train Loss: 0.21904973685741425\nBatch 211/2082 - Train Loss: 0.32893136143684387\nBatch 221/2082 - Train Loss: 0.27909067273139954\nBatch 231/2082 - Train Loss: 0.16414247453212738\nBatch 241/2082 - Train Loss: 0.2246890813112259\nBatch 251/2082 - Train Loss: 0.3217882215976715\nBatch 261/2082 - Train Loss: 0.29727840423583984\nBatch 271/2082 - Train Loss: 0.15776163339614868\nBatch 281/2082 - Train Loss: 0.3135340213775635\nBatch 291/2082 - Train Loss: 0.7185673117637634\nBatch 301/2082 - Train Loss: 0.23243014514446259\nBatch 311/2082 - Train Loss: 0.28487253189086914\nBatch 321/2082 - Train Loss: 0.31774061918258667\nBatch 331/2082 - Train Loss: 0.14776673913002014\nBatch 341/2082 - Train Loss: 0.23942634463310242\nBatch 351/2082 - Train Loss: 0.04974205791950226\nBatch 361/2082 - Train Loss: 0.2881162464618683\nBatch 371/2082 - Train Loss: 0.06789680570363998\nBatch 381/2082 - Train Loss: 0.1886899173259735\nBatch 391/2082 - Train Loss: 0.20844189822673798\nBatch 401/2082 - Train Loss: 0.2001953423023224\nBatch 411/2082 - Train Loss: 0.22981254756450653\nBatch 421/2082 - Train Loss: 0.20684246718883514\nBatch 431/2082 - Train Loss: 0.35564082860946655\nBatch 441/2082 - Train Loss: 0.18147660791873932\nBatch 451/2082 - Train Loss: 0.188649982213974\nBatch 461/2082 - Train Loss: 0.20282897353172302\nBatch 471/2082 - Train Loss: 0.0707574412226677\nBatch 481/2082 - Train Loss: 0.20475377142429352\nBatch 491/2082 - Train Loss: 0.20073539018630981\nBatch 501/2082 - Train Loss: 0.10790528357028961\nBatch 511/2082 - Train Loss: 0.22866062819957733\nBatch 521/2082 - Train Loss: 0.25643202662467957\nBatch 531/2082 - Train Loss: 0.3311648666858673\nBatch 541/2082 - Train Loss: 0.20866122841835022\nBatch 551/2082 - Train Loss: 0.13064299523830414\nBatch 561/2082 - Train Loss: 0.26953527331352234\nBatch 571/2082 - Train Loss: 0.28156670928001404\nBatch 581/2082 - Train Loss: 0.17200398445129395\nBatch 591/2082 - Train Loss: 0.21909815073013306\nBatch 601/2082 - Train Loss: 0.12991221249103546\nBatch 611/2082 - Train Loss: 0.16492542624473572\nBatch 621/2082 - Train Loss: 0.16978256404399872\nBatch 631/2082 - Train Loss: 0.21017226576805115\nBatch 641/2082 - Train Loss: 0.13658316433429718\nBatch 651/2082 - Train Loss: 0.19796150922775269\nBatch 661/2082 - Train Loss: 0.15592454373836517\nBatch 671/2082 - Train Loss: 0.25051188468933105\nBatch 681/2082 - Train Loss: 0.04949314892292023\nBatch 691/2082 - Train Loss: 0.21626363694667816\nBatch 701/2082 - Train Loss: 0.1242520809173584\nBatch 711/2082 - Train Loss: 0.21832972764968872\nBatch 721/2082 - Train Loss: 0.18531498312950134\nBatch 731/2082 - Train Loss: 0.19127434492111206\nBatch 741/2082 - Train Loss: 0.26325324177742004\nBatch 751/2082 - Train Loss: 0.1852060854434967\nBatch 761/2082 - Train Loss: 0.17624229192733765\nBatch 771/2082 - Train Loss: 0.2174982726573944\nBatch 781/2082 - Train Loss: 0.21858488023281097\nBatch 791/2082 - Train Loss: 0.16611485183238983\nBatch 801/2082 - Train Loss: 0.34886038303375244\nBatch 811/2082 - Train Loss: 0.25627976655960083\nBatch 821/2082 - Train Loss: 0.22500304877758026\nBatch 831/2082 - Train Loss: 0.178704172372818\nBatch 841/2082 - Train Loss: 0.1158103421330452\nBatch 851/2082 - Train Loss: 0.14598940312862396\nBatch 861/2082 - Train Loss: 0.20067688822746277\nBatch 871/2082 - Train Loss: 0.21981559693813324\nBatch 881/2082 - Train Loss: 0.15239928662776947\nBatch 891/2082 - Train Loss: 0.14635257422924042\nBatch 901/2082 - Train Loss: 0.25527918338775635\nBatch 911/2082 - Train Loss: 0.347525954246521\nBatch 921/2082 - Train Loss: 0.18014977872371674\nBatch 931/2082 - Train Loss: 0.38609063625335693\nBatch 941/2082 - Train Loss: 0.2672334909439087\nBatch 951/2082 - Train Loss: 0.16610050201416016\nBatch 961/2082 - Train Loss: 0.14994943141937256\nBatch 971/2082 - Train Loss: 0.23406147956848145\nBatch 981/2082 - Train Loss: 0.2414032518863678\nBatch 991/2082 - Train Loss: 0.2687651515007019\nBatch 1001/2082 - Train Loss: 0.17687059938907623\nBatch 1011/2082 - Train Loss: 0.2378511279821396\nBatch 1021/2082 - Train Loss: 0.24877768754959106\nBatch 1031/2082 - Train Loss: 0.39031314849853516\nBatch 1041/2082 - Train Loss: 0.4855579733848572\nBatch 1051/2082 - Train Loss: 0.2630873918533325\nBatch 1061/2082 - Train Loss: 0.12358134984970093\nBatch 1071/2082 - Train Loss: 0.1343710720539093\nBatch 1081/2082 - Train Loss: 0.2475288361310959\nBatch 1091/2082 - Train Loss: 0.19818462431430817\nBatch 1101/2082 - Train Loss: 0.10893168300390244\nBatch 1111/2082 - Train Loss: 0.249710813164711\nBatch 1121/2082 - Train Loss: 0.28300362825393677\nBatch 1131/2082 - Train Loss: 0.06063726544380188\nBatch 1141/2082 - Train Loss: 0.17192712426185608\nBatch 1151/2082 - Train Loss: 0.16480551660060883\nBatch 1161/2082 - Train Loss: 0.1794969141483307\nBatch 1171/2082 - Train Loss: 0.07610931247472763\nBatch 1181/2082 - Train Loss: 0.17043252289295197\nBatch 1191/2082 - Train Loss: 0.1678236573934555\nBatch 1201/2082 - Train Loss: 0.22461912035942078\nBatch 1211/2082 - Train Loss: 0.157692551612854\nBatch 1221/2082 - Train Loss: 0.31164517998695374\nBatch 1231/2082 - Train Loss: 0.29151540994644165\nBatch 1241/2082 - Train Loss: 0.11993635445833206\nBatch 1251/2082 - Train Loss: 0.1203770712018013\nBatch 1261/2082 - Train Loss: 0.19209454953670502\nBatch 1271/2082 - Train Loss: 0.19291135668754578\nBatch 1281/2082 - Train Loss: 0.09712418168783188\nBatch 1291/2082 - Train Loss: 0.3194670081138611\nBatch 1301/2082 - Train Loss: 0.20522762835025787\nBatch 1311/2082 - Train Loss: 0.09936290234327316\nBatch 1321/2082 - Train Loss: 0.16492559015750885\nBatch 1331/2082 - Train Loss: 0.12019823491573334\nBatch 1341/2082 - Train Loss: 0.19477194547653198\nBatch 1351/2082 - Train Loss: 0.2183932214975357\nBatch 1361/2082 - Train Loss: 0.18009932339191437\nBatch 1371/2082 - Train Loss: 0.33083343505859375\nBatch 1381/2082 - Train Loss: 0.14835000038146973\nBatch 1391/2082 - Train Loss: 0.24593192338943481\nBatch 1401/2082 - Train Loss: 0.31535643339157104\nBatch 1411/2082 - Train Loss: 0.21640144288539886\nBatch 1421/2082 - Train Loss: 0.3023606538772583\nBatch 1431/2082 - Train Loss: 0.2677600681781769\nBatch 1441/2082 - Train Loss: 0.13637536764144897\nBatch 1451/2082 - Train Loss: 0.2165651172399521\nBatch 1461/2082 - Train Loss: 0.14412088692188263\nBatch 1471/2082 - Train Loss: 0.19749002158641815\nBatch 1481/2082 - Train Loss: 0.21748878061771393\nBatch 1491/2082 - Train Loss: 0.22842663526535034\nBatch 1501/2082 - Train Loss: 0.2747642993927002\nBatch 1511/2082 - Train Loss: 0.28900623321533203\nBatch 1521/2082 - Train Loss: 0.09455842524766922\nBatch 1531/2082 - Train Loss: 0.2674197554588318\nBatch 1541/2082 - Train Loss: 0.1595550924539566\nBatch 1551/2082 - Train Loss: 0.09445247054100037\nBatch 1561/2082 - Train Loss: 0.1934559941291809\nBatch 1571/2082 - Train Loss: 0.2109297811985016\nBatch 1581/2082 - Train Loss: 0.19346237182617188\nBatch 1591/2082 - Train Loss: 0.29013630747795105\nBatch 1601/2082 - Train Loss: 0.3230433762073517\nBatch 1611/2082 - Train Loss: 0.2785317599773407\nBatch 1621/2082 - Train Loss: 0.29658153653144836\nBatch 1631/2082 - Train Loss: 0.20768892765045166\nBatch 1641/2082 - Train Loss: 0.16834551095962524\nBatch 1651/2082 - Train Loss: 0.3057345747947693\nBatch 1661/2082 - Train Loss: 0.31084534525871277\nBatch 1671/2082 - Train Loss: 0.24199172854423523\nBatch 1681/2082 - Train Loss: 0.2678096890449524\nBatch 1691/2082 - Train Loss: 0.24287554621696472\nBatch 1701/2082 - Train Loss: 0.40397462248802185\nBatch 1711/2082 - Train Loss: 0.11004786193370819\nBatch 1721/2082 - Train Loss: 0.24886023998260498\nBatch 1731/2082 - Train Loss: 0.22353939712047577\nBatch 1741/2082 - Train Loss: 0.14291039109230042\nBatch 1751/2082 - Train Loss: 0.27289149165153503\nBatch 1761/2082 - Train Loss: 0.17250388860702515\nBatch 1771/2082 - Train Loss: 0.23911282420158386\nBatch 1781/2082 - Train Loss: 0.20400817692279816\nBatch 1791/2082 - Train Loss: 0.2524246275424957\nBatch 1801/2082 - Train Loss: 0.13728764653205872\nBatch 1811/2082 - Train Loss: 0.07781234383583069\nBatch 1821/2082 - Train Loss: 0.10296174138784409\nBatch 1831/2082 - Train Loss: 0.1833900511264801\nBatch 1841/2082 - Train Loss: 0.1337304711341858\nBatch 1851/2082 - Train Loss: 0.4023483693599701\nBatch 1861/2082 - Train Loss: 0.13707676529884338\nBatch 1871/2082 - Train Loss: 0.11621876060962677\nBatch 1881/2082 - Train Loss: 0.364052951335907\nBatch 1891/2082 - Train Loss: 0.23027025163173676\nBatch 1901/2082 - Train Loss: 0.34222620725631714\nBatch 1911/2082 - Train Loss: 0.17852969467639923\nBatch 1921/2082 - Train Loss: 0.15974989533424377\nBatch 1931/2082 - Train Loss: 0.2883045971393585\nBatch 1941/2082 - Train Loss: 0.3508286476135254\nBatch 1951/2082 - Train Loss: 0.15578486025333405\nBatch 1961/2082 - Train Loss: 0.40521565079689026\nBatch 1971/2082 - Train Loss: 0.14636096358299255\nBatch 1981/2082 - Train Loss: 0.2847645580768585\nBatch 1991/2082 - Train Loss: 0.34846559166908264\nBatch 2001/2082 - Train Loss: 0.16382326185703278\nBatch 2011/2082 - Train Loss: 0.07815030962228775\nBatch 2021/2082 - Train Loss: 0.18728893995285034\nBatch 2031/2082 - Train Loss: 0.23937393724918365\nBatch 2041/2082 - Train Loss: 0.08456866443157196\nBatch 2051/2082 - Train Loss: 0.4654398560523987\nBatch 2061/2082 - Train Loss: 0.28848502039909363\nBatch 2071/2082 - Train Loss: 0.27316799759864807\nBatch 2081/2082 - Train Loss: 0.0799984261393547\nEpoch 7 - Average Train Loss: 0.21598569288542074\nBatch 1/261 - Validation Loss: 0.10094042867422104\nBatch 11/261 - Validation Loss: 0.3325243890285492\nBatch 21/261 - Validation Loss: 0.14984412491321564\nBatch 31/261 - Validation Loss: 0.2551628351211548\nBatch 41/261 - Validation Loss: 0.2679999768733978\nBatch 51/261 - Validation Loss: 0.15598629415035248\nBatch 61/261 - Validation Loss: 0.18273116648197174\nBatch 71/261 - Validation Loss: 0.15744702517986298\nBatch 81/261 - Validation Loss: 0.2810465097427368\nBatch 91/261 - Validation Loss: 0.3291989266872406\nBatch 101/261 - Validation Loss: 0.16042280197143555\nBatch 111/261 - Validation Loss: 0.3164672553539276\nBatch 121/261 - Validation Loss: 0.17543090879917145\nBatch 131/261 - Validation Loss: 0.5078293681144714\nBatch 141/261 - Validation Loss: 0.35467737913131714\nBatch 151/261 - Validation Loss: 0.3362163007259369\nBatch 161/261 - Validation Loss: 0.1811087727546692\nBatch 171/261 - Validation Loss: 0.3978692889213562\nBatch 181/261 - Validation Loss: 0.2959625720977783\nBatch 191/261 - Validation Loss: 0.3008010983467102\nBatch 201/261 - Validation Loss: 0.5416021347045898\nBatch 211/261 - Validation Loss: 0.17658661305904388\nBatch 221/261 - Validation Loss: 0.09216370433568954\nBatch 231/261 - Validation Loss: 0.1605250984430313\nBatch 241/261 - Validation Loss: 0.1951187402009964\nBatch 251/261 - Validation Loss: 0.415838360786438\nBatch 261/261 - Validation Loss: 0.06988377124071121\nEpoch 7 - Average Validation Loss: 0.2425975027526247\nEpoch 8/10\nBatch 1/2082 - Train Loss: 0.1895800083875656\nBatch 11/2082 - Train Loss: 0.10410405695438385\nBatch 21/2082 - Train Loss: 0.11567536741495132\nBatch 31/2082 - Train Loss: 0.21266275644302368\nBatch 41/2082 - Train Loss: 0.1811692863702774\nBatch 51/2082 - Train Loss: 0.24501743912696838\nBatch 61/2082 - Train Loss: 0.2227400243282318\nBatch 71/2082 - Train Loss: 0.15631887316703796\nBatch 81/2082 - Train Loss: 0.33164122700691223\nBatch 91/2082 - Train Loss: 0.18376538157463074\nBatch 101/2082 - Train Loss: 0.38694870471954346\nBatch 111/2082 - Train Loss: 0.18655383586883545\nBatch 121/2082 - Train Loss: 0.22497083246707916\nBatch 131/2082 - Train Loss: 0.1967836320400238\nBatch 141/2082 - Train Loss: 0.1834748089313507\nBatch 151/2082 - Train Loss: 0.2422296702861786\nBatch 161/2082 - Train Loss: 0.1605675220489502\nBatch 171/2082 - Train Loss: 0.191256582736969\nBatch 181/2082 - Train Loss: 0.28668689727783203\nBatch 191/2082 - Train Loss: 0.17158228158950806\nBatch 201/2082 - Train Loss: 0.13159511983394623\nBatch 211/2082 - Train Loss: 0.22320793569087982\nBatch 221/2082 - Train Loss: 0.21014545857906342\nBatch 231/2082 - Train Loss: 0.28600090742111206\nBatch 241/2082 - Train Loss: 0.37801429629325867\nBatch 251/2082 - Train Loss: 0.16189579665660858\nBatch 261/2082 - Train Loss: 0.14411166310310364\nBatch 271/2082 - Train Loss: 0.4553171396255493\nBatch 281/2082 - Train Loss: 0.17982357740402222\nBatch 291/2082 - Train Loss: 0.20306582748889923\nBatch 301/2082 - Train Loss: 0.13853572309017181\nBatch 311/2082 - Train Loss: 0.21842683851718903\nBatch 321/2082 - Train Loss: 0.2833481431007385\nBatch 331/2082 - Train Loss: 0.4296896457672119\nBatch 341/2082 - Train Loss: 0.3364616930484772\nBatch 351/2082 - Train Loss: 0.11916441470384598\nBatch 361/2082 - Train Loss: 0.14556533098220825\nBatch 371/2082 - Train Loss: 0.3507416248321533\nBatch 381/2082 - Train Loss: 0.18968239426612854\nBatch 391/2082 - Train Loss: 0.2473292052745819\nBatch 401/2082 - Train Loss: 0.12135399878025055\nBatch 411/2082 - Train Loss: 0.31535452604293823\nBatch 421/2082 - Train Loss: 0.17150236666202545\nBatch 431/2082 - Train Loss: 0.2916158437728882\nBatch 441/2082 - Train Loss: 0.2518916726112366\nBatch 451/2082 - Train Loss: 0.14068849384784698\nBatch 461/2082 - Train Loss: 0.27931898832321167\nBatch 471/2082 - Train Loss: 0.09178630262613297\nBatch 481/2082 - Train Loss: 0.2964639365673065\nBatch 491/2082 - Train Loss: 0.09416088461875916\nBatch 501/2082 - Train Loss: 0.24561896920204163\nBatch 511/2082 - Train Loss: 0.17068028450012207\nBatch 521/2082 - Train Loss: 0.10364916920661926\nBatch 531/2082 - Train Loss: 0.25908219814300537\nBatch 541/2082 - Train Loss: 0.17937423288822174\nBatch 551/2082 - Train Loss: 0.22857701778411865\nBatch 561/2082 - Train Loss: 0.1822468489408493\nBatch 571/2082 - Train Loss: 0.1988283097743988\nBatch 581/2082 - Train Loss: 0.21919916570186615\nBatch 591/2082 - Train Loss: 0.13532105088233948\nBatch 601/2082 - Train Loss: 0.1606176495552063\nBatch 611/2082 - Train Loss: 0.2665529251098633\nBatch 621/2082 - Train Loss: 0.4557135999202728\nBatch 631/2082 - Train Loss: 0.15467187762260437\nBatch 641/2082 - Train Loss: 0.15362560749053955\nBatch 651/2082 - Train Loss: 0.10640191286802292\nBatch 661/2082 - Train Loss: 0.14098325371742249\nBatch 671/2082 - Train Loss: 0.11484026908874512\nBatch 681/2082 - Train Loss: 0.24087531864643097\nBatch 691/2082 - Train Loss: 0.3899185061454773\nBatch 701/2082 - Train Loss: 0.20814798772335052\nBatch 711/2082 - Train Loss: 0.12657402455806732\nBatch 721/2082 - Train Loss: 0.11019017547369003\nBatch 731/2082 - Train Loss: 0.21201668679714203\nBatch 741/2082 - Train Loss: 0.1243584156036377\nBatch 751/2082 - Train Loss: 0.19746340811252594\nBatch 761/2082 - Train Loss: 0.20564430952072144\nBatch 771/2082 - Train Loss: 0.18423691391944885\nBatch 781/2082 - Train Loss: 0.17839518189430237\nBatch 791/2082 - Train Loss: 0.26582878828048706\nBatch 801/2082 - Train Loss: 0.2913624346256256\nBatch 811/2082 - Train Loss: 0.2921859622001648\nBatch 821/2082 - Train Loss: 0.18433323502540588\nBatch 831/2082 - Train Loss: 0.22069257497787476\nBatch 841/2082 - Train Loss: 0.16533999145030975\nBatch 851/2082 - Train Loss: 0.3565424680709839\nBatch 861/2082 - Train Loss: 0.14776194095611572\nBatch 871/2082 - Train Loss: 0.055121053010225296\nBatch 881/2082 - Train Loss: 0.14925707876682281\nBatch 891/2082 - Train Loss: 0.13593235611915588\nBatch 901/2082 - Train Loss: 0.1139470562338829\nBatch 911/2082 - Train Loss: 0.07436609268188477\nBatch 921/2082 - Train Loss: 0.24497193098068237\nBatch 931/2082 - Train Loss: 0.15522253513336182\nBatch 941/2082 - Train Loss: 0.2472141534090042\nBatch 951/2082 - Train Loss: 0.19305886328220367\nBatch 961/2082 - Train Loss: 0.17574676871299744\nBatch 971/2082 - Train Loss: 0.11388373374938965\nBatch 981/2082 - Train Loss: 0.27750667929649353\nBatch 991/2082 - Train Loss: 0.25736114382743835\nBatch 1001/2082 - Train Loss: 0.23418451845645905\nBatch 1011/2082 - Train Loss: 0.10194858908653259\nBatch 1021/2082 - Train Loss: 0.20757818222045898\nBatch 1031/2082 - Train Loss: 0.1475769281387329\nBatch 1041/2082 - Train Loss: 0.26586470007896423\nBatch 1051/2082 - Train Loss: 0.3835511803627014\nBatch 1061/2082 - Train Loss: 0.1657329499721527\nBatch 1071/2082 - Train Loss: 0.0793921947479248\nBatch 1081/2082 - Train Loss: 0.16164548695087433\nBatch 1091/2082 - Train Loss: 0.16814306378364563\nBatch 1101/2082 - Train Loss: 0.30411437153816223\nBatch 1111/2082 - Train Loss: 0.1973552405834198\nBatch 1121/2082 - Train Loss: 0.22748889029026031\nBatch 1131/2082 - Train Loss: 0.2648184299468994\nBatch 1141/2082 - Train Loss: 0.12292943149805069\nBatch 1151/2082 - Train Loss: 0.18050019443035126\nBatch 1161/2082 - Train Loss: 0.33404111862182617\nBatch 1171/2082 - Train Loss: 0.23327870666980743\nBatch 1181/2082 - Train Loss: 0.1690320521593094\nBatch 1191/2082 - Train Loss: 0.310202956199646\nBatch 1201/2082 - Train Loss: 0.1793338656425476\nBatch 1211/2082 - Train Loss: 0.18239302933216095\nBatch 1221/2082 - Train Loss: 0.0909215435385704\nBatch 1231/2082 - Train Loss: 0.38254615664482117\nBatch 1241/2082 - Train Loss: 0.27601295709609985\nBatch 1251/2082 - Train Loss: 0.2296522706747055\nBatch 1261/2082 - Train Loss: 0.15324421226978302\nBatch 1271/2082 - Train Loss: 0.2578323185443878\nBatch 1281/2082 - Train Loss: 0.20739972591400146\nBatch 1291/2082 - Train Loss: 0.2679552137851715\nBatch 1301/2082 - Train Loss: 0.2673182785511017\nBatch 1311/2082 - Train Loss: 0.34015384316444397\nBatch 1321/2082 - Train Loss: 0.21011820435523987\nBatch 1331/2082 - Train Loss: 0.28001081943511963\nBatch 1341/2082 - Train Loss: 0.10006596893072128\nBatch 1351/2082 - Train Loss: 0.2489168345928192\nBatch 1361/2082 - Train Loss: 0.14143986999988556\nBatch 1371/2082 - Train Loss: 0.3786497712135315\nBatch 1381/2082 - Train Loss: 0.20976272225379944\nBatch 1391/2082 - Train Loss: 0.1522160917520523\nBatch 1401/2082 - Train Loss: 0.2803719639778137\nBatch 1411/2082 - Train Loss: 0.10446637123823166\nBatch 1421/2082 - Train Loss: 0.4114961326122284\nBatch 1431/2082 - Train Loss: 0.21426719427108765\nBatch 1441/2082 - Train Loss: 0.3362605571746826\nBatch 1451/2082 - Train Loss: 0.10510072857141495\nBatch 1461/2082 - Train Loss: 0.13607850670814514\nBatch 1471/2082 - Train Loss: 0.09521051496267319\nBatch 1481/2082 - Train Loss: 0.09340238571166992\nBatch 1491/2082 - Train Loss: 0.30430054664611816\nBatch 1501/2082 - Train Loss: 0.30015090107917786\nBatch 1511/2082 - Train Loss: 0.1854352355003357\nBatch 1521/2082 - Train Loss: 0.3902374505996704\nBatch 1531/2082 - Train Loss: 0.3474544882774353\nBatch 1541/2082 - Train Loss: 0.06778701394796371\nBatch 1551/2082 - Train Loss: 0.27277472615242004\nBatch 1561/2082 - Train Loss: 0.3205849826335907\nBatch 1571/2082 - Train Loss: 0.3074541985988617\nBatch 1581/2082 - Train Loss: 0.24135653674602509\nBatch 1591/2082 - Train Loss: 0.09634923189878464\nBatch 1601/2082 - Train Loss: 0.17747674882411957\nBatch 1611/2082 - Train Loss: 0.2965426743030548\nBatch 1621/2082 - Train Loss: 0.24096161127090454\nBatch 1631/2082 - Train Loss: 0.25475311279296875\nBatch 1641/2082 - Train Loss: 0.16292594373226166\nBatch 1651/2082 - Train Loss: 0.2470012903213501\nBatch 1661/2082 - Train Loss: 0.23122665286064148\nBatch 1671/2082 - Train Loss: 0.33053627610206604\nBatch 1681/2082 - Train Loss: 0.24731603264808655\nBatch 1691/2082 - Train Loss: 0.35648176074028015\nBatch 1701/2082 - Train Loss: 0.2311284840106964\nBatch 1711/2082 - Train Loss: 0.10254319757223129\nBatch 1721/2082 - Train Loss: 0.21149040758609772\nBatch 1731/2082 - Train Loss: 0.09780001640319824\nBatch 1741/2082 - Train Loss: 0.25776976346969604\nBatch 1751/2082 - Train Loss: 0.19421477615833282\nBatch 1761/2082 - Train Loss: 0.17401359975337982\nBatch 1771/2082 - Train Loss: 0.22175750136375427\nBatch 1781/2082 - Train Loss: 0.11237922310829163\nBatch 1791/2082 - Train Loss: 0.241136372089386\nBatch 1801/2082 - Train Loss: 0.15215253829956055\nBatch 1811/2082 - Train Loss: 0.32037824392318726\nBatch 1821/2082 - Train Loss: 0.18809132277965546\nBatch 1831/2082 - Train Loss: 0.09585520625114441\nBatch 1841/2082 - Train Loss: 0.2744462490081787\nBatch 1851/2082 - Train Loss: 0.3498368263244629\nBatch 1861/2082 - Train Loss: 0.17426416277885437\nBatch 1871/2082 - Train Loss: 0.2314326912164688\nBatch 1881/2082 - Train Loss: 0.40593016147613525\nBatch 1891/2082 - Train Loss: 0.17924968898296356\nBatch 1901/2082 - Train Loss: 0.16605135798454285\nBatch 1911/2082 - Train Loss: 0.3325003981590271\nBatch 1921/2082 - Train Loss: 0.20532231032848358\nBatch 1931/2082 - Train Loss: 0.3392236530780792\nBatch 1941/2082 - Train Loss: 0.21666519343852997\nBatch 1951/2082 - Train Loss: 0.1813027411699295\nBatch 1961/2082 - Train Loss: 0.41439372301101685\nBatch 1971/2082 - Train Loss: 0.14110331237316132\nBatch 1981/2082 - Train Loss: 0.19996890425682068\nBatch 1991/2082 - Train Loss: 0.11908837407827377\nBatch 2001/2082 - Train Loss: 0.1897176057100296\nBatch 2011/2082 - Train Loss: 0.16267703473567963\nBatch 2021/2082 - Train Loss: 0.2981748878955841\nBatch 2031/2082 - Train Loss: 0.17981857061386108\nBatch 2041/2082 - Train Loss: 0.36829525232315063\nBatch 2051/2082 - Train Loss: 0.3197542130947113\nBatch 2061/2082 - Train Loss: 0.2761841118335724\nBatch 2071/2082 - Train Loss: 0.24935179948806763\nBatch 2081/2082 - Train Loss: 0.21592244505882263\nEpoch 8 - Average Train Loss: 0.20724701756874492\nBatch 1/261 - Validation Loss: 0.09838096797466278\nBatch 11/261 - Validation Loss: 0.3291993737220764\nBatch 21/261 - Validation Loss: 0.1465882807970047\nBatch 31/261 - Validation Loss: 0.24922259151935577\nBatch 41/261 - Validation Loss: 0.26642516255378723\nBatch 51/261 - Validation Loss: 0.15439747273921967\nBatch 61/261 - Validation Loss: 0.18740791082382202\nBatch 71/261 - Validation Loss: 0.15905366837978363\nBatch 81/261 - Validation Loss: 0.278532475233078\nBatch 91/261 - Validation Loss: 0.32599613070487976\nBatch 101/261 - Validation Loss: 0.1594632863998413\nBatch 111/261 - Validation Loss: 0.32213306427001953\nBatch 121/261 - Validation Loss: 0.17375001311302185\nBatch 131/261 - Validation Loss: 0.5137763619422913\nBatch 141/261 - Validation Loss: 0.35295426845550537\nBatch 151/261 - Validation Loss: 0.3297368884086609\nBatch 161/261 - Validation Loss: 0.18259239196777344\nBatch 171/261 - Validation Loss: 0.4012105166912079\nBatch 181/261 - Validation Loss: 0.30008018016815186\nBatch 191/261 - Validation Loss: 0.3019542396068573\nBatch 201/261 - Validation Loss: 0.5392135381698608\nBatch 211/261 - Validation Loss: 0.1668194830417633\nBatch 221/261 - Validation Loss: 0.09061775356531143\nBatch 231/261 - Validation Loss: 0.15612225234508514\nBatch 241/261 - Validation Loss: 0.19561919569969177\nBatch 251/261 - Validation Loss: 0.4222498834133148\nBatch 261/261 - Validation Loss: 0.06411702185869217\nEpoch 8 - Average Validation Loss: 0.24245565988380333\nEpoch 9/10\nBatch 1/2082 - Train Loss: 0.1398146003484726\nBatch 11/2082 - Train Loss: 0.0810018926858902\nBatch 21/2082 - Train Loss: 0.26406729221343994\nBatch 31/2082 - Train Loss: 0.228520005941391\nBatch 41/2082 - Train Loss: 0.10796588659286499\nBatch 51/2082 - Train Loss: 0.1771763414144516\nBatch 61/2082 - Train Loss: 0.26823875308036804\nBatch 71/2082 - Train Loss: 0.26859623193740845\nBatch 81/2082 - Train Loss: 0.09293424338102341\nBatch 91/2082 - Train Loss: 0.18528661131858826\nBatch 101/2082 - Train Loss: 0.14475198090076447\nBatch 111/2082 - Train Loss: 0.09239418804645538\nBatch 121/2082 - Train Loss: 0.16000962257385254\nBatch 131/2082 - Train Loss: 0.18403001129627228\nBatch 141/2082 - Train Loss: 0.1582619994878769\nBatch 151/2082 - Train Loss: 0.1523517221212387\nBatch 161/2082 - Train Loss: 0.11083026975393295\nBatch 171/2082 - Train Loss: 0.1263483166694641\nBatch 181/2082 - Train Loss: 0.1572272628545761\nBatch 191/2082 - Train Loss: 0.21636052429676056\nBatch 201/2082 - Train Loss: 0.1551813781261444\nBatch 211/2082 - Train Loss: 0.1404198408126831\nBatch 221/2082 - Train Loss: 0.1928752064704895\nBatch 231/2082 - Train Loss: 0.2467183619737625\nBatch 241/2082 - Train Loss: 0.18792898952960968\nBatch 251/2082 - Train Loss: 0.17615115642547607\nBatch 261/2082 - Train Loss: 0.35107794404029846\nBatch 271/2082 - Train Loss: 0.3366837799549103\nBatch 281/2082 - Train Loss: 0.4772866368293762\nBatch 291/2082 - Train Loss: 0.3461333215236664\nBatch 301/2082 - Train Loss: 0.2771303057670593\nBatch 311/2082 - Train Loss: 0.20046366751194\nBatch 321/2082 - Train Loss: 0.1332380473613739\nBatch 331/2082 - Train Loss: 0.20500943064689636\nBatch 341/2082 - Train Loss: 0.19873397052288055\nBatch 351/2082 - Train Loss: 0.20113761723041534\nBatch 361/2082 - Train Loss: 0.21135450899600983\nBatch 371/2082 - Train Loss: 0.1938081532716751\nBatch 381/2082 - Train Loss: 0.1854548156261444\nBatch 391/2082 - Train Loss: 0.14279548823833466\nBatch 401/2082 - Train Loss: 0.21474313735961914\nBatch 411/2082 - Train Loss: 0.1124153658747673\nBatch 421/2082 - Train Loss: 0.12962765991687775\nBatch 431/2082 - Train Loss: 0.09969659894704819\nBatch 441/2082 - Train Loss: 0.2043975591659546\nBatch 451/2082 - Train Loss: 0.11410511285066605\nBatch 461/2082 - Train Loss: 0.18665622174739838\nBatch 471/2082 - Train Loss: 0.424159973859787\nBatch 481/2082 - Train Loss: 0.21738660335540771\nBatch 491/2082 - Train Loss: 0.15720626711845398\nBatch 501/2082 - Train Loss: 0.2755202651023865\nBatch 511/2082 - Train Loss: 0.129819855093956\nBatch 521/2082 - Train Loss: 0.3679063022136688\nBatch 531/2082 - Train Loss: 0.14028964936733246\nBatch 541/2082 - Train Loss: 0.058978959918022156\nBatch 551/2082 - Train Loss: 0.1580081582069397\nBatch 561/2082 - Train Loss: 0.1765161156654358\nBatch 571/2082 - Train Loss: 0.15140952169895172\nBatch 581/2082 - Train Loss: 0.21442413330078125\nBatch 591/2082 - Train Loss: 0.3661988079547882\nBatch 601/2082 - Train Loss: 0.22467494010925293\nBatch 611/2082 - Train Loss: 0.18111711740493774\nBatch 621/2082 - Train Loss: 0.12615421414375305\nBatch 631/2082 - Train Loss: 0.1358916461467743\nBatch 641/2082 - Train Loss: 0.20628981292247772\nBatch 651/2082 - Train Loss: 0.1372445970773697\nBatch 661/2082 - Train Loss: 0.1958015114068985\nBatch 671/2082 - Train Loss: 0.24632634222507477\nBatch 681/2082 - Train Loss: 0.25071030855178833\nBatch 691/2082 - Train Loss: 0.238507941365242\nBatch 701/2082 - Train Loss: 0.3274424374103546\nBatch 711/2082 - Train Loss: 0.20853400230407715\nBatch 721/2082 - Train Loss: 0.1377137303352356\nBatch 731/2082 - Train Loss: 0.25886526703834534\nBatch 741/2082 - Train Loss: 0.22629348933696747\nBatch 751/2082 - Train Loss: 0.049629293382167816\nBatch 761/2082 - Train Loss: 0.19022934138774872\nBatch 771/2082 - Train Loss: 0.33915475010871887\nBatch 781/2082 - Train Loss: 0.23717699944972992\nBatch 791/2082 - Train Loss: 0.16515325009822845\nBatch 801/2082 - Train Loss: 0.17935772240161896\nBatch 811/2082 - Train Loss: 0.3892204165458679\nBatch 821/2082 - Train Loss: 0.20279991626739502\nBatch 831/2082 - Train Loss: 0.18628127872943878\nBatch 841/2082 - Train Loss: 0.19707582890987396\nBatch 851/2082 - Train Loss: 0.1405039280653\nBatch 861/2082 - Train Loss: 0.20850218832492828\nBatch 871/2082 - Train Loss: 0.042567845433950424\nBatch 881/2082 - Train Loss: 0.11930901557207108\nBatch 891/2082 - Train Loss: 0.24432185292243958\nBatch 901/2082 - Train Loss: 0.2103717178106308\nBatch 911/2082 - Train Loss: 0.18443016707897186\nBatch 921/2082 - Train Loss: 0.16597923636436462\nBatch 931/2082 - Train Loss: 0.24794773757457733\nBatch 941/2082 - Train Loss: 0.28435656428337097\nBatch 951/2082 - Train Loss: 0.12700307369232178\nBatch 961/2082 - Train Loss: 0.24669377505779266\nBatch 971/2082 - Train Loss: 0.1421702355146408\nBatch 981/2082 - Train Loss: 0.0724070742726326\nBatch 991/2082 - Train Loss: 0.25009405612945557\nBatch 1001/2082 - Train Loss: 0.14180612564086914\nBatch 1011/2082 - Train Loss: 0.4085873067378998\nBatch 1021/2082 - Train Loss: 0.25772181153297424\nBatch 1031/2082 - Train Loss: 0.12508994340896606\nBatch 1041/2082 - Train Loss: 0.25014203786849976\nBatch 1051/2082 - Train Loss: 0.22190630435943604\nBatch 1061/2082 - Train Loss: 0.26436108350753784\nBatch 1071/2082 - Train Loss: 0.08981364220380783\nBatch 1081/2082 - Train Loss: 0.126455619931221\nBatch 1091/2082 - Train Loss: 0.18089595437049866\nBatch 1101/2082 - Train Loss: 0.11886295676231384\nBatch 1111/2082 - Train Loss: 0.18921132385730743\nBatch 1121/2082 - Train Loss: 0.19140543043613434\nBatch 1131/2082 - Train Loss: 0.2137538641691208\nBatch 1141/2082 - Train Loss: 0.20224176347255707\nBatch 1151/2082 - Train Loss: 0.07819877564907074\nBatch 1161/2082 - Train Loss: 0.1430152803659439\nBatch 1171/2082 - Train Loss: 0.2053867131471634\nBatch 1181/2082 - Train Loss: 0.20908668637275696\nBatch 1191/2082 - Train Loss: 0.2796442210674286\nBatch 1201/2082 - Train Loss: 0.2502305507659912\nBatch 1211/2082 - Train Loss: 0.17247487604618073\nBatch 1221/2082 - Train Loss: 0.07230791449546814\nBatch 1231/2082 - Train Loss: 0.14658968150615692\nBatch 1241/2082 - Train Loss: 0.0815332755446434\nBatch 1251/2082 - Train Loss: 0.17630833387374878\nBatch 1261/2082 - Train Loss: 0.23775413632392883\nBatch 1271/2082 - Train Loss: 0.19262142479419708\nBatch 1281/2082 - Train Loss: 0.15398229658603668\nBatch 1291/2082 - Train Loss: 0.23830370604991913\nBatch 1301/2082 - Train Loss: 0.1843358427286148\nBatch 1311/2082 - Train Loss: 0.137316033244133\nBatch 1321/2082 - Train Loss: 0.17615151405334473\nBatch 1331/2082 - Train Loss: 0.2723709046840668\nBatch 1341/2082 - Train Loss: 0.155954971909523\nBatch 1351/2082 - Train Loss: 0.09883939474821091\nBatch 1361/2082 - Train Loss: 0.21999560296535492\nBatch 1371/2082 - Train Loss: 0.1715514063835144\nBatch 1381/2082 - Train Loss: 0.19314558804035187\nBatch 1391/2082 - Train Loss: 0.1692766398191452\nBatch 1401/2082 - Train Loss: 0.23254789412021637\nBatch 1411/2082 - Train Loss: 0.24041934311389923\nBatch 1421/2082 - Train Loss: 0.2468002885580063\nBatch 1431/2082 - Train Loss: 0.35474255681037903\nBatch 1441/2082 - Train Loss: 0.15223011374473572\nBatch 1451/2082 - Train Loss: 0.16585588455200195\nBatch 1461/2082 - Train Loss: 0.20831987261772156\nBatch 1471/2082 - Train Loss: 0.0976446121931076\nBatch 1481/2082 - Train Loss: 0.0639677345752716\nBatch 1491/2082 - Train Loss: 0.11809790134429932\nBatch 1501/2082 - Train Loss: 0.2946700155735016\nBatch 1511/2082 - Train Loss: 0.20122972130775452\nBatch 1521/2082 - Train Loss: 0.09684209525585175\nBatch 1531/2082 - Train Loss: 0.3035818040370941\nBatch 1541/2082 - Train Loss: 0.21420623362064362\nBatch 1551/2082 - Train Loss: 0.13826407492160797\nBatch 1561/2082 - Train Loss: 0.10110176354646683\nBatch 1571/2082 - Train Loss: 0.17167086899280548\nBatch 1581/2082 - Train Loss: 0.2649088203907013\nBatch 1591/2082 - Train Loss: 0.1416170597076416\nBatch 1601/2082 - Train Loss: 0.06433767825365067\nBatch 1611/2082 - Train Loss: 0.1810951977968216\nBatch 1621/2082 - Train Loss: 0.2074175477027893\nBatch 1631/2082 - Train Loss: 0.10293170809745789\nBatch 1641/2082 - Train Loss: 0.2459995001554489\nBatch 1651/2082 - Train Loss: 0.13602696359157562\nBatch 1661/2082 - Train Loss: 0.3205004930496216\nBatch 1671/2082 - Train Loss: 0.20762723684310913\nBatch 1681/2082 - Train Loss: 0.19987685978412628\nBatch 1691/2082 - Train Loss: 0.10456734895706177\nBatch 1701/2082 - Train Loss: 0.16546529531478882\nBatch 1711/2082 - Train Loss: 0.1368340104818344\nBatch 1721/2082 - Train Loss: 0.23420575261116028\nBatch 1731/2082 - Train Loss: 0.17646144330501556\nBatch 1741/2082 - Train Loss: 0.3013296127319336\nBatch 1751/2082 - Train Loss: 0.24893851578235626\nBatch 1761/2082 - Train Loss: 0.136458620429039\nBatch 1771/2082 - Train Loss: 0.19361643493175507\nBatch 1781/2082 - Train Loss: 0.3389495611190796\nBatch 1791/2082 - Train Loss: 0.3132987916469574\nBatch 1801/2082 - Train Loss: 0.2227662354707718\nBatch 1811/2082 - Train Loss: 0.488906592130661\nBatch 1821/2082 - Train Loss: 0.3176725208759308\nBatch 1831/2082 - Train Loss: 0.12262272834777832\nBatch 1841/2082 - Train Loss: 0.3385566473007202\nBatch 1851/2082 - Train Loss: 0.07757015526294708\nBatch 1861/2082 - Train Loss: 0.21262317895889282\nBatch 1871/2082 - Train Loss: 0.17707106471061707\nBatch 1881/2082 - Train Loss: 0.16602973639965057\nBatch 1891/2082 - Train Loss: 0.11196519434452057\nBatch 1901/2082 - Train Loss: 0.16752001643180847\nBatch 1911/2082 - Train Loss: 0.1570485681295395\nBatch 1921/2082 - Train Loss: 0.18904365599155426\nBatch 1931/2082 - Train Loss: 0.26021677255630493\nBatch 1941/2082 - Train Loss: 0.2948424816131592\nBatch 1951/2082 - Train Loss: 0.16422735154628754\nBatch 1961/2082 - Train Loss: 0.09844999760389328\nBatch 1971/2082 - Train Loss: 0.15645647048950195\nBatch 1981/2082 - Train Loss: 0.1926150619983673\nBatch 1991/2082 - Train Loss: 0.14623846113681793\nBatch 2001/2082 - Train Loss: 0.0474948026239872\nBatch 2011/2082 - Train Loss: 0.14632323384284973\nBatch 2021/2082 - Train Loss: 0.12648385763168335\nBatch 2031/2082 - Train Loss: 0.2284034788608551\nBatch 2041/2082 - Train Loss: 0.16973422467708588\nBatch 2051/2082 - Train Loss: 0.06492588669061661\nBatch 2061/2082 - Train Loss: 0.2569010853767395\nBatch 2071/2082 - Train Loss: 0.15787656605243683\nBatch 2081/2082 - Train Loss: 0.10152816027402878\nEpoch 9 - Average Train Loss: 0.1987684524236077\nBatch 1/261 - Validation Loss: 0.09952061623334885\nBatch 11/261 - Validation Loss: 0.33361485600471497\nBatch 21/261 - Validation Loss: 0.15266752243041992\nBatch 31/261 - Validation Loss: 0.24778498709201813\nBatch 41/261 - Validation Loss: 0.2688102722167969\nBatch 51/261 - Validation Loss: 0.15889540314674377\nBatch 61/261 - Validation Loss: 0.18832731246948242\nBatch 71/261 - Validation Loss: 0.16161251068115234\nBatch 81/261 - Validation Loss: 0.28147122263908386\nBatch 91/261 - Validation Loss: 0.32822030782699585\nBatch 101/261 - Validation Loss: 0.16044507920742035\nBatch 111/261 - Validation Loss: 0.3257409334182739\nBatch 121/261 - Validation Loss: 0.17419444024562836\nBatch 131/261 - Validation Loss: 0.5207942724227905\nBatch 141/261 - Validation Loss: 0.35797664523124695\nBatch 151/261 - Validation Loss: 0.336412638425827\nBatch 161/261 - Validation Loss: 0.18140386044979095\nBatch 171/261 - Validation Loss: 0.40163692831993103\nBatch 181/261 - Validation Loss: 0.3043231666088104\nBatch 191/261 - Validation Loss: 0.3024342358112335\nBatch 201/261 - Validation Loss: 0.5491304397583008\nBatch 211/261 - Validation Loss: 0.16877461969852448\nBatch 221/261 - Validation Loss: 0.09029654413461685\nBatch 231/261 - Validation Loss: 0.15951916575431824\nBatch 241/261 - Validation Loss: 0.19952744245529175\nBatch 251/261 - Validation Loss: 0.4328969717025757\nBatch 261/261 - Validation Loss: 0.06301136314868927\nEpoch 9 - Average Validation Loss: 0.2444550607253537\nEpoch 10/10\nBatch 1/2082 - Train Loss: 0.14299969375133514\nBatch 11/2082 - Train Loss: 0.11676894128322601\nBatch 21/2082 - Train Loss: 0.21847271919250488\nBatch 31/2082 - Train Loss: 0.2852802276611328\nBatch 41/2082 - Train Loss: 0.2336582988500595\nBatch 51/2082 - Train Loss: 0.1265595704317093\nBatch 61/2082 - Train Loss: 0.27853646874427795\nBatch 71/2082 - Train Loss: 0.1595762073993683\nBatch 81/2082 - Train Loss: 0.21545396745204926\nBatch 91/2082 - Train Loss: 0.07669186592102051\nBatch 101/2082 - Train Loss: 0.2913722097873688\nBatch 111/2082 - Train Loss: 0.15521080791950226\nBatch 121/2082 - Train Loss: 0.2758941352367401\nBatch 131/2082 - Train Loss: 0.14271186292171478\nBatch 141/2082 - Train Loss: 0.09011191129684448\nBatch 151/2082 - Train Loss: 0.27117598056793213\nBatch 161/2082 - Train Loss: 0.16631865501403809\nBatch 171/2082 - Train Loss: 0.05704610049724579\nBatch 181/2082 - Train Loss: 0.16026778519153595\nBatch 191/2082 - Train Loss: 0.22808560729026794\nBatch 201/2082 - Train Loss: 0.24903897941112518\nBatch 211/2082 - Train Loss: 0.11508309841156006\nBatch 221/2082 - Train Loss: 0.2055915743112564\nBatch 231/2082 - Train Loss: 0.09935031831264496\nBatch 241/2082 - Train Loss: 0.12093298882246017\nBatch 251/2082 - Train Loss: 0.1821896731853485\nBatch 261/2082 - Train Loss: 0.06065867841243744\nBatch 271/2082 - Train Loss: 0.1040063127875328\nBatch 281/2082 - Train Loss: 0.2117708921432495\nBatch 291/2082 - Train Loss: 0.4216727912425995\nBatch 301/2082 - Train Loss: 0.14138776063919067\nBatch 311/2082 - Train Loss: 0.07275297492742538\nBatch 321/2082 - Train Loss: 0.22491782903671265\nBatch 331/2082 - Train Loss: 0.19179023802280426\nBatch 341/2082 - Train Loss: 0.29738253355026245\nBatch 351/2082 - Train Loss: 0.19746136665344238\nBatch 361/2082 - Train Loss: 0.20514929294586182\nBatch 371/2082 - Train Loss: 0.10062553733587265\nBatch 381/2082 - Train Loss: 0.1464386284351349\nBatch 391/2082 - Train Loss: 0.16063448786735535\nBatch 401/2082 - Train Loss: 0.13225236535072327\nBatch 411/2082 - Train Loss: 0.14810751378536224\nBatch 421/2082 - Train Loss: 0.061226215213537216\nBatch 431/2082 - Train Loss: 0.19099248945713043\nBatch 441/2082 - Train Loss: 0.29343706369400024\nBatch 451/2082 - Train Loss: 0.12600451707839966\nBatch 461/2082 - Train Loss: 0.1356792002916336\nBatch 471/2082 - Train Loss: 0.30223873257637024\nBatch 481/2082 - Train Loss: 0.23981140553951263\nBatch 491/2082 - Train Loss: 0.049696583300828934\nBatch 501/2082 - Train Loss: 0.2042602300643921\nBatch 511/2082 - Train Loss: 0.19546443223953247\nBatch 521/2082 - Train Loss: 0.3112943172454834\nBatch 531/2082 - Train Loss: 0.12241838872432709\nBatch 541/2082 - Train Loss: 0.19929544627666473\nBatch 551/2082 - Train Loss: 0.19481389224529266\nBatch 561/2082 - Train Loss: 0.32213637232780457\nBatch 571/2082 - Train Loss: 0.096534363925457\nBatch 581/2082 - Train Loss: 0.2801032066345215\nBatch 591/2082 - Train Loss: 0.1669265180826187\nBatch 601/2082 - Train Loss: 0.15805882215499878\nBatch 611/2082 - Train Loss: 0.2250879406929016\nBatch 621/2082 - Train Loss: 0.165944442152977\nBatch 631/2082 - Train Loss: 0.20177291333675385\nBatch 641/2082 - Train Loss: 0.08272269368171692\nBatch 651/2082 - Train Loss: 0.23387207090854645\nBatch 661/2082 - Train Loss: 0.18989159166812897\nBatch 671/2082 - Train Loss: 0.23390914499759674\nBatch 681/2082 - Train Loss: 0.272879958152771\nBatch 691/2082 - Train Loss: 0.20182281732559204\nBatch 701/2082 - Train Loss: 0.22969350218772888\nBatch 711/2082 - Train Loss: 0.10260221362113953\nBatch 721/2082 - Train Loss: 0.1459650844335556\nBatch 731/2082 - Train Loss: 0.2779785096645355\nBatch 741/2082 - Train Loss: 0.26417720317840576\nBatch 751/2082 - Train Loss: 0.29533740878105164\nBatch 761/2082 - Train Loss: 0.19878019392490387\nBatch 771/2082 - Train Loss: 0.2695988714694977\nBatch 781/2082 - Train Loss: 0.1553032249212265\nBatch 791/2082 - Train Loss: 0.19069059193134308\nBatch 801/2082 - Train Loss: 0.1522587388753891\nBatch 811/2082 - Train Loss: 0.16560843586921692\nBatch 821/2082 - Train Loss: 0.21079786121845245\nBatch 831/2082 - Train Loss: 0.2521358132362366\nBatch 841/2082 - Train Loss: 0.17924493551254272\nBatch 851/2082 - Train Loss: 0.4015137851238251\nBatch 861/2082 - Train Loss: 0.0814189538359642\nBatch 871/2082 - Train Loss: 0.25887659192085266\nBatch 881/2082 - Train Loss: 0.17109528183937073\nBatch 891/2082 - Train Loss: 0.15778547525405884\nBatch 901/2082 - Train Loss: 0.08152547478675842\nBatch 911/2082 - Train Loss: 0.2125868797302246\nBatch 921/2082 - Train Loss: 0.21717669069766998\nBatch 931/2082 - Train Loss: 0.3733048439025879\nBatch 941/2082 - Train Loss: 0.09326306730508804\nBatch 951/2082 - Train Loss: 0.12962254881858826\nBatch 961/2082 - Train Loss: 0.1471712589263916\nBatch 971/2082 - Train Loss: 0.11372598260641098\nBatch 981/2082 - Train Loss: 0.20208270847797394\nBatch 991/2082 - Train Loss: 0.30042362213134766\nBatch 1001/2082 - Train Loss: 0.22564564645290375\nBatch 1011/2082 - Train Loss: 0.18322502076625824\nBatch 1021/2082 - Train Loss: 0.18644310534000397\nBatch 1031/2082 - Train Loss: 0.15088880062103271\nBatch 1041/2082 - Train Loss: 0.12128515541553497\nBatch 1051/2082 - Train Loss: 0.11471987515687943\nBatch 1061/2082 - Train Loss: 0.15722453594207764\nBatch 1071/2082 - Train Loss: 0.08761882036924362\nBatch 1081/2082 - Train Loss: 0.16614995896816254\nBatch 1091/2082 - Train Loss: 0.20545773208141327\nBatch 1101/2082 - Train Loss: 0.12811674177646637\nBatch 1111/2082 - Train Loss: 0.0930122584104538\nBatch 1121/2082 - Train Loss: 0.17107915878295898\nBatch 1131/2082 - Train Loss: 0.08706025034189224\nBatch 1141/2082 - Train Loss: 0.13531409204006195\nBatch 1151/2082 - Train Loss: 0.14897599816322327\nBatch 1161/2082 - Train Loss: 0.16853351891040802\nBatch 1171/2082 - Train Loss: 0.19326740503311157\nBatch 1181/2082 - Train Loss: 0.2178455889225006\nBatch 1191/2082 - Train Loss: 0.339580774307251\nBatch 1201/2082 - Train Loss: 0.166426882147789\nBatch 1211/2082 - Train Loss: 0.3128747045993805\nBatch 1221/2082 - Train Loss: 0.27058592438697815\nBatch 1231/2082 - Train Loss: 0.14591941237449646\nBatch 1241/2082 - Train Loss: 0.11296162754297256\nBatch 1251/2082 - Train Loss: 0.11867496371269226\nBatch 1261/2082 - Train Loss: 0.15629130601882935\nBatch 1271/2082 - Train Loss: 0.39784058928489685\nBatch 1281/2082 - Train Loss: 0.15820540487766266\nBatch 1291/2082 - Train Loss: 0.11119995266199112\nBatch 1301/2082 - Train Loss: 0.2380821257829666\nBatch 1311/2082 - Train Loss: 0.10654422640800476\nBatch 1321/2082 - Train Loss: 0.24335093796253204\nBatch 1331/2082 - Train Loss: 0.17327100038528442\nBatch 1341/2082 - Train Loss: 0.16713707149028778\nBatch 1351/2082 - Train Loss: 0.27918100357055664\nBatch 1361/2082 - Train Loss: 0.11568071693181992\nBatch 1371/2082 - Train Loss: 0.18632331490516663\nBatch 1381/2082 - Train Loss: 0.26745566725730896\nBatch 1391/2082 - Train Loss: 0.24611520767211914\nBatch 1401/2082 - Train Loss: 0.2632627487182617\nBatch 1411/2082 - Train Loss: 0.14979910850524902\nBatch 1421/2082 - Train Loss: 0.26682716608047485\nBatch 1431/2082 - Train Loss: 0.16905373334884644\nBatch 1441/2082 - Train Loss: 0.16494600474834442\nBatch 1451/2082 - Train Loss: 0.12756167352199554\nBatch 1461/2082 - Train Loss: 0.22084391117095947\nBatch 1471/2082 - Train Loss: 0.1952359676361084\nBatch 1481/2082 - Train Loss: 0.34499821066856384\nBatch 1491/2082 - Train Loss: 0.1755264848470688\nBatch 1501/2082 - Train Loss: 0.11658414453268051\nBatch 1511/2082 - Train Loss: 0.1127239242196083\nBatch 1521/2082 - Train Loss: 0.31762248277664185\nBatch 1531/2082 - Train Loss: 0.20031297206878662\nBatch 1541/2082 - Train Loss: 0.27684393525123596\nBatch 1551/2082 - Train Loss: 0.2065752148628235\nBatch 1561/2082 - Train Loss: 0.14314056932926178\nBatch 1571/2082 - Train Loss: 0.3683899939060211\nBatch 1581/2082 - Train Loss: 0.18406184017658234\nBatch 1591/2082 - Train Loss: 0.1836826354265213\nBatch 1601/2082 - Train Loss: 0.20677483081817627\nBatch 1611/2082 - Train Loss: 0.3070172965526581\nBatch 1621/2082 - Train Loss: 0.15163147449493408\nBatch 1631/2082 - Train Loss: 0.2850077450275421\nBatch 1641/2082 - Train Loss: 0.21016837656497955\nBatch 1651/2082 - Train Loss: 0.21423083543777466\nBatch 1661/2082 - Train Loss: 0.14346471428871155\nBatch 1671/2082 - Train Loss: 0.1796976923942566\nBatch 1681/2082 - Train Loss: 0.1994355469942093\nBatch 1691/2082 - Train Loss: 0.10936431586742401\nBatch 1701/2082 - Train Loss: 0.15558679401874542\nBatch 1711/2082 - Train Loss: 0.20701323449611664\nBatch 1721/2082 - Train Loss: 0.11021499335765839\nBatch 1731/2082 - Train Loss: 0.14323996007442474\nBatch 1741/2082 - Train Loss: 0.12576018273830414\nBatch 1751/2082 - Train Loss: 0.2551198899745941\nBatch 1761/2082 - Train Loss: 0.2625124454498291\nBatch 1771/2082 - Train Loss: 0.344165563583374\nBatch 1781/2082 - Train Loss: 0.20261742174625397\nBatch 1791/2082 - Train Loss: 0.20661896467208862\nBatch 1801/2082 - Train Loss: 0.2069852650165558\nBatch 1811/2082 - Train Loss: 0.20446883141994476\nBatch 1821/2082 - Train Loss: 0.2214348167181015\nBatch 1831/2082 - Train Loss: 0.16322140395641327\nBatch 1841/2082 - Train Loss: 0.09334507584571838\nBatch 1851/2082 - Train Loss: 0.16643399000167847\nBatch 1861/2082 - Train Loss: 0.17064745724201202\nBatch 1871/2082 - Train Loss: 0.15266339480876923\nBatch 1881/2082 - Train Loss: 0.1891864836215973\nBatch 1891/2082 - Train Loss: 0.23455901443958282\nBatch 1901/2082 - Train Loss: 0.2217099517583847\nBatch 1911/2082 - Train Loss: 0.14044944941997528\nBatch 1921/2082 - Train Loss: 0.22092027962207794\nBatch 1931/2082 - Train Loss: 0.1784546673297882\nBatch 1941/2082 - Train Loss: 0.15914765000343323\nBatch 1951/2082 - Train Loss: 0.059102676808834076\nBatch 1961/2082 - Train Loss: 0.16345448791980743\nBatch 1971/2082 - Train Loss: 0.11155218631029129\nBatch 1981/2082 - Train Loss: 0.2059294730424881\nBatch 1991/2082 - Train Loss: 0.2792760729789734\nBatch 2001/2082 - Train Loss: 0.20298202335834503\nBatch 2011/2082 - Train Loss: 0.2317051738500595\nBatch 2021/2082 - Train Loss: 0.2660429775714874\nBatch 2031/2082 - Train Loss: 0.1798335760831833\nBatch 2041/2082 - Train Loss: 0.151765838265419\nBatch 2051/2082 - Train Loss: 0.2383510023355484\nBatch 2061/2082 - Train Loss: 0.19565866887569427\nBatch 2071/2082 - Train Loss: 0.14319537580013275\nBatch 2081/2082 - Train Loss: 0.2084885686635971\nEpoch 10 - Average Train Loss: 0.19163932122905647\nBatch 1/261 - Validation Loss: 0.10207593441009521\nBatch 11/261 - Validation Loss: 0.3351094126701355\nBatch 21/261 - Validation Loss: 0.1512141227722168\nBatch 31/261 - Validation Loss: 0.2467489093542099\nBatch 41/261 - Validation Loss: 0.2693880498409271\nBatch 51/261 - Validation Loss: 0.15458810329437256\nBatch 61/261 - Validation Loss: 0.18793755769729614\nBatch 71/261 - Validation Loss: 0.1629238873720169\nBatch 81/261 - Validation Loss: 0.2802143096923828\nBatch 91/261 - Validation Loss: 0.32970720529556274\nBatch 101/261 - Validation Loss: 0.165199413895607\nBatch 111/261 - Validation Loss: 0.3270777761936188\nBatch 121/261 - Validation Loss: 0.1720326989889145\nBatch 131/261 - Validation Loss: 0.5289363861083984\nBatch 141/261 - Validation Loss: 0.36089056730270386\nBatch 151/261 - Validation Loss: 0.34230440855026245\nBatch 161/261 - Validation Loss: 0.1799769252538681\nBatch 171/261 - Validation Loss: 0.40126219391822815\nBatch 181/261 - Validation Loss: 0.30800923705101013\nBatch 191/261 - Validation Loss: 0.3076615631580353\nBatch 201/261 - Validation Loss: 0.5425576567649841\nBatch 211/261 - Validation Loss: 0.16547860205173492\nBatch 221/261 - Validation Loss: 0.09105298668146133\nBatch 231/261 - Validation Loss: 0.1623396873474121\nBatch 241/261 - Validation Loss: 0.20396114885807037\nBatch 251/261 - Validation Loss: 0.43091171979904175\nBatch 261/261 - Validation Loss: 0.056163884699344635\nEpoch 10 - Average Validation Loss: 0.24614020073037038\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/kaggle/working/clean_indot5_model_lower\"\n",
        "\n",
        "# Simpan model dan tokenizer\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T13:02:27.268256Z",
          "iopub.execute_input": "2024-12-16T13:02:27.268574Z",
          "iopub.status.idle": "2024-12-16T13:02:29.344172Z",
          "shell.execute_reply.started": "2024-12-16T13:02:27.268533Z",
          "shell.execute_reply": "2024-12-16T13:02:29.343285Z"
        },
        "id": "W1reX22KjYon",
        "outputId": "d21c4f2c-8d10-4f85-c468-a39ece19d2a3"
      },
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "('/kaggle/working/clean_indot5_model_lower/tokenizer_config.json',\n '/kaggle/working/clean_indot5_model_lower/special_tokens_map.json',\n '/kaggle/working/clean_indot5_model_lower/spiece.model',\n '/kaggle/working/clean_indot5_model_lower/added_tokens.json')"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r clean_indot5_model_non_lower.zip /kaggle/working/clean_indot5_model_lower"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T13:02:29.345189Z",
          "iopub.execute_input": "2024-12-16T13:02:29.345473Z",
          "iopub.status.idle": "2024-12-16T13:03:18.240583Z",
          "shell.execute_reply.started": "2024-12-16T13:02:29.345448Z",
          "shell.execute_reply": "2024-12-16T13:03:18.239722Z"
        },
        "id": "WrGO5gsHjYon",
        "outputId": "abe67ec6-1b77-40fc-d640-3e339e2feba9"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "  adding: kaggle/working/clean_indot5_model_lower/ (stored 0%)\n  adding: kaggle/working/clean_indot5_model_lower/added_tokens.json (deflated 83%)\n  adding: kaggle/working/clean_indot5_model_lower/model.safetensors (deflated 7%)\n  adding: kaggle/working/clean_indot5_model_lower/config.json (deflated 48%)\n  adding: kaggle/working/clean_indot5_model_lower/special_tokens_map.json (deflated 85%)\n  adding: kaggle/working/clean_indot5_model_lower/tokenizer_config.json (deflated 94%)\n  adding: kaggle/working/clean_indot5_model_lower/generation_config.json (deflated 29%)\n  adding: kaggle/working/clean_indot5_model_lower/spiece.model (deflated 49%)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, num_epochs + 1), train_losses, label=\"Training Loss\", color='b', marker='o')\n",
        "plt.plot(range(1, num_epochs + 1), val_losses, label=\"Validation Loss\", color='r', marker='o')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T13:03:18.242556Z",
          "iopub.execute_input": "2024-12-16T13:03:18.243004Z",
          "iopub.status.idle": "2024-12-16T13:03:18.571383Z",
          "shell.execute_reply.started": "2024-12-16T13:03:18.242963Z",
          "shell.execute_reply": "2024-12-16T13:03:18.570533Z"
        },
        "id": "JXJ3kDjK8hHB",
        "outputId": "7820ebb4-0e06-4a3c-b9b5-70c28b63edc7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 1000x600 with 1 Axes>",
            "image/png": "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"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "# Generate predictions and evaluate\n",
        "model.eval()\n",
        "predictions, references, inputs = [], [], []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "    # Generate summary\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=150,\n",
        "        min_length=30,\n",
        "        num_beams=4,\n",
        "        no_repeat_ngram_size=2\n",
        "    )\n",
        "\n",
        "    # Decode predictions and references, remove special tokens\n",
        "    predictions.extend([re.sub(r\"<extra_id_\\d+>\", \"\", tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)).strip()\n",
        "                        for output in outputs])\n",
        "    references.extend([tokenizer.decode(label, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "                       for label in batch['labels']])\n",
        "    inputs.extend([tokenizer.decode(input_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "                   for input_id in input_ids])\n",
        "\n",
        "# Create a DataFrame to store inputs, predictions, and references\n",
        "df_results = pd.DataFrame({\n",
        "    'input': inputs,\n",
        "    'references': references,\n",
        "    'predictions': predictions\n",
        "})\n",
        "df_results.to_csv('generated_summaries1_indot5.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T13:26:21.886692Z",
          "iopub.execute_input": "2024-12-16T13:26:21.887359Z",
          "iopub.status.idle": "2024-12-16T13:35:00.922641Z",
          "shell.execute_reply.started": "2024-12-16T13:26:21.887326Z",
          "shell.execute_reply": "2024-12-16T13:35:00.921852Z"
        },
        "id": "oKElvFo28hHC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluasi ROUGE\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
        "\n",
        "for ref, pred in zip(references, predictions):\n",
        "    score = scorer.score(ref, pred)\n",
        "    rouge_scores[\"rouge1\"].append(score[\"rouge1\"].fmeasure)\n",
        "    rouge_scores[\"rouge2\"].append(score[\"rouge2\"].fmeasure)\n",
        "    rouge_scores[\"rougeL\"].append(score[\"rougeL\"].fmeasure)\n",
        "\n",
        "avg_rouge1 = sum(rouge_scores[\"rouge1\"]) / len(rouge_scores[\"rouge1\"])\n",
        "avg_rouge2 = sum(rouge_scores[\"rouge2\"]) / len(rouge_scores[\"rouge2\"])\n",
        "avg_rougeL = sum(rouge_scores[\"rougeL\"]) / len(rouge_scores[\"rougeL\"])\n",
        "\n",
        "print(f\"Average ROUGE-1: {avg_rouge1}\")\n",
        "print(f\"Average ROUGE-2: {avg_rouge2}\")\n",
        "print(f\"Average ROUGE-L: {avg_rougeL}\")\n",
        "\n",
        "# Evaluasi BERTScore\n",
        "P, R, F1 = bert_score(predictions, references, lang=\"en\")\n",
        "print(f\"Average BERTScore Precision: {P.mean().item()}\")\n",
        "print(f\"Average BERTScore Recall: {R.mean().item()}\")\n",
        "print(f\"Average BERTScore F1: {F1.mean().item()}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T13:11:53.733746Z",
          "iopub.execute_input": "2024-12-16T13:11:53.734070Z",
          "iopub.status.idle": "2024-12-16T13:12:23.120005Z",
          "shell.execute_reply.started": "2024-12-16T13:11:53.734040Z",
          "shell.execute_reply": "2024-12-16T13:12:23.119110Z"
        },
        "colab": {
          "referenced_widgets": [
            "a1b42db7031b46379147043ef32346f2",
            "9e7acd1b83a640b288234b73efc770eb",
            "6626db4cff1443e8978c552030cc65b2",
            "c0b8d055d631451790be3e1159c06f2b",
            "a458a298f5a241d5b28aa44f7691938b",
            "6d7824f4d0774cb7a880c9705240c957"
          ]
        },
        "id": "vPZrhUtz8hHC",
        "outputId": "55d219a1-2ee6-482f-b157-6af7ea269dde"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Average ROUGE-1: 0.43882495485273226\nAverage ROUGE-2: 0.2952317826792874\nAverage ROUGE-L: 0.4018151993430279\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1b42db7031b46379147043ef32346f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e7acd1b83a640b288234b73efc770eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6626db4cff1443e8978c552030cc65b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0b8d055d631451790be3e1159c06f2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a458a298f5a241d5b28aa44f7691938b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d7824f4d0774cb7a880c9705240c957"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Average BERTScore Precision: 0.8703164458274841\nAverage BERTScore Recall: 0.890876054763794\nAverage BERTScore F1: 0.8803143501281738\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "# Generate predictions and evaluate\n",
        "model.eval()\n",
        "predictions, references, inputs = [], [], []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "    # Generate summary\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        min_length=30,\n",
        "        max_length=150,\n",
        "        num_beams=10,\n",
        "        no_repeat_ngram_size=2,\n",
        "        repetition_penalty=3.0,\n",
        "        length_penalty=1.2,\n",
        "        early_stopping=True,\n",
        "        use_cache=True,\n",
        "        do_sample=True,\n",
        "        temperature=1.0,\n",
        "        top_k=50,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    # Decode predictions and references, remove special tokens\n",
        "    predictions.extend([re.sub(r\"<extra_id_\\d+>\", \"\", tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)).strip()\n",
        "                        for output in outputs])\n",
        "    references.extend([tokenizer.decode(label, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "                       for label in batch['labels']])\n",
        "    inputs.extend([tokenizer.decode(input_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "                   for input_id in input_ids])\n",
        "\n",
        "# Create a DataFrame to store inputs, predictions, and references\n",
        "df_results = pd.DataFrame({\n",
        "    'input': inputs,\n",
        "    'references': references,\n",
        "    'predictions': predictions\n",
        "})\n",
        "df_results.to_csv('generated_summaries2_indot5.csv', index=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T13:12:23.121082Z",
          "iopub.execute_input": "2024-12-16T13:12:23.121362Z",
          "iopub.status.idle": "2024-12-16T13:23:23.314607Z",
          "shell.execute_reply.started": "2024-12-16T13:12:23.121335Z",
          "shell.execute_reply": "2024-12-16T13:23:23.313723Z"
        },
        "id": "lBIxv_tq8hHC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluasi ROUGE\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
        "\n",
        "for ref, pred in zip(references, predictions):\n",
        "    score = scorer.score(ref, pred)\n",
        "    rouge_scores[\"rouge1\"].append(score[\"rouge1\"].fmeasure)\n",
        "    rouge_scores[\"rouge2\"].append(score[\"rouge2\"].fmeasure)\n",
        "    rouge_scores[\"rougeL\"].append(score[\"rougeL\"].fmeasure)\n",
        "\n",
        "avg_rouge1 = sum(rouge_scores[\"rouge1\"]) / len(rouge_scores[\"rouge1\"])\n",
        "avg_rouge2 = sum(rouge_scores[\"rouge2\"]) / len(rouge_scores[\"rouge2\"])\n",
        "avg_rougeL = sum(rouge_scores[\"rougeL\"]) / len(rouge_scores[\"rougeL\"])\n",
        "\n",
        "print(f\"Average ROUGE-1: {avg_rouge1}\")\n",
        "print(f\"Average ROUGE-2: {avg_rouge2}\")\n",
        "print(f\"Average ROUGE-L: {avg_rougeL}\")\n",
        "\n",
        "# Evaluasi BERTScore\n",
        "P, R, F1 = bert_score(predictions, references, lang=\"en\")\n",
        "print(f\"Average BERTScore Precision: {P.mean().item()}\")\n",
        "print(f\"Average BERTScore Recall: {R.mean().item()}\")\n",
        "print(f\"Average BERTScore F1: {F1.mean().item()}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T13:23:23.315984Z",
          "iopub.execute_input": "2024-12-16T13:23:23.316654Z",
          "iopub.status.idle": "2024-12-16T13:23:43.777421Z",
          "shell.execute_reply.started": "2024-12-16T13:23:23.316614Z",
          "shell.execute_reply": "2024-12-16T13:23:43.776577Z"
        },
        "id": "ShoIdz6Y8hHC",
        "outputId": "5c715f6a-613f-4712-a501-951aadf978cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Average ROUGE-1: 0.4266494850204733\nAverage ROUGE-2: 0.2826254909596367\nAverage ROUGE-L: 0.38945232022491805\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Average BERTScore Precision: 0.8710520267486572\nAverage BERTScore Recall: 0.8854697942733765\nAverage BERTScore F1: 0.8780692219734192\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T13:23:43.781951Z",
          "iopub.execute_input": "2024-12-16T13:23:43.782210Z",
          "iopub.status.idle": "2024-12-16T13:23:52.466364Z",
          "shell.execute_reply.started": "2024-12-16T13:23:43.782186Z",
          "shell.execute_reply": "2024-12-16T13:23:52.465183Z"
        },
        "id": "CytYdot-jYon",
        "outputId": "bbc1dd29-ce69-4cb8-d5f3-afa70462083e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.26.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.6.2)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Login dengan token Hugging Face\n",
        "login(token=\"\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T13:23:52.468350Z",
          "iopub.execute_input": "2024-12-16T13:23:52.469220Z",
          "iopub.status.idle": "2024-12-16T13:23:52.521475Z",
          "shell.execute_reply.started": "2024-12-16T13:23:52.469175Z",
          "shell.execute_reply": "2024-12-16T13:23:52.520895Z"
        },
        "id": "YrYeDYqLjYon"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo\n",
        "\n",
        "# Membuat repository\n",
        "repo_name = 'clean-model-indo-t5-10epoch-lower'\n",
        "create_repo(repo_name)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T13:23:52.522459Z",
          "iopub.execute_input": "2024-12-16T13:23:52.522735Z",
          "iopub.status.idle": "2024-12-16T13:23:52.934153Z",
          "shell.execute_reply.started": "2024-12-16T13:23:52.522709Z",
          "shell.execute_reply": "2024-12-16T13:23:52.933358Z"
        },
        "id": "sNitjm_-jYon",
        "outputId": "b46a6a96-354c-40e8-987e-f89bee3e628f"
      },
      "outputs": [
        {
          "execution_count": 21,
          "output_type": "execute_result",
          "data": {
            "text/plain": "RepoUrl('https://huggingface.co/intanutami/clean-model-indo-t5-10epoch-lower', endpoint='https://huggingface.co', repo_type='model', repo_id='intanutami/clean-model-indo-t5-10epoch-lower')"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_folder\n",
        "\n",
        "# Upload model ke repository yang sudah ada\n",
        "upload_folder(\n",
        "    repo_id='intanutami/clean-model-indo-t5-10epoch-lower',\n",
        "    folder_path='/kaggle/working/clean_indot5_model_lower',\n",
        "    commit_message=\"Upload my trained model\"\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-16T13:23:52.935207Z",
          "iopub.execute_input": "2024-12-16T13:23:52.935468Z",
          "iopub.status.idle": "2024-12-16T13:24:14.390847Z",
          "shell.execute_reply.started": "2024-12-16T13:23:52.935436Z",
          "shell.execute_reply": "2024-12-16T13:24:14.389975Z"
        },
        "id": "jmZngQITjYon",
        "outputId": "ae8df64d-4de3-4be0-a7b3-d36a22e7b51a",
        "colab": {
          "referenced_widgets": [
            "4100e1d913f84f52b38177ae6c5e38eb",
            "01d966aaae844b8da16919a4bdc39082",
            "a6b4fb673e8243a6a2579e68d7c160a3",
            "ed0e5d4815974f34a1e8ae031ee003ed",
            "a893b58bd57341ba8fac0a378331a09e",
            "2593f41cb4bc43d6bcbdf1d9b38cb496"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed0e5d4815974f34a1e8ae031ee003ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a893b58bd57341ba8fac0a378331a09e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "spiece.model:   0%|          | 0.00/793k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2593f41cb4bc43d6bcbdf1d9b38cb496"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 22,
          "output_type": "execute_result",
          "data": {
            "text/plain": "CommitInfo(commit_url='https://huggingface.co/intanutami/clean-model-indo-t5-10epoch-lower/commit/d50a9efd2643dfb5590e4ac8393aed6444c5cf86', commit_message='Upload my trained\\xa0model', commit_description='', oid='d50a9efd2643dfb5590e4ac8393aed6444c5cf86', pr_url=None, repo_url=RepoUrl('https://huggingface.co/intanutami/clean-model-indo-t5-10epoch-lower', endpoint='https://huggingface.co', repo_type='model', repo_id='intanutami/clean-model-indo-t5-10epoch-lower'), pr_revision=None, pr_num=None)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    }
  ]
}